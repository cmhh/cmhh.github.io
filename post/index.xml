<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on cmhh</title>
    <link>/post/</link>
    <description>Recent content in Posts on cmhh</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 08 Aug 2021 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Online Learning with Akka</title>
      <link>/2021/2021-08-08-online-learning-with-akka/</link>
      <pubDate>Sun, 08 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/2021-08-08-online-learning-with-akka/</guid>
      <description>Overview In a previous post, we looked at how we could consume messages in a Kafka topic, and use those messages for online training of a classifier. In this post, we look at how Akka could be used to solve the same problem, in broadly the same way. Akka is an implementation of the actor model, and is particularly well suited for building event-driven, reactive systems since the actors involved communicate exclusively by sending and reacting to messages.</description>
    </item>
    
    <item>
      <title>Online Learning with Apache Kafka</title>
      <link>/2021/2021-07-30-online-learning-with-apache-kafka/</link>
      <pubDate>Fri, 30 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/2021-07-30-online-learning-with-apache-kafka/</guid>
      <description>Overview I recently came across a scenario at work that would, like so many things in practice, be particularly well modelled as an ordered set of events. The messages would have a large number of uses in practice, but one would involve using them to train some sort of predictive model. Moreover, the messages would be arriving in real-time, and the patterns underpinning the messages might well change over time. Based on previous research, Kafka seemed like it would be a good fit, though I don&amp;rsquo;t yet have a huge amount of practical experience using it.</description>
    </item>
    
    <item>
      <title>A Brief Look at Apache Sedona</title>
      <link>/2021/2021-04-19-a-brief-look-at-apache-sedona/</link>
      <pubDate>Mon, 19 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/2021-04-19-a-brief-look-at-apache-sedona/</guid>
      <description>Overview Apache Sedona, formerly GeoSpark, is a library that let&amp;rsquo;s us make spatial RDDs and DataFrames in Apache Spark, as well as to run spatial queries. Feature classes can get very large, and so being able to run various geoprocessing tasks in a distributed context seems worthwhile. So, in this post, we take a very brief look at Apache Sedona.
This post is not intended to provide comprehensive coverage of the features provided by Sedona–that would be pretty unrealistic for a short blog post.</description>
    </item>
    
    <item>
      <title>Working Productively on Windows Using Windows Subsystem for Linux 2 and Docker</title>
      <link>/2020/2020-12-05-working-productively-on-windows-using-wsl2-and-docker/</link>
      <pubDate>Sat, 05 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/2020-12-05-working-productively-on-windows-using-wsl2-and-docker/</guid>
      <description>Overview A relatively common scenario in enterprise is to provide users a Windows desktop with a relatively small set of tools, and without administrative access. Depending on the type of work required, this sort of configuration might be perfectly reasonable, but for others it will be productivity-limiting at best.
Here we discuss the use of Windows Subsystem for Linux 2 (WSL2), along with Docker, as a means of end-user enablement that might offer a reasonable compromise between safety and productivity.</description>
    </item>
    
    <item>
      <title>Docker on Windows with Windows Subsystem for Linux 2</title>
      <link>/2020/2020-10-31-docker-on-windows-with-wsl2/</link>
      <pubDate>Sat, 31 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/2020-10-31-docker-on-windows-with-wsl2/</guid>
      <description>Overview Windows Subsystem for Linux version 2 (WSL2) brings something like a native Linux environment to Windows. And because it&amp;rsquo;s native, we can even run Docker on WSL2 and access the running containers from the Windows host without hassle.
Dependencies All that is required to run WSL2 is Windows Server 2019 or higher, or Windows 10 (any edition) build 2004 or higher. And, of course, we must also install a working Linux distribution.</description>
    </item>
    
    <item>
      <title>Using PostGIS as a Spatial Backend for R</title>
      <link>/2020/2020-10-31-using-postgis-as-a-spatial-backend-for-r/</link>
      <pubDate>Sat, 31 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/2020-10-31-using-postgis-as-a-spatial-backend-for-r/</guid>
      <description>Overview R is a wildly extensible tool, and that extensibility means it can be used in a surprising array of domains. This versatility is great, but there are times when R is just not the best tool for the job–at least not by itself. Geospatial applications are a good example. Packages such as rgdal, sf, and raster make R quite usable in this domain, but R&amp;rsquo;s largely in-memory approach can make geoprocessing tasks involving large spatial objects a little challenging.</description>
    </item>
    
    <item>
      <title>Seasonal Adjustment as a Service</title>
      <link>/2020/2020-06-01-seasonal-adjustment-as-a-service/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/2020-06-01-seasonal-adjustment-as-a-service/</guid>
      <description>Overview X13-ARIMA-SEATS is a widely used seasonal adjustment program developed by the United States Census Bureau. It&amp;rsquo;s a command-line tool, and I&amp;rsquo;ve wondered whether it would be useful to run as a web service–where I work, the standard security model makes running command-line tools a little problematic, so running a single service that could be accessed by any other client remotely would seem, on the face of it, a useful proposition.</description>
    </item>
    
    <item>
      <title>Building Tile Services on-the-fly with GeoServer</title>
      <link>/2019/2019-09-17-building-tile-services-on-the-fly-with-geoserver/</link>
      <pubDate>Tue, 17 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/2019-09-17-building-tile-services-on-the-fly-with-geoserver/</guid>
      <description>Overview Interactive web applications can be a great way to present information to a diverse audience. If your interface is well designed and intuitive, then almost anybody will be able to use it. And where data has a spatial dimension, slippy maps can be an effective way of presenting information. But sometimes the objects we want to render are a bit on the large side. This means that our web browser can struggle to render all the required features, and it also means that mobile users of an app might end up using excessive amounts of data.</description>
    </item>
    
    <item>
      <title>Web Maps and Tiles with QGIS</title>
      <link>/2017/2017-01-26-web-maps-and-tiles-with-qgis/</link>
      <pubDate>Thu, 26 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/2017-01-26-web-maps-and-tiles-with-qgis/</guid>
      <description>Overview In an earlier post, we looked at how to style a map using TileMill (actually, using a fork called TileOven), render tiles with Mapnik, and finally serve tiles using TileStache.
Serving tiles in this way is useful, avoiding the need to bundle potentially large features with an online map or web app. At the same time, the outlined process is a little involved.
In this post we look at the open source Quantum GIS.</description>
    </item>
    
    <item>
      <title>Serving Geospatial Features with Mapnik and TileStache</title>
      <link>/2016/2016-11-28-serving-geospatial-features-with-mapnik-and-tilestache/</link>
      <pubDate>Mon, 28 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/2016-11-28-serving-geospatial-features-with-mapnik-and-tilestache/</guid>
      <description>Overview When making maps for display on the web, the size of the various objects being rendered can quickly become overwhelming. For example, a polygon feature class containing New Zealand Meshblocks, a set of roughly 40,000 mutually exclusive and exhaustive polygons, is more than 100MB when stored as a shapefile, but much larger uncompressed and rendered in a browser window.
There are various approaches one can take to attempt to deal with this issue, though useful solutions tend to depend on server-side processing.</description>
    </item>
    
    <item>
      <title>Routing in R Using the Open Source Routing Machine (OSRM)</title>
      <link>/2016/2016-11-27-routing-in-r-using-osrm/</link>
      <pubDate>Sun, 27 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/2016-11-27-routing-in-r-using-osrm/</guid>
      <description>Overview Edit 2020-07-16: The original version of this post included a bespoke decoder for geometries encoded as Google polylines. The post has been updated to use the googlePolylines package instead.
I often find myself needing to establish the travel time or distance between arrays of addresses. In the past I have used ArcMap’s Network Analyst tool, but have found the syntax to be clunky at best, and the performance to be very mediocre.</description>
    </item>
    
    <item>
      <title>Using R Packages to Disseminate Data</title>
      <link>/2016/2016-07-20-data-packages/</link>
      <pubDate>Wed, 20 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/2016-07-20-data-packages/</guid>
      <description>Introduction R packages can include datasets, and it is very common to do so. This shouldn’t really come as a surprise–R is a statistical language in the end, and it is normal to include working examples with pacakges; and including datasets against which to run unit tests and other examples is clearly a useful thing to do.
But datasets themselves aren’t central to most packages–statistical methodologies often are, and the data is there for testing or illustrative purposes.</description>
    </item>
    
    <item>
      <title>Web Services with Node.js</title>
      <link>/2016/2016-05-28-web-services-with-nodejs/</link>
      <pubDate>Sat, 28 May 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/2016-05-28-web-services-with-nodejs/</guid>
      <description>What is node.js? According to the Node.js website:
 Node.js is a JavaScript runtime built on Chrome&amp;rsquo;s V8 JavaScript engine. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient.
  Node.js&#39; package ecosystem, npm, is the largest ecosystem of open source libraries in the world.
 Event-driven, non-blocking I/O model Node.js program execution consists of a single-threaded event loop. But all I/O methods in the Node.</description>
    </item>
    
  </channel>
</rss>
