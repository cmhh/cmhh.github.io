<!DOCTYPE html>
<html lang="en-us">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>cmhh  | A Brief Look at Apache Sedona</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.7.1/css/bulma.min.css" />
    <link href="https://cmhh.github.io/css/highlight/atom-one-dark-reasonable.css" rel='stylesheet' type='text/css' />
    <link rel="stylesheet" href="https://cmhh.github.io/css/blog.css" />
    <link rel="stylesheet" href="https://cmhh.github.io/css/custom.css" />
</head>
<body>

    
    <nav class="navbar is-fixed-top" role="navigation" aria-label="main navigation">
        <div class="navbar-brand">
            <a class="navbar-item" href="https://cmhh.github.io/">Home</a>
            
        </div>
    </nav>
    

    
    <section class="hero is-info is-medium">
        <div class="hero-body" style="background-image: url(https://cmhh.github.io/img/bg-blog-2.jpg);">
            <div class="container has-text-centered">
                <br>
                <h1 class="title is-size-1">
                    
                        A Brief Look at Apache Sedona
                    
                </h1>
                
            </div>
        </div>
    </section>


<div class="container">
    <div class="section">
    

<div class="columns">
    <div class="column is-9">
        <div class="tile is-child box">
            <div class="content">
                
<script src="../../rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="../../rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="../../rmarkdown-libs/anchor-sections/anchor-sections.js"></script>
<script src="../../rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="../../rmarkdown-libs/plotly-binding/plotly.js"></script>
<script src="../../rmarkdown-libs/typedarray/typedarray.min.js"></script>
<script src="../../rmarkdown-libs/jquery/jquery.min.js"></script>
<link href="../../rmarkdown-libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="../../rmarkdown-libs/crosstalk/js/crosstalk.min.js"></script>
<link href="../../rmarkdown-libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="../../rmarkdown-libs/plotly-main/plotly-latest.min.js"></script>


<div id="overview" class="section level1">
<h1>Overview</h1>
<p><a href="https://sedona.apache.org/">Apache Sedona</a>, formerly GeoSpark, is a library that let’s us make spatial RDDs and DataFrames in Apache Spark, as well as to run spatial queries. Feature classes can get very large, and so being able to run various geoprocessing tasks in a distributed context seems worthwhile. So, in this post, we take a very brief look at Apache Sedona.</p>
<p>This post is not intended to provide comprehensive coverage of the features provided by Sedona–that would be pretty unrealistic for a short blog post. Rather, we will just run a couple of simple examples which will give some sense of both the overall interface and also <code>performance</code> relative to some other common tools.</p>
<p>For the purposes of illustration, we consider just two examples:</p>
<ul>
<li>dissolve a set of polygons via <code>ST_Union_Aggr</code> / <code>ST_Union</code></li>
<li>attach polygon attributes to a set of points using <code>ST_Intersects</code></li>
</ul>
<p>The polygons we will dissolve are <a href="https://koordinates.com/from/datafinder.stats.govt.nz/layer/105173/">Meshblock Higher Geographies 2021 (high definition)</a> provided by <a href="https://stats.govt.nz">Stats NZ</a>, and the grouping variable used will be Regional Council. For the task of attaching polygon attributes to points, we borrow the meshblock code from the meshblock 2021 feature class above, and we attach it to <a href="https://koordinates.com/from/data.linz.govt.nz/layer/53353/">NZ Street Address</a> provided by <a href="https://www.linz.govt.nz/">Land Information New Zealand (LINZ)</a>.</p>
<p>Neither of these tasks is <em>massive</em>, with each being comfortably achievable on a single, typically endowed laptop. In this setting, PostGIS is faster than Sedona, but Sedona offers a more scalable solution overall. Sedona can perform as well as PostGIS when using a cluster with several workers, and Sedona should scale to very large problems where vertical scaling of PostGIS would become challenging.</p>
</div>
<div id="environment" class="section level1">
<h1>Environment</h1>
<p>The majority of the examples in this post were executed on a Dell XPS 15 laptop with 16GB of DDR4 RAM @ 2667MHz, and 12 x Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz. In this environment, we run Spark in pseudo-distributed mode. That is, we use multiple workers, but all on a single host, and using an exclusively local filesystem.</p>
<p>Spark is really intended to be used in a genuinely distributed environment, though, and so for comparison we also run our code using AWS Elastic MapReduce. We use emr-6.2.0, with Hadoop 3.2.1 and Spark 3.0.1, and the instances themselves are m5.xlarge which have 4 virtual cores and 16GiB of RAM. We vary the number of worker nodes, choosing configurations with both 6 and 12 workers. Importantly, in this scenario we also use a distributed filesystem (HDFS).</p>
</div>
<div id="example-1---dissolve-meshblock-by-regional-council" class="section level1">
<h1>Example 1 - Dissolve Meshblock by Regional Council</h1>
<p>As noted, for our first example we take the union of all meshblocks by Regional Council. Visually:</p>
<p><img src="../../img/sedona/dissolve01.png" /></p>
<p>We discuss each approach in detail below, but a high-level summary is as follows:</p>
<div class="three-col-container">
<div class="three-col ttle">
PostGIS
</div>
<div class="three-col ttle">
R
</div>
<div class="three-col ttle">
Sedona
</div>
<div class="three-col">
<pre class="sql"><code>SELECT
  regc2021_v, 
  ST_Multi(ST_Union(geom)) as geom
FROM 
  statsnz.meshblock2021
GROUP BY 
  regc2021_v
ORDER BY
  regc2021_v
  
  
  </code></pre>
</div>
<div class="three-col">
<pre class="r"><code>rc &lt;- mbhg %&gt;%
  group_by(REGC2021_V) %&gt;%
  summarise(n = n())
  
  
  
  
  
  
  
  
  </code></pre>
</div>
<div class="three-col">
<pre class="scala"><code>val rc = spark.sql(&quot;&quot;&quot;
  SELECT 
    REGC2021_V, 
    ST_Union_Aggr(geometry) 
      as geometry 
  FROM 
    mb 
  GROUP BY
    REGC2021_V 
  ORDER BY
    REGC2021_V
&quot;&quot;&quot;)</code></pre>
</div>
<div class="three-col">
121 seconds
</div>
<div class="three-col">
124 seconds
</div>
<div class="three-col">
152 seconds (stand-alone)
</div>
</div>
<p>Sedona will scale to some extent–when run on a cluster using Elastic MapReduce (6.2.0), Sedona took 122 seconds with 6 workers, and 108 workers with 12 workers. Other optimisations are likely possible when tuning various cluster parameters, such as number of workers, driver memory, and so on.</p>
<div id="postgis" class="section level2">
<h2>PostGIS</h2>
<p>Assuming we have stored the meshblock features in a PostGIS table called <code>statsnz.meshblock2021</code>, we can dissolve them by running the following SQL query:</p>
<pre class="sql"><code>SELECT
  regc2021_v, ST_Multi(ST_Union(geom)) as geom
FROM 
  statsnz.meshblock2021
GROUP BY 
  regc2021_v
ORDER BY
  regc2021_v</code></pre>
<p>This took 121 seconds.</p>
</div>
<div id="r-sf" class="section level2">
<h2>R / <code>sf</code></h2>
<p>To import the meshblock data in R, we run:</p>
<pre class="r"><code>library(sf)

system.time({
  mbhg &lt;- st_read(&quot;meshblock-higher-geographies-2021-high-definition.shp&quot;)  
})</code></pre>
<p>This took 3 seconds, and the resulting data frame occupies 300MB in memory. To dissolve it, we run:</p>
<pre class="r"><code>rc &lt;- mbhg %&gt;%
  group_by(REGC2021_V) %&gt;%
  summarise(n = n())</code></pre>
<p>This took around 124 seconds, and the resulting data frame occupies 3.9MB in memory.</p>
</div>
<div id="sedona---local" class="section level2">
<h2>Sedona - Local</h2>
<p>We first load our meshblock feature class as follows:</p>
<pre class="scala"><code>import org.apache.sedona.core.formatMapper.shapefileParser.ShapefileReader

val mbrdd = ShapefileReader.readToPolygonRDD(
  sc, // the cluster SparkContext
  &quot;meshblock-higher-geographies-2021-high-definition&quot;
)</code></pre>
<p>The type of mbrdd is <code>PolygonRDD</code>, which is-a <code>SpatialRDD</code>. <code>SpatialRDD</code> has a standard <code>RDD</code> under the hood, and has methods for adding indexes, doing spatial joins, and so on. However, we can convert <code>SpatialRDD</code>s to <code>DataFrame</code>s, and then run reasonably standard looking SQL queries on the result. This is probably going to be the most accessible option for those who aren’t terribly familiar with Spark or Java / Scala. To convert to <code>DataFrame</code>:</p>
<pre class="scala"><code>import org.apache.sedona.sql.utils.{Adapter, SedonaSQLRegistrator}

SedonaSQLRegistrator.registerAll(spark)

val mbdf = Adapter
  .toDf(mbrdd, spark)
  .repartition(32)
  .persist</code></pre>
<p>The result is a <code>DataFrame</code> where our geometry is stored as type <code>Geometry</code> in a column called geometry, and calling <code>registerAll(spark)</code> ensures that the <code>Geometry</code> type is understood by Spark. For example, we create a user-defined function which calculates the area of a polygon:</p>
<pre class="scala"><code>import org.locationtech.jts.geom.Geometry
import org.apache.spark.sql.functions.{col, udf}

val f: (x: Geometry) =&gt; x.getArea</code></pre>
<p>and use it to derive an area column:</p>
<pre class="scala"><code>mbdf
  .withColumn(&quot;area&quot;, f(col(&quot;geometry&quot;)))
  .select(&quot;MB2021_V1_&quot;, &quot;REGC2021_V&quot;, &quot;REGC2021_2&quot;, &quot;area&quot;)
  .limit(10)
  .show</code></pre>
<pre class="plaintext"><code>+----------+----------+------------------+--------------------+
|MB2021_V1_|REGC2021_V|        REGC2021_2|                area|
+----------+----------+------------------+--------------------+
|   0973300|        03|    Waikato Region|   68346.86758174733|
|   4002221|        06|Hawke&#39;s Bay Region|  42925.980450310104|
|   4008822|        13| Canterbury Region|   37448.43920328615|
|   2311100|        13| Canterbury Region|3.8148438963027686E7|
|   2815503|        13| Canterbury Region|  24410.008602414582|
|   0221305|        02|   Auckland Region|    884246.943146247|
|   1152900|        03|    Waikato Region| 4.621434800254852E7|
|   4011098|        13| Canterbury Region|   8207.470212947434|
|   4011489|        13| Canterbury Region|1.0198131361068603E7|
|   2830201|        14|      Otago Region|   96059.66604287364|
+----------+----------+------------------+--------------------+</code></pre>
<p>Either way, we now create a view from the <code>DataFrame</code> which we can use from our SQL context:</p>
<pre class="scala"><code>mbdf.createOrReplaceTempView(&quot;mb&quot;)</code></pre>
<p>and then use to write standard looking spatial SQL queries. For example, we now add an area column in an arguably more straightforward manner:</p>
<pre class="scala"><code>spark
  .sql(&quot;&quot;&quot; 
    SELECT 
      MB2021_V1_, 
      REGC2021_V, 
      REGC2021_2, 
      ST_AREA(geometry) as area
    FROM
      mb
    LIMIT 10
  &quot;&quot;&quot;)
  .show</code></pre>
<pre class="plaintext"><code>+----------+----------+------------------+--------------------+
|MB2021_V1_|REGC2021_V|        REGC2021_2|                area|
+----------+----------+------------------+--------------------+
|   0973300|        03|    Waikato Region|   68346.86758174733|
|   4002221|        06|Hawke&#39;s Bay Region|  42925.980450310104|
|   4008822|        13| Canterbury Region|   37448.43920328615|
|   2311100|        13| Canterbury Region|3.8148438963027686E7|
|   2815503|        13| Canterbury Region|  24410.008602414582|
|   0221305|        02|   Auckland Region|    884246.943146247|
|   1152900|        03|    Waikato Region| 4.621434800254852E7|
|   4011098|        13| Canterbury Region|   8207.470212947434|
|   4011489|        13| Canterbury Region|1.0198131361068603E7|
|   2830201|        14|      Otago Region|   96059.66604287364|
+----------+----------+------------------+--------------------+</code></pre>
<p>More importantly, we can dissolve the meshblocks by region as desired as follows:</p>
<pre class="scala"><code>val rc = spark.sql(
  &quot;&quot;&quot;
  SELECT 
    REGC2021_V, 
    ST_Union_Aggr(geometry) as geometry 
  FROM 
    mb 
  GROUP BY
    REGC2021_V 
  ORDER BY
    REGC2021_V
  &quot;&quot;&quot; 
)</code></pre>
<pre class="scala"><code>rc: org.apache.spark.sql.DataFrame = [REGC2021_V: string, geometry: geometry]</code></pre>
<p>Spark SQL execution is lazy, so we need to trigger an action in order to see how long this actually takes. I like to write a little function I can re-use for timing purposes as follows:</p>
<pre class="scala"><code>def timeit[T](block: =&gt;T): (T, Double) = {
  val startTime = System.nanoTime
  val res: T = block
  (res, (System.nanoTime - startTime) / 1e9)
}</code></pre>
<p>So:</p>
<pre class="scala"><code>val (_, t) = timeit { rc.show }</code></pre>
<pre class="scala"><code>t: Double = 152.024319529</code></pre>
<p>At 152 seconds this is a little slower than PostGIS, but pretty reasonable all the same. However, there is a pretty big caveat here. Spark is pretty sophisticated, and in practice performance will vary a lot depending on how things are configured. We can vary the number of executors, the number of cores each executor can use, driver memory, and so on, and so on. In this case we did not spend any time tuning these parameters, but we did deliberately repartition the input meshblock features into 36 chunks, and this has a dramatic effect on performance. The following chart shows runtime as a function of the number of partitions:</p>
<div id="htmlwidget-1" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"data":[{"x":[1,6,12,36,54,75,100,200],"y":[4905,729,310,152,138,158,170,269],"text":["partitions:   1<br />time: 4905","partitions:   6<br />time:  729","partitions:  12<br />time:  310","partitions:  36<br />time:  152","partitions:  54<br />time:  138","partitions:  75<br />time:  158","partitions: 100<br />time:  170","partitions: 200<br />time:  269"],"type":"scatter","mode":"lines+markers","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","marker":{"autocolorscale":false,"color":"rgba(0,0,0,1)","opacity":1,"size":5.66929133858268,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)"}},"frame":null}],"layout":{"margin":{"t":26.2283105022831,"r":7.30593607305936,"b":40.1826484018265,"l":48.9497716894977},"plot_bgcolor":"rgba(235,235,235,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-8.95,209.95],"tickmode":"array","ticktext":["0","50","100","150","200"],"tickvals":[0,50,100,150,200],"categoryorder":"array","categoryarray":["0","50","100","150","200"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"number of partitions","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-100.35,5143.35],"tickmode":"array","ticktext":["0","1000","2000","3000","4000","5000"],"tickvals":[0,1000,2000,3000,4000,5000],"categoryorder":"array","categoryarray":["0","1000","2000","3000","4000","5000"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"runtime (seconds)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"1fe9b1b56290f":{"x":{},"y":{},"type":"scatter"},"1fe9b4d0a955d":{"x":{},"y":{}}},"cur_data":"1fe9b1b56290f","visdat":{"1fe9b1b56290f":["function (y) ","x"],"1fe9b4d0a955d":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>The machine this task was run on has 12 cores, and the best performance seems to be roughly where there are 3 times as many partitions as cores.</p>
<p>Note that those familiar with Spark could attempt to do things a bit more… primitively. Looking again at the original object, <code>mbrdd</code>, we can get the underlying <code>RDD</code> as:</p>
<pre class="scala"><code>mbrdd.getRawSpatialRDD.rdd</code></pre>
<pre class="scala"><code>res0: org.apache.spark.api.java.RDD[org.locationtech.jts.geom.Geometry] = MapPartitionsRDD[1] at map at ShapefileReader.java:111</code></pre>
<p>This is essentially just a collection of objects of type <code>Geometry</code>. We can call a method <code>getUserData</code> on a <code>Geometry</code> object to get the attributes associated with a feature as a single tab-delimited string. The 24th field is the regional council code, and we can use this to create a pair <code>RDD</code>:</p>
<pre class="scala"><code>val pairrdd = mbrdd
  .getRawSpatialRDD.rdd
  .groupBy(x =&gt; x.getUserData.toString.split(&quot;\t&quot;)(24))</code></pre>
<p>We can then fold the geometries in each key-value pair naively as follows:</p>
<pre class="scala"><code>val rc = pairrdd
  .map(x =&gt; {
    val y = x._2
    (x._1 -&gt; y.tail.foldLeft(y.head)(_.union(_)))
  })</code></pre>
<p>This does in fact give us what we want, however the approach turns out to be way too naive, taking 45 minutes all up! That said, it would scale if we threw enough workers at it. Still, at times this sort of approach will be useful.</p>
</div>
<div id="sedona---elastic-mapreduce" class="section level2">
<h2>Sedona - Elastic MapReduce</h2>
<p>We deployed on Elastic MapReduce, the code required is the same, with very minor differences. In this case, we naively set the number of partitions for each input feature class to be the number of cores times the number of workers. In addition, when using EMR we store our source shapefiles in the Hadoop filesystem. A cluster with 6 workers was able to dissolve the features as required in 122 seconds, and a cluster with 12 workers dissolved the features in 108 seconds.</p>
</div>
</div>
<div id="example-2---intersection-of-addresses-and-meshblocks" class="section level1">
<h1>Example 2 - Intersection of Addresses and Meshblocks</h1>
<p>For the next example, we find the meshblock polygon each of our address points belongs to, and we then borrow attributes from the containing polygon, attaching them directly as attributes of each address point. Visually:</p>
<p><img src="../../img/sedona/intersect01.png" /></p>
<p>We discuss each approach in detail below, but a high-level summary is as follows:</p>
<div class="three-col-container">
<div class="three-col ttle">
PostGIS
</div>
<div class="three-col ttle">
R
</div>
<div class="three-col ttle">
Sedona
</div>
<div class="three-col">
<pre class="sql"><code>SELECT 
  addr.address_id, 
  addr.full_add_1,  
  mb.mb2021_v1_, 
  addr.geom 
FROM
  linz.address addr, 
  statsnz.meshblock2021 mb 
WHERE 
  st_intersects(addr.geom, mb.geom)
  
  </code></pre>
</div>
<div class="three-col">
<pre class="r"><code>sf::st_intersection(
  select(
    addr, ADDRESS_ID, FULL_ADD_1
  ), 
  select(mbhg, MB2021_V1_)
) 






</code></pre>
</div>
<div class="three-col">
<pre class="scala"><code>val rc = spark.sql(&quot;&quot;&quot;
  SELECT 
    REGC2021_V, 
    ST_Union_Aggr(geometry) 
      as geometry 
  FROM 
    mb 
  GROUP BY
    REGC2021_V 
  ORDER BY
    REGC2021_V
&quot;&quot;&quot;)</code></pre>
</div>
<div class="three-col">
193 seconds
</div>
<div class="three-col">
3194 seconds
</div>
<div class="three-col">
268 seconds (stand-alone)
</div>
</div>
<p>As before, Sedona will scale to some extent–when run on a cluster using Elastic MapReduce (6.2.0), Sedona took 154 seconds with 6 workers, and 110 workers with 12 workers. Other optimisations are likely possible when tuning various cluster parameters, such as number of workers, driver memory, and so on. R is extremely slow in this case because we do not create and exploit any kind of spatial indexing.</p>
<div id="postgis-1" class="section level2">
<h2>PostGIS</h2>
<p>Assuming we have address points stored in a PostGIS table called <code>linz.address</code>, and meshblock as before in <code>statsnz.meshblock2021</code>, we can borrow attributes from each addresses enclosing meshblock as follows:</p>
<pre class="sql"><code>SELECT 
  addr.address_id, 
  addr.full_add_1,  
  mb.mb2021_v1_, 
  addr.geom 
FROM
  linz.address addr, 
  statsnz.meshblock2021 mb 
WHERE 
  st_intersects(addr.geom, mb.geom)</code></pre>
<p>This took 193 seconds. Note that to ensure this query can be executed efficiently, it is necessary to ensure spatial indexes are created for both input features ahead of time. For example:</p>
<pre class="sql"><code>CREATE INDEX address_geom_idx ON linz.address using GIST(geom);
CREATE INDEX meshblock2021_geom_idx ON statsnz.meshblock2021 using GIST(geom);</code></pre>
</div>
<div id="r-sf-1" class="section level2">
<h2>R / <code>sf</code></h2>
<p>To import the address data in R, we run:</p>
<pre class="r"><code>system.time({
  addr &lt;- st_read(&quot;nz-street-address.shp&quot;)  
})</code></pre>
<p>This took 40 seconds, and the resulting data frame occupies 1.5GB in memory. To intersect this with the meshblock feature class:</p>
<pre class="r"><code>sf::st_intersection(
  select(addr, ADDRESS_ID, FULL_ADD_1), 
  select(mbhg, MB2021_V1_)
) </code></pre>
<p>This took 3194 seconds, or 53 minutes. This is slow because there is no way to make use of a spatial index, and to find which meshblock an address is in will require a full scan of the meshblock feature class for each address. Note, though, that it would be easy to parallelise this task simply by breaking the address input into ranges and intersecting each concurrently.</p>
</div>
<div id="sedona---local-1" class="section level2">
<h2>Sedona - Local</h2>
<p>To create a <code>DataFrame</code> containing addresses, we run:</p>
<pre class="scala"><code>val addrpath = &quot;shp/linz/nz-street-address&quot;
val addrrdd = ShapefileReader.readToGeometryRDD(sc, addrpath)
val addr = Adapter.toDf(addrrdd, spark).repartition(32).persist
addr.createOrReplaceTempView(&quot;addr&quot;)</code></pre>
<p>And to intersect this with the meshblock <code>DataFrame</code>, we run:</p>
<pre class="scala"><code>val addrmb = spark
  .sql(&quot;&quot;&quot;
  SELECT 
    addr.ADDRESS_ID, 
    addr.FULL_ADD_1, 
    mb.MB2021_V1_, 
    addr.geometry 
  FROM 
    addr, 
    mb  
  WHERE 
    st_intersects(addr.geometry, bb.geometry)
  &quot;&quot;&quot;)</code></pre>
<p>This took 268 seconds. This is reasonable, but still considerably slower than using PostGIS. As for the PostGIS query, it is necessary to ensure that spatial indexes are used in order for this query to run efficiently. One way of doing this is to ensure we set the Spark config <code>sedona.global.index</code> to <code>true</code>. Without doing this, the query will run <em>much</em> slower.</p>
<p>As before, we can get similar results using <code>RDD</code>s directly. For example, we can produce a new pair <code>RDD</code> that contains meshblocks as a key, and all the points within the meshblock as values in the following way:</p>
<pre class="scala"><code>import org.apache.sedona.core.enums.IndexType
import org.apache.sedona.core.spatialOperator.JoinQuery

addrrdd.spatialPartitioning(GridType.KDBTREE)
mbrdd.spatialPartitioning(addrrdd.getPartitioner())
addrrdd.buildIndex(IndexType.QUADTREE, true)
mbrdd.buildIndex(IndexType.QUADTREE, true)

val result = JoinQuery.SpatialJoinQuery(addrrdd, mbrdd, true, false)</code></pre>
<p>Again, those familiar with Spark already will find objects like this relatively easy to work with. For example, to print a list of meshblock and address ID pairs:</p>
<pre class="scala"><code>result.rdd.flatMap(x =&gt; {
  val mbid = x._1.getUserData.toString.split(&quot;\t&quot;)(1)
  val addrids = x._2.asScala.map(_.getUserData.toString.split(&quot;\t&quot;)(1))
  addrids.map((mbid, _))
}).take(10).foreach(println)</code></pre>
<pre class="plaintext"><code>(4003499,2099993)
(4003499,2099992)
(4003499,2099994)
(1619000,2099934)
(2817502,2010408)
(3031002,2099960)
(4008278,2099958)
(4007449,2100082)
(4006988,2099944)
(1618904,2099935)</code></pre>
</div>
<div id="sedona---elastic-mapreduce-1" class="section level2">
<h2>Sedona - Elastic MapReduce</h2>
<p>When deployed on Elastic MapReduce, the code required is the same, with very minor differences. In this case, we again naively set the number of partitions for each input feature class to be the number of cores times the number of workers. A cluster with 6 workers was able to dissolve the features as required in 154 seconds, and a cluster with 12 workers dissolved the features in 110 seconds</p>
</div>
</div>
<div id="appendix---getting-a-repl-up-and-running" class="section level1">
<h1>Appendix - Getting a REPL Up and Running</h1>
<div id="sbt-console" class="section level2">
<h2><code>sbt console</code></h2>
<p>One of the easier ways to get running with Spark is to set up a basic sbt project. We can then use the sbt console to run code interactively, or else output a jar file which can be used with <code>spark-shell</code>. To get going, all we need is a <code>build.sbt</code> file with the following content:</p>
<pre class="scala"><code>name := &quot;&quot;&quot;sedonatest&quot;&quot;&quot;
version := &quot;0.0.1&quot;

scalaVersion := &quot;2.12.13&quot;
scalacOptions += &quot;-Ydelambdafy:inline&quot;

libraryDependencies ++= Seq(
  &quot;org.apache.spark&quot; %% &quot;spark-core&quot; % &quot;3.0.1&quot;, 
  &quot;org.apache.spark&quot; %% &quot;spark-sql&quot; % &quot;3.0.1&quot;, 
  &quot;org.apache.sedona&quot; %% &quot;sedona-core-3.0&quot; % &quot;1.0.0-incubating&quot;,
  &quot;org.apache.sedona&quot; %% &quot;sedona-sql-3.0&quot; % &quot;1.0.0-incubating&quot;,
  &quot;org.apache.sedona&quot; %% &quot;sedona-viz-3.0&quot; % &quot;1.0.0-incubating&quot;,
  &quot;org.apache.sedona&quot; %% &quot;sedona-viz-3.0&quot; % &quot;1.0.0-incubating&quot;,
  &quot;org.locationtech.jts&quot; % &quot;jts-core&quot; % &quot;1.18.1&quot;,
  &quot;org.geotools&quot; % &quot;gt-main&quot; % &quot;24.0&quot;,
  &quot;org.geotools&quot; % &quot;gt-referencing&quot; % &quot;24.0&quot;,
  &quot;org.geotools&quot; % &quot;gt-epsg-hsql&quot; % &quot;24.0&quot;
)

resolvers ++= Seq(
  &quot;Open Source Geospatial Foundation Repository&quot; at &quot;https://repo.osgeo.org/repository/release/&quot;,
  &quot;Apache Software Foundation Snapshots&quot; at &quot;https://repository.apache.org/content/groups/snapshots&quot;,
  &quot;Java.net repository&quot; at &quot;https://download.java.net/maven/2&quot;
)</code></pre>
<p>From within the project folder, we first run <code>sbt</code> to get an interactive build shell, and then <code>console</code> to get to the Scala REPL. Run this way, the Spark and Sedona libraries will both be available.</p>
<p>Unlike <code>spark-shell</code>, <code>sbt console</code> will not provide a <code>SparkSession</code> instance. Assuming we’re running in standalone mode, we can create one as follows:</p>
<pre class="scala"><code>val spark: SparkSession = SparkSession
    .builder
    .master(&quot;local[*]&quot;)
    .appName(&quot;sedonatest&quot;)
    .config(
      &quot;spark.serializer&quot;, 
      &quot;org.apache.spark.serializer.KryoSerializer&quot;
    ) 
    .config(
      &quot;spark.kryo.registrator&quot;, 
      &quot;org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator&quot;
    ) 
    .config(&quot;sedona.global.index&quot;,&quot;true&quot;)
    .getOrCreate</code></pre>
<p>And, of course, to get a <code>SparkContext</code>:</p>
<pre class="scala"><code>val sc = spark.sparkContext</code></pre>
</div>
<div id="spark-shell" class="section level2">
<h2><code>spark-shell</code></h2>
<p>If we have access to a Spark cluster and wish to use <code>spark-shell</code> instead, then we’d modify the dependencies in <code>build.sbt</code> as follows:</p>
<pre class="scala"><code>libraryDependencies ++= Seq(
  &quot;org.apache.spark&quot; %% &quot;spark-core&quot; % &quot;3.0.1&quot; % &quot;provided&quot;, 
  &quot;org.apache.spark&quot; %% &quot;spark-sql&quot; % &quot;3.0.1&quot; % &quot;provided&quot;, 
  &quot;org.apache.sedona&quot; %% &quot;sedona-core-3.0&quot; % &quot;1.0.0-incubating&quot;,
  &quot;org.apache.sedona&quot; %% &quot;sedona-sql-3.0&quot; % &quot;1.0.0-incubating&quot;,
  &quot;org.apache.sedona&quot; %% &quot;sedona-viz-3.0&quot; % &quot;1.0.0-incubating&quot;,
  &quot;org.apache.sedona&quot; %% &quot;sedona-viz-3.0&quot; % &quot;1.0.0-incubating&quot;,
  &quot;org.locationtech.jts&quot; % &quot;jts-core&quot; % &quot;1.18.1&quot;,
  &quot;org.geotools&quot; % &quot;gt-main&quot; % &quot;24.0&quot;,
  &quot;org.geotools&quot; % &quot;gt-referencing&quot; % &quot;24.0&quot;,
  &quot;org.geotools&quot; % &quot;gt-epsg-hsql&quot; % &quot;24.0&quot;
)</code></pre>
<p>We’d typically make a so-called ‘fat jar’, and adding <code>% "provided"</code> ensures the Spark dependencies, which will be present already, aren’t included in the archive. Note that we’re using Spark 3.0.1 here, so adjust this as required to match whatever is installed in your cluster.</p>
<p>To make a fat jar, we ensure <code>./project/assembly.sbt</code> is present with the following content:</p>
<pre class="scala"><code>addSbtPlugin(&quot;com.eed3si9n&quot; % &quot;sbt-assembly&quot; % &quot;0.14.10&quot;)</code></pre>
<p>We can then run <code>sbt assembly</code> from the project root, producing, in this case, <code>./target/scala-2.12/sedonatest-assembly-0.0.1.jar</code>. We then start <code>spark-shell</code> as follows:</p>
<pre class="bash"><code>spark-shell --jars sedonatest-assembly-0.0.1.jar</code></pre>
<p>If we wish the jar file to have a different name, <code>sedonatest.jar</code> for example, we’d add the following to <code>build.sbt</code>:</p>
<pre class="scala"><code>assemblyJarName in assembly := &quot;sedonatest.jar&quot;</code></pre>
<p>Note that if we get duplicate dependency errors when running <code>assembly</code>, then we can add something like the following to <code>build.sbt</code>:</p>
<pre class="scala"><code>assemblyMergeStrategy in assembly := {
  case PathList(&quot;META-INF&quot;, xs @ _*) =&gt; MergeStrategy.discard 
  case x =&gt; MergeStrategy.first
}</code></pre>
<p>Elastic MapReduce uses YARN as a resource manager, so we also omit the following line when creating our <code>SparkSession</code>:</p>
<pre class="scala"><code>  .master(&quot;local[*]&quot;)</code></pre>
</div>
</div>
<div id="appendix---data" class="section level1">
<h1>Appendix - Data</h1>
<p><a href="https://koordinates.com/from/datafinder.stats.govt.nz/layer/105173/">Meshblock Higher Geographies 2021 (high definition)</a> is a set of 53596 polygons covering the full extent of New Zealand. It was downloaded in ESRI shapefile format. Zipped, it weighs in at 155MB, and unzipped, 504MB. All other higher geographies maintained by Stats NZ are collections of meshblocks, and the attribute table provided includes concordances.</p>
<p><a href="https://koordinates.com/from/data.linz.govt.nz/layer/53353/">NZ Street Address</a> is a set of 2107573 address points. Zipped, the shapefile is 178MB, and unzipped it is 2.8GB. The address points are split into 3 different shapefiles when using <a href="https://koordinates.com">Koordinates</a>, which we collapsed into one post-download.</p>
<p>Note that Sedona seems to require shapefiles to be stored in a certain way. Specifically, that we put shapefiles in a folder which has the same name as the individual components. For the meshblocks, this means:</p>
<pre><code>meshblock-higher-geographies-2021-high-definition
├── meshblock-higher-geographies-2021-high-definition.cpg
├── meshblock-higher-geographies-2021-high-definition.dbf
├── meshblock-higher-geographies-2021-high-definition.prj
├── meshblock-higher-geographies-2021-high-definition.shp
├── meshblock-higher-geographies-2021-high-definition.shx
├── meshblock-higher-geographies-2021-high-definition.txt
└── meshblock-higher-geographies-2021-high-definition.xml</code></pre>
<p>Most other systems don’t require us to collect the components in a folder in this way, even if it might be sensible to organise things this way. In R, for example, we’d just refer to the <code>shp</code> component and the rest would be detected for as. For example:</p>
<pre class="r"><code>addr &lt;- sf::st_read(&quot;nz-street-address.shp&quot;)</code></pre>
<p>Note that when using Elastic MapReduce, we need to copy our input features to the Hadoop filesystem. For example:</p>
<pre class="bash"><code>hdfs dfs -copyFromLocal meshblock-higher-geographies-2021-high-definition /user/hadoop/</code></pre>
</div>

            </div>
        </div>
    </div>
    <div class="column is-3">
        <div class="card">
    <div class="card-content">
        <h1 class="title is-5">Tags</h1>
        <div class="tags">
        
            <span class="tag"><a href="https://cmhh.github.io/tags/akka-http">akka-http</a></span>
        
            <span class="tag"><a href="https://cmhh.github.io/tags/docker">docker</a></span>
        
            <span class="tag"><a href="https://cmhh.github.io/tags/geoserver">geoserver</a></span>
        
            <span class="tag"><a href="https://cmhh.github.io/tags/geospark">geospark</a></span>
        
            <span class="tag"><a href="https://cmhh.github.io/tags/nodejs">nodejs</a></span>
        
            <span class="tag"><a href="https://cmhh.github.io/tags/postgis">postgis</a></span>
        
            <span class="tag"><a href="https://cmhh.github.io/tags/postgresql">postgresql</a></span>
        
            <span class="tag"><a href="https://cmhh.github.io/tags/qgis">qgis</a></span>
        
            <span class="tag"><a href="https://cmhh.github.io/tags/r">r</a></span>
        
            <span class="tag"><a href="https://cmhh.github.io/tags/r-packages">r-packages</a></span>
        
            <span class="tag"><a href="https://cmhh.github.io/tags/seasonal-adjustment">seasonal-adjustment</a></span>
        
            <span class="tag"><a href="https://cmhh.github.io/tags/sedona">sedona</a></span>
        
            <span class="tag"><a href="https://cmhh.github.io/tags/shiny">shiny</a></span>
        
            <span class="tag"><a href="https://cmhh.github.io/tags/spark">spark</a></span>
        
            <span class="tag"><a href="https://cmhh.github.io/tags/tiles">tiles</a></span>
        
            <span class="tag"><a href="https://cmhh.github.io/tags/tilestache">tilestache</a></span>
        
            <span class="tag"><a href="https://cmhh.github.io/tags/vuejs">vuejs</a></span>
        
            <span class="tag"><a href="https://cmhh.github.io/tags/wmts">wmts</a></span>
        
            <span class="tag"><a href="https://cmhh.github.io/tags/wsl2">wsl2</a></span>
        
            <span class="tag"><a href="https://cmhh.github.io/tags/x13-arima-seats">x13-arima-seats</a></span>
        
        </div>          
    </div>
</div><br>
        <div class="card">
    <div class="card-content">
        <h1 class="title is-5">Recent posts</h1>
        
            <h1><a href="https://cmhh.github.io/post/sedona/">A Brief Look at Apache Sedona</a></h1>
            <time class="has-text-grey-light is-size-7">19 April 2021</time>
        
            <h1><a href="https://cmhh.github.io/post/dockerdev/">Working Productively on Windows Using Windows Subsystem for Linux 2 and Docker</a></h1>
            <time class="has-text-grey-light is-size-7">5 December 2020</time>
        
            <h1><a href="https://cmhh.github.io/post/rpostgis/">Using PostGIS as a Spatial Backend for R</a></h1>
            <time class="has-text-grey-light is-size-7">31 October 2020</time>
        
            <h1><a href="https://cmhh.github.io/post/docker_wsl2/">Docker on Windows with Windows Subsystem for Linux 2</a></h1>
            <time class="has-text-grey-light is-size-7">31 October 2020</time>
        
            <h1><a href="https://cmhh.github.io/post/seasadj/">Seasonal Adjustment as a Service</a></h1>
            <time class="has-text-grey-light is-size-7">1 June 2020</time>
        
    </div>
</div>
    <br>
                


    
<br>
        <div class="card">
    <div class="card-content">
        <h1 class="title is-5">Archives</h1>
        
            <a href="https://cmhh.github.io/archives/2021">2021</a> (1)<br>
        
            <a href="https://cmhh.github.io/archives/2020">2020</a> (4)<br>
        
            <a href="https://cmhh.github.io/archives/2019">2019</a> (1)<br>
        
            <a href="https://cmhh.github.io/archives/2017">2017</a> (1)<br>
        
            <a href="https://cmhh.github.io/archives/2016">2016</a> (3)<br>
        
    </div>
</div>

    </div>
</div>


    </div>
</div>

<footer class="footer has-background-grey-darker has-text-white">
    <div class="content has-text-centered">
        <p>
            <span class="icon is-large"><a href="https://github.com/cmhh" class="mysocial" rel="me"><i class="fab fa-github fa-3x"></i></a></span>&nbsp;&nbsp;
            <span class="icon is-large"><a href="mailto://cmhhansen@outlook.com" class="mysocial" rel="me"><i class="fas fa-envelope fa-3x"></i></a></span>&nbsp;&nbsp;
            <br><br>
            Copyright &copy; cmhh 2021 - Theme by <a href="https://jeffprod.com" class="mysocial">JeffProd.com</a>
        </p>
    </div>
</footer>

<script defer src="https://use.fontawesome.com/releases/v5.1.0/js/all.js"></script>
<script src="https://cmhh.github.io/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<script src="//yihui.org/js/math-code.js"></script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

</body>
</html>
