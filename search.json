[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "cmhh",
    "section": "",
    "text": "Loading spatial data to Snowflake can be a little awkward. Here we describe a simple and performant approach using the ODBC or JDBC drivers from R.\n\n\n\n\n\n\nApr 17, 2022\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThere are many ways we can deploy Shiny applications for external consumption. This post describes several possible methods.\n\n\n\n\n\n\nMar 3, 2022\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nDoes it bother you when you’re forced to fetch features via an API, instead of just downloading a file from a file server? Same! As a solution of sorts, I wondered how easily we could make a service which could used to download features directly in a requested format.\n\n\n\n\n\n\nFeb 25, 2022\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nShiny provides a low-entry option for R programmers to develop single-page web applications. But, in my opinion, it is over-used. In this post we consider where Shiny might be a good option, and where it might not.\n\n\n\n\n\n\nJan 20, 2022\n\n\n34 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this post we attempt to approximate a household survey collection as an agent-based simulation. The simulation designed using the actor model, and implemented in Scala using Akka.\n\n\n\n\n\n\nJan 9, 2022\n\n\n20 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nSometimes creating a bespoke data service is easy enough. But maybe sometimes it’s just easier to expose data that already exists in a database. Here we consider a couple of options, PostGraphile and PostGREST, for doing just that.\n\n\n\n\n\n\nAug 14, 2021\n\n\n15 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImagine an online learning scenario where we wish to update an existing model using streaming event data, say. Here we consider how we might approach such a problem using the actor model as implemented by Akka.\n\n\n\n\n\n\nAug 8, 2021\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImagine an online learning scenario where we wish to update an existing model using streaming event data, say. Here we consider how we might approach such a problem using Apache Kafka.\n\n\n\n\n\n\nJul 30, 2021\n\n\n13 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2021\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2020\n\n\n13 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nHere we look at how we can leverage PostgreSQL and the PostGIS extension to usefully complement R, mostly by offloading large geoprocessing tasks, and as a library for storing large feature classes for shared use across a potentially large userbase.\n\n\n\n\n\n\nOct 31, 2020\n\n\n13 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2020\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nX13-ARIMA-SEATS is a widely used seasonal adjustment program developed by the U.S. Census Bureau. Here we look at how one might go wrapping this program as a web service for remote execution.\n\n\n\n\n\n\nJun 1, 2020\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nSending large vector features over-the-wire for use in slippy maps is bad, m’kay. One possible solution is to appeal to a tile service or similar, and here we look at how we might leverage the GeoServer API to enable end-users to quickly deploy their own tile sets on-the-fly.\n\n\n\n\n\n\nSep 17, 2019\n\n\n14 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this post we look at how we can use QGIS Server to publish a range of services using WMS, WFS and WCS.\n\n\n\n\n\n\nJan 26, 2017\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nWhen making maps for display on the web, the size of the various objects being rendered can quickly become overwhelming. One solution is to appeal to a tile service, and here we consider one approach using Mapnik and TileStache.\n\n\n\n\n\n\nNov 28, 2016\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nI often find myself needing to establish the travel time or distance between arrays of addresses from R. Here we describe how we can use a local install of Open Source Routing Machine as a solution which is highly performant, and relatively easy to implement.\n\n\n\n\n\n\nNov 27, 2016\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/online-learning-with-kafka/index.html",
    "href": "posts/online-learning-with-kafka/index.html",
    "title": "Online Learning with Apache Kafka",
    "section": "",
    "text": "I recently came across a scenario at work that would, like so many things in practice, be particularly well modelled as an ordered set of events. The messages would have a large number of uses in practice, but one would involve using them to train some sort of predictive model. Moreover, the messages would be arriving in real-time, and the patterns underpinning the messages might well change over time. Based on previous research, Kafka seemed like it would be a good fit, though I don’t yet have a huge amount of practical experience using it. So I decided to create a simplified version of my real use case, and then write it up as a post. We’ll use a producer to read messages in a topic, and a consumer will then read those messages and update our model. If nothing else, the exercise should also serve as a reasonably accessible example of the use of Kafka producers and consumers. Let’s get to it…"
  },
  {
    "objectID": "posts/online-learning-with-kafka/index.html#install-and-start-kafka",
    "href": "posts/online-learning-with-kafka/index.html#install-and-start-kafka",
    "title": "Online Learning with Apache Kafka",
    "section": "Install and Start Kafka",
    "text": "Install and Start Kafka\nEven though all the source for this exercise is provided as an sbt project, we still need a running broker. There are a number of ways we can do this, but it will suffice to simply download a pre-compiled bundle from the Kafka website directly. At the time of writing, the most recent version of Kafka is 2.8.0, and it can be downloaded via the following link:\nKafka 2.8.0 for Scala 2.13\nI just downloaded this to the root of the project folder, and unpacked it there, the result being a folder name kafka_2.13-2.8.0. We first start Apache Zookeeper as follows:\n./kafka_2.13-2.8.0/bin/zookeeper-server-start.sh \\\n  ./kafka_2.13-2.8.0/config/zookeeper.properties\nKafka is distributed, and Zookeeper is used to coordinate the nodes. Note that this function will soon be handled natively by Kafka so will not be a requirement much longer. Either way, we then start our Kafka broker by running:\n./kafka_2.13-2.8.0/bin/kafka-server-start.sh \\\n  ./kafka_2.13-2.8.0/config/server.properties\nKafdrop is a useful web UI which can be used to monitor Kafka brokers, and we can get this up and running quickly via Docker as follows:\ndocker run -d --rm \\\n  --net host \\\n  -e KAFKA_BROKERCONNECT=localhost:9092 \\\n  -e JVM_OPTS=\"-Xms32M -Xmx64M\" \\\n  -e SERVER_SERVLET_CONTEXTPATH=\"/\" \\\n  obsidiandynamics/kafdrop\nThe default port is 9000, so we then just visit localhost:9000 to see the following UI:"
  },
  {
    "objectID": "posts/online-learning-with-kafka/index.html#create-a-kafka-topic",
    "href": "posts/online-learning-with-kafka/index.html#create-a-kafka-topic",
    "title": "Online Learning with Apache Kafka",
    "section": "Create a Kafka Topic",
    "text": "Create a Kafka Topic\nHere, our events or messages will just be instances of the MnistRecord case class, so we need to create a topic to hold them. We can do this at the terminal, assuming our Kafka broker is running, as follows:\n./kafka_2.13-2.8.0/bin/kafka-topics.sh \\\n  --bootstrap-server localhost:9092 \\\n  --create --partitions 1 --replication-factor 1 \\\n  --topic mnist\nTopics can also be added and deleted easily enough via the Kafdrop web UI."
  },
  {
    "objectID": "posts/online-learning-with-kafka/index.html#model-our-messages",
    "href": "posts/online-learning-with-kafka/index.html#model-our-messages",
    "title": "Online Learning with Apache Kafka",
    "section": "Model Our Messages",
    "text": "Model Our Messages\nThe MNIST database consists of handwritten images of 28x28 pixels, each assigned a label from 0 to 9. The image data is easily read from the provided files as an array of bytes in row-major order, so we define a simple case class as follows (the class has a number of methods, but we omit the details here):\ncase class Image(data: Array[Byte], rows: Int, cols: Int, byrow: Boolean)\nNote that the byrow parameter is included since I also tested this code with the EMNIST database. This is a database which is intended to largely be a drop-in replacement for MNIST, but with a larger number of categories. However, while the IDX format is used for both, EMNIST appears to store pixels in column-major order, whereas MNIST is in row-major order. Either way, the labelled images can then be modelled simply, also using a case class, as follows:\ncase class MnistRecord(image: Image, label: Int)\nThe MNIST data has been bundled as a resource in the provided sbt project, and a basic, single-use iterator is provided also. For example, to read the training data:\n\nscala> val testIt = MnistIterator(MnistType.TRAIN)\nval testIt: org.cmhh.MnistIterator = <iterator>"
  },
  {
    "objectID": "posts/online-learning-with-kafka/index.html#serialisation-deserialisation",
    "href": "posts/online-learning-with-kafka/index.html#serialisation-deserialisation",
    "title": "Online Learning with Apache Kafka",
    "section": "Serialisation / Deserialisation",
    "text": "Serialisation / Deserialisation\nMessages are transmitted and stored as sequences of bytes, and so we need the ability to convert our messages to byte arrays (serialisation) and back again (deserialisation). The MnistRecord type is relatively simple–our images are already stored as byte arrays, and we just need to append one more byte to account for the image label.\nTo create a serialiser we mix in the Serializer[T] trait, and we must provide a concrete implementation of method serialize with signature:\ndef serialize(topic: String, data: T): Array[Byte]\nTo create a deserialiser we mix in the Deserialzer[T] trait, and we must provide a concrete implemenation of method deserialize with signature:\ndef deserialize(topic: String, data: Array[Byte]): T\nWe can define everything together in one class, though one needs to take care since the two traits have methods with the same name, so we need to override things just so. The full implementation in this case is:\nimport org.apache.kafka.common.serialization.{Serializer, Deserializer, Serde, Serdes}\n\nclass MnistRecordSerde() extends Serde[MnistRecord] \nwith Serializer[MnistRecord] with Deserializer[MnistRecord] with Serializable {\n  override def serializer(): Serializer[MnistRecord] = this\n  override def deserializer(): Deserializer[MnistRecord] = this\n  override def close(): Unit = ()\n  override def configure(configs: java.util.Map[String, _], isKey: Boolean): Unit = ()\n  val Serde: Serde[MnistRecord] = Serdes.serdeFrom(this, this)\n\n  override def serialize(topic: String, data: MnistRecord): Array[Byte] = \n    data.image.data :+ data.label.toByte\n\n  override def deserialize(topic: String, data: Array[Byte]): MnistRecord = {\n    val m = data.size - 1\n    val n = math.sqrt(m).toInt\n    if (n * n != m) sys.error(\"oops.\")\n    MnistRecord(\n      Image(data.take(m), n, n, true),\n      data(m).toInt\n    )\n  }\n}"
  },
  {
    "objectID": "posts/online-learning-with-kafka/index.html#producer",
    "href": "posts/online-learning-with-kafka/index.html#producer",
    "title": "Online Learning with Apache Kafka",
    "section": "Producer",
    "text": "Producer\nOur producer is very simple. In this case it is a standard driver program, and the complete program is as follows:\npackage org.cmhh\n\nimport org.apache.kafka.clients.producer._\nimport org.apache.kafka.common.serialization._\nimport scala.jdk.CollectionConverters._\nimport scala.concurrent.duration.Duration\nimport scala.concurrent.{Await, Future}\nimport scala.concurrent.ExecutionContext.Implicits.global\nimport scala.util.{Success, Failure}\nimport java.util.Properties\n\nobject ProducerApp extends App {\n  val delay = if (args.size > 0) args(0).toInt else 0\n  val trainIt: MnistIterator = MnistIterator(MnistType.TRAIN)\n  val n = trainIt.size\n\n  val props = new Properties()\n  props.put(\"bootstrap.servers\", \"localhost:9092\")\n  props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\n  props.put(\"value.serializer\", \"org.cmhh.MnistRecordSerde\")\n\n  val producer = new KafkaProducer[String, MnistRecord](props)\n\n  print(\"\\n\\nSending messages...\\n\\n0.0%...\")\n\n  val res = Future.sequence(trainIt.zipWithIndex.map(rec => {\n    val msg = new ProducerRecord[String, MnistRecord](\n      \"mnist\", \n      s\"%05d\".format(rec._2), \n      rec._1\n    )\n\n    if ((rec._2 + 1) % (n / 10) == 0) \n      print(s\"${(rec._2 + 1).toDouble / n * 100}%... \")\n      \n    if (delay > 0) Thread.sleep(delay)\n    Future { producer.send(msg).get }\n  }))\n\n  Await.ready(res, Duration.Inf)\n\n  print(\"\\n\\nDone.\\n\\n\")\n}\nWe first create an instance of KafkaProducer, which is responsible for publishing our messages. This client object requires a configuration in order to be initialised, in this case an instance of Properties. Our properties include the address of our broker, as well as the serialisation classes for our keys and for our messages. In this case, our message keys will be the strings 00001 through 60000, and they’ll be serialised via StringSerializer. Our messages are, of course, MnistRecord, and they’ll be serialised using the custom serialiser MnistRecordSerde. Otherwise, the driver program simply iterates over the training set, sending each, and then waiting some amount of time in milliseconds before moving on to the next, and we are notified on standard output how far we are through the full set in 10 percentage point increments. Messages are published by calling the send method of our KafkaProducer. This is asynchronous, and returns a Java-style Future, which we then convert to a Scala Future for slightly more idiomatic use."
  },
  {
    "objectID": "posts/online-learning-with-kafka/index.html#consumer",
    "href": "posts/online-learning-with-kafka/index.html#consumer",
    "title": "Online Learning with Apache Kafka",
    "section": "Consumer",
    "text": "Consumer\nOur consumer is a little more complicated. First, we initialise a simple neural network using the Deeplearning4j library. The complete setup is as follows:\ndef cnn(learningRate: Double = 0.01, seed: Int = 1234): MultiLayerNetwork = {\n  val architecture = new NeuralNetConfiguration.Builder()\n    .seed(seed)\n    .updater(new Sgd(learningRate)) \n    .weightInit(WeightInit.XAVIER)\n    .l2(1e-4)\n    .list()\n    .layer(0, new ConvolutionLayer.Builder(3, 3)\n      .stride(2,2)\n      .padding(1,1)\n      .nOut(32)\n      .activation(Activation.RELU)\n      .build()\n    )\n    .layer(1, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)\n      .kernelSize(2,2)\n      .stride(1,1)\n      .build()\n    )\n    .layer(2, new DenseLayer.Builder()\n      .nOut(100)\n      .activation(Activation.RELU)\n      .build()\n    )\n    .layer(3, new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT)\n      .nOut(10)\n      .activation(Activation.SOFTMAX)\n      .build()\n    )\n    .setInputType(InputType.convolutionalFlat(28, 28, 1))\n    .build\n\n  val network = new MultiLayerNetwork(architecture)\n  network.init()\n  network\n}\nWe’re not aiming for best-in-breed here–just something that we can train reasonably quickly, and which shows increasing accuracy as more of the topic is consumed. The full consumer application then looks as follows:\npackage org.cmhh\n\nimport org.apache.kafka.clients.consumer._\nimport org.apache.kafka.common.serialization._\nimport scala.jdk.CollectionConverters._\nimport scala.concurrent.duration.Duration\nimport scala.concurrent.{Await, Future}\nimport scala.concurrent.ExecutionContext.Implicits.global\nimport scala.util.{Success, Failure}\nimport java.util.Properties\nimport java.time.{Duration => JDuration}\nimport org.nd4j.evaluation.classification.Evaluation\n\nobject ConsumerApp extends App {\n  val t = new Thread {\n    override def run: Unit = try {\n      val props = new Properties()\n      props.put(\"bootstrap.servers\", \"localhost:9092\")\n      props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\")\n      props.put(\"value.deserializer\", \"org.cmhh.MnistRecordSerde\")\n      props.put(\"group.id\", \"mnist-train\")\n      props.put(\"auto.offset.reset\", \"earliest\")\n      props.put(\"enable.auto.commit\", false)\n\n      val consumer = new KafkaConsumer[String, MnistRecord](props)\n      consumer.subscribe(java.util.Collections.singletonList(\"mnist\"))\n      \n      val network = model.ann() \n      val mnistTest = new MnistDataSetIterator(100, MnistType.TEST)\n\n      while (true) {\n        val recs = consumer.poll(JDuration.ofMillis(1000))\n        if (recs.count() > 0) {\n          val it = recs.iterator\n          while(it.hasNext) { \n            val im = it.next().value.toNd4j\n            network.fit(im._1, im._2)\n          }\n\n          val eval = network.evaluate[Evaluation](mnistTest)\n          println(\n            f\"number read: %%d, accuracy: %%1.4f\".format(recs.count(), eval.accuracy)\n          )\n        }\n      }\n    } catch {\n        case e: Throwable =>\n          println(\"Program terminated.\\n\\n\")\n          println(e.getMessage())\n    }\n  }\n\n  t.start\n  scala.io.StdIn.readLine(\"\\n\\nPress ENTER to stop...\\n\\n\")\n  t.interrupt()\n  t.join()\n}\nTo retrieve messages from a topic, we need a KafkaConsumer object. We pass a basic configuration on instantiation which includes the address of our Kafka broker, the deserialisers for our keys and messages (which are of type MnistRecord), and we also ensure we read the topic from the start. Once the KafkaConsumer, consumer, is created, we subscribe to the mnist topic. Then, we poll the topic continuously for 1 second at a time, and use all the messages retrieved in that time to update our neural network. At the end of each loop, we report how many cases were retrieved, and how the model accuracy has changed as a result. We place the whole loop on a separate thread, mostly so we can wait for users to terminate the program by pressing ENTER.\nNote that Deeplearning4j does provide iterators for the MNIST database, but here we’ve rolled our own simple version. In part this is because Deeplearning4j is but one choice we could have made for the machine learning component, but also because it’s instructive to see how one goes about creating such iterators using the traits provided–in useful real-world scenarios, our data won’t be pre-bundled, and we’ll have no choice.\nNote also that while our neural network is simple enough, we run our test set through the model frequently, and without a GPU back-end this will be quite slow. On my machine I have an NVIDIA GeForce GTX 1650 Mobile / Max-Q GPU with 4GB of memory, and an Intel i7-9750H (12) @ 4.500GHz CPU. The training set of 10,000 images takes around 100 seconds to evaluate using the CPU back-end, but about 1.3 seconds using the GPU. At the very least one should probably change the settings so more messages are consumed on each iteration if using CPU–or simply don’t evaluate the test accuracy at all."
  },
  {
    "objectID": "posts/online-learning-with-kafka/index.html#results",
    "href": "posts/online-learning-with-kafka/index.html#results",
    "title": "Online Learning with Apache Kafka",
    "section": "Results",
    "text": "Results\nTo demonstrate the application, we start the consumer and producer applications each in separate terminals. In the first case, we send a new record to our topic every 10 milliseconds or so. Once the topic has records in it, our consumer starts printing out the learning progress. The rate at which we add new messages dictates our consumer processes about 20 or so new images for each 100 millisecond iteration.\n\nIf we just load all the images into our topic as fast as we can, we find our consumer processes 500 records for every 100 millisecond iteration. This is because the consumer configuration parameter max.poll.records has a default value of 500, and we have not changed it."
  },
  {
    "objectID": "posts/online-learning-with-kafka/index.html#summary",
    "href": "posts/online-learning-with-kafka/index.html#summary",
    "title": "Online Learning with Apache Kafka",
    "section": "Summary",
    "text": "Summary\nIn this post we demonstrated the basic usage of a producers and consumers when working with Kafka, including important considerations such as serialisation and deserialisation of our message types. The specific use-case for our consumer here, while not a great example of a useful real-world scenario, was to continuously update a classifier in real-time as new messages were received. There are a number of extensions that one could consider to make this more interesting, of course–we could develop web interfaces for creating new messages and append them to our topic, we could serialise our classifier for offline use, and so on. In an upcoming post we’ll replicate most of what was done in this post, but using Akka actors rather than Kafka."
  },
  {
    "objectID": "posts/seasonal-adjustment-as-a-service/index.html",
    "href": "posts/seasonal-adjustment-as-a-service/index.html",
    "title": "Seasonal Adjustment as a Service",
    "section": "",
    "text": "A Simple Example…\nThe X13 binary ships with a relatively simple example using the famous Box and Jenkins air passengers data. That is, a simple specification is provided in a file called testairline.spc:\nseries{\n  title=\"International Airline Passengers Data from Box and Jenkins\"\n  start=1949.01\n  data=(\n    112 118 132 129 121 135 148 148 136 119 104 118\n    115 126 141 135 125 149 170 170 158 133 114 140\n    145 150 178 163 172 178 199 199 184 162 146 166\n    171 180 193 181 183 218 230 242 209 191 172 194\n    196 196 236 235 229 243 264 272 237 211 180 201\n    204 188 235 227 234 264 302 293 259 229 203 229\n    242 233 267 269 270 315 364 347 312 274 237 278\n    284 277 317 313 318 374 413 405 355 306 271 306\n    315 301 356 348 355 422 465 467 404 347 305 336\n    340 318 362 348 363 435 491 505 404 359 310 337\n    360 342 406 396 420 472 548 559 463 407 362 405\n    417 391 419 461 472 535 622 606 508 461 390 432)\n  span=(1952.01, )\n}\nspectrum{\n  savelog=peaks \n}\ntransform{\n  function=auto\n  savelog=autotransform  \n}\nregression{\n  aictest=(td easter)\n  savelog=aictest  \n}\nautomdl{  \n  savelog=automodel  \n}\noutlier{ }\nx11{}\nOne could adjust this at the command line by running something like (there are a number of ways the program can be invoked):\nx13ashtml testairline -g g\nAdding the g flag with value g tells X13 we wish to save output to subdirectory g, and we will end up with a number of text files matching the pattern g/testairline.*. For example, a range of diagnostics will be written as key-value pairs in a file called g/testairline.udg, the first few lines of which would look similar to:\ndate: Jun  1, 2020  \ntime: 14.19.08 \nversion: 1.1\nbuild: 39\noutput: html\nsrstit: International Airline Passengers Data from Box and Jenkins\nsrsnam: testairline                                                     \nfreq:    12\nspan:  1st month,1952 to 12th month,1960\nnobs:   108\nconstant:     0.0000000000E+00\ntransform: Automatic selection\n...\nThe seasonally adjusted and trend series would be saved as g/testairline.d11 and g/tsairline.d12, respectively, and the first few rows of the D11 table would look as follows:\ndate    testairline.d11\n------  -----------------------\n195201  +0.192925972176234E+03\n195202  +0.198588812698691E+03\n195203  +0.185791217572829E+03\n195204  +0.185841707777349E+03\n195205  +0.186173988743725E+03\n195206  +0.192599054707605E+03\n195207  +0.187601439031062E+03\n195208  +0.200184926772685E+03\n...\n\n\n…as a Service\nI created a basic library which can be used to run X13 programmatically on the JVM (the library is written in Scala), and also as a web service (the web service itself is written using Akka HTTP). The above test specification would be converted to JSON in somewhat obvious fashion as follows:\n{\n  \"airpassengers\":{\n    \"series\":{\n      \"title\":\"International Airline Passengers Data from Box and Jenkins\",\n      \"start\":\"1949.01\",\n      \"data\":[\n        112, 118, 132, 129, 121, 135, 148, 148, 136, 119, 104, 118,\n        115, 126, 141, 135, 125, 149, 170, 170, 158, 133, 114, 140,\n        145, 150, 178, 163, 172, 178, 199, 199, 184, 162, 146, 166,\n        171, 180, 193, 181, 183, 218, 230, 242, 209, 191, 172, 194,\n        196, 196, 236, 235, 229, 243, 264, 272, 237, 211, 180, 201,\n        204, 188, 235, 227, 234, 264, 302, 293, 259, 229, 203, 229,\n        242, 233, 267, 269, 270, 315, 364, 347, 312, 274, 237, 278,\n        284, 277, 317, 313, 318, 374, 413, 405, 355, 306, 271, 306,\n        315, 301, 356, 348, 355, 422, 465, 467, 404, 347, 305, 336,\n        340, 318, 362, 348, 363, 435, 491, 505, 404, 359, 310, 337,\n        360, 342, 406, 396, 420, 472, 548, 559, 463, 407, 362, 405,\n        417, 391, 419, 461, 472, 535, 622, 606, 508, 461, 390, 432\n      ],\n      \"span\":[\"1952.01\", null]\n    },\n    \"spectrum\":{\n      \"savelog\":\"peaks\"\n    },\n    \"transform\":{\n      \"function\":\"auto\",\n      \"savelog\":\"autotransform\"\n    },\n    \"regression\":{\n      \"aictest\":[\"td\", \"easter\"],\n      \"savelog\":\"aictest\"  \n    },\n    \"automdl\":{\n      \"savelog\":\"automodel\"\n    },\n    \"outlier\":null,\n    \"x11\":null\n  }\n}\nIf this was saved as testairline.json, and a service instance was running at localhost:9001, then the adjustment could be run via curl, say, as follows:\ncurl \\\n  -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -d @testairline.json \\\n  localhost:9001/seasadj/adjust \\\n  --compressed --output testairline_output.json\nThe resulting JSON file contains most of the X13 output where files that can be imported as time series are saved in an array called series, and the diagnostics are parsed into an array called diagnostics.\n\nNote that more than a single series can be included in a single JSON file / string in a fairly obvious way, and the use of composites is permitted. Moreover, because the full set of results can get somewhat large, it is possible to edit the results. For example, if we only wished to keep the seasonally adjusted and trend time series output we could append the query parameter save=sa,trn; and if we wanted to suppress all the dates and just include the start date and frequency we could append allDates=false.\n\n\nA Basic JavaScript ‘Client’\nOf course, once we have a web service, we can call it from any language or tool that has the ability to POST and parse JSON, which is pretty much everything. Client libraries could then be written for various target languages, if required, allowing users to use the service easily, and in an idiomatic way. One fun example involves calling the service in the browser using JavaScript. I like the idea of making component libraries using a framework such as Vue.js, and this could be done here. In the following example we create a simple Vue.js component which knows how to fetch an unadjusted time series by ID using a service I created earlier, adjust the result, and then populate a highchart:\n\n\n\n\n\n\n\nFor those interested, the full source of this ‘application’ is as follows:\n<script src=\"https://unpkg.com/axios/dist/axios.min.js\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/vue@2.6.11\"></script>\n<script src=\"https://code.highcharts.com/highcharts.src.js\"></script>\n\n<div id=app></div>\n<script>\nVue.component('snzseasadjchart', {\n  props: {\n    name: {required: true},\n    code: { required: true }\n  },\n  data: function() {\n    return {\n      tsdata: {}\n    }\n  },\n  mounted() {\n    this.$_update(this.code);\n  },\n  watch: {\n    code() {\n      this.$_update(this.code);\n    },\n    tsdata() {\n      Highcharts.chart(this.name, {\n        title: {\n          text: this.tsdata.title,\n          useHTML: true\n        },\n        yAxis: {\n          title: {\n            text: this.tsdata.units\n          }\n        },\n        xAxis: {\n          categories: this.tsdata.period\n        },\n        series: [\n          {\n            name: \"unadjusted\",\n            data: this.tsdata.ori\n          },\n          {\n            name: \"seasonally adjusted\",\n            data: this.tsdata.sa\n          },\n          {\n            name: \"trend\",\n            data: this.tsdata.trn\n          }\n        ],\n        chart: {\n          type: 'line',\n          zoomType: 'xy',\n          height: null\n        },\n        plotOptions: {\n          series: {\n            allowPointSelect: true,\n            marker: {\n              enabled: false\n            }\n          }\n        },\n        credits: {\n          enabled: true,\n          href: \"https://www.stats.govt.nz/large-datasets/csv-files-for-download/\",\n          text: \"Stats NZ\"\n        }\n      });\n    }\n  },\n  methods: {\n    $_update(seriesCode) {\n      let context = this;\n      axios\n        .get(`https://cmhh.hopto.org/snzts/v1/series?format=json&seriesCode=${this.code}`)\n        .then(response => {\n          let actual = response.data[0];\n          let title = actual.outcomes.join(\", \");\n          let start = actual.period[0];\n          let data = actual.value.toString();\n          let period = 12 / actual.interval;\n          let units = actual.units;\n          let magnitude = actual.magnitude;\n          let request = \n            `{\"x\":{\"series\":{\"title\":\"${title}\",\"start\":\"${start}\",` +\n            `\"period\":${period},\"data\":[${data}]},\"x11\":null}}`;\n          axios\n            .post(\"https://cmhh.hopto.org/seasadj/adjust?save=ori,sa,trn\", request)\n            .then(response => {\n              let res = response.data.x;\n              context.tsdata = {\n                title:title,\n                units:units,\n                magnitude:magnitude,\n                period:res.series.ori.date,\n                ori:res.series.ori.value,\n                sa:res.series.sa.value,\n                trn:res.series.trn.value\n              };\n            });\n        });\n    }\n  },\n  template: `<div class=\"snztschart\" v-bind:id=\"name\"></div>`\n});\n</script>\n<script>\nlet app = new Vue({\n  el: '#app',\n  data() {\n    return {\n      code: \"HLFQ.SAA3AZ\"\n    }\n  },\n  template:`\n    <div>\n      <label for=\"cars\">Choose a series:</label>\n      <select v-model=\"code\">\n        <option value=\"HLFQ.SAA3AZ\" selected>total employed</option>\n        <option value=\"HLFQ.SAB3AZ\">total unemployed</option>\n        <option value=\"HLFQ.SAC3AZ\">not in labour force</option>\n        <option value=\"HLFQ.SAE3AZ\">labour force participation rate</option>\n        <option value=\"HLFQ.SAH3AZ\">employment rate</option>\n        <option value=\"HLFQ.SAF3AZ\">unemployment rate</option>\n        <option value=\"HLFQ.SAD3AZ\">working-age population</option>\n      </select>\n      <hr>\n      <snzseasadjchart v-bind:code=\"code\" name=ts02></snzseasadjchart>\n    </div>\n  `\n});\n</script>"
  },
  {
    "objectID": "posts/building-tile-services-on-the-fly-with-geoserver/index.html",
    "href": "posts/building-tile-services-on-the-fly-with-geoserver/index.html",
    "title": "Building Tile Services on-the-fly with GeoServer",
    "section": "",
    "text": "Interactive web applications can be a great way to present information to a diverse audience. If your interface is well designed and intuitive, then almost anybody will be able to use it. And where data has a spatial dimension, slippy maps can be an effective way of presenting information. But sometimes the objects we want to render are a bit on the large side. This means that our web browser can struggle to render all the required features, and it also means that mobile users of an app might end up using excessive amounts of data.\nA good solution to the problem, that the spatial features we need to display are large, is to use some sort of tile service. Slippy maps can, for the most part, fetch information from a tile service only for the active extent. So, here we look at using GeoServer as a means of publishing such services. We put special emphasis on ease of use and deployment, so cover the following:\n\nthe use of Docker to simplify the installation and deployment of GeoServer itself\nthe use of GeoServer’s REST API\nwrapping calls to GeoServer’s REST API to simplify a number of common use cases\n\nSpecifically, we’ll demonstrate how we can create new tile services on-the-fly entirely within R (though we could use any language capable of making HTTP calls and manipulating geospatial data–Python with geopandas, for example).\nVideo\nNote that there is already an R package, geosapi, that provides what looks like a largely complete wrapper for the GeoServer REST API. I wasn’t aware of it when I put this together. The use case here is a bit different, but we nevertheless could possibly have made use of the geosapi package."
  },
  {
    "objectID": "posts/building-tile-services-on-the-fly-with-geoserver/index.html#source-data",
    "href": "posts/building-tile-services-on-the-fly-with-geoserver/index.html#source-data",
    "title": "Building Tile Services on-the-fly with GeoServer",
    "section": "Source data",
    "text": "Source data\nThroughout this document, we use spatial features sourced from Stats NZ geographic data service. Specifically:\n\nMeshblock 2018 Clipped (generalised)\nStatistical Area 1 2018 (genaralised)\nStatistical Area 2 2018 (generalised)\nTerritorial Authority 2018 Clipped (generalised)\nRegional Council 2018 Clipped (generalised)\n\nIn addition, we also combine this with contextual information found on NZ.Stat:\n\nSubnational population estimates (RC, SA2), by age and sex, at 30 June 1996, 2001, 2006-18 (2018 boundaries)\nSubnational population estimates (TA, SA2), by age and sex, at 30 June 1996, 2001, 2006-18 (2018 boundaries)"
  },
  {
    "objectID": "posts/building-tile-services-on-the-fly-with-geoserver/index.html#installation",
    "href": "posts/building-tile-services-on-the-fly-with-geoserver/index.html#installation",
    "title": "Building Tile Services on-the-fly with GeoServer",
    "section": "Installation",
    "text": "Installation\nTo install the R package used throughout this document, simply run:\ndevtools::install_github(\"cmhh/geoserveR\")\nThe R package requires a running instance of GeoServer to be useful. We can get GeoServer up and running easily using Docker. There is no shortage of existing images online, but we’ll build our own. We do this because it’s pretty straight forward, and because it’s instructive. The R package contains a Dockerfile in docker/Dockerfile as follows:\nFROM ubuntu:18.04\n\nENV DEBIAN_FRONTEND noninteractive\n\nENV GEOSERVER_VERSION 2.15.2\n\nRUN apt-get update && \\\n  apt-get install -y \\\n    ca-certificates openssl wget openjdk-8-jre \\\n    openssh-server openssh-client unzip && \\\n  apt-get clean && \\\n  update-ca-certificates && \\\n  cd /usr/local && \\\n  wget http://sourceforge.net/projects/geoserver/files/GeoServer/${GEOSERVER_VERSION}/geoserver-${GEOSERVER_VERSION}-bin.zip && \\\n  unzip geoserver-${GEOSERVER_VERSION}-bin.zip && \\\n  mv geoserver-${GEOSERVER_VERSION} geoserver && \\\n  rm geoserver-${GEOSERVER_VERSION}-bin.zip\n\nENV GEOSERVER_HOME /usr/local/geoserver\n\nENV GEOSERVER_DATA_DIR ${GEOSERVER_HOME}/data_dir\n\nEXPOSE 8080\n\nCMD [\"sh\", \"/usr/local/geoserver/bin/startup.sh\"]\nThe R package can be used to generate the command required to build the image since the location of Dockerfile will vary depending on the user’s setup. For example:\n> geoserveR::docker_build(tag = 'geoserver')\ndocker build -t geoserver \"/home/cmhh/R/x86_64-pc-linux-gnu-library/3.5/geoserveR/docker\"\nRunning the resulting command at the terminal will result in the creation of an image called, in this case, geoserver. We can start this via:\n$ docker run -d --name geoserver --rm -p 8080:8080 geoserver\nThe container will take a short while to get up and running completely, but when done, simply open a browser at http://localhost:8080/geoserver. To login, the username is admin, and the password is geoserver (obviously, we’d configure this differently in a production setting… more on that later). And that’s it."
  },
  {
    "objectID": "posts/building-tile-services-on-the-fly-with-geoserver/index.html#testing-geoserver",
    "href": "posts/building-tile-services-on-the-fly-with-geoserver/index.html#testing-geoserver",
    "title": "Building Tile Services on-the-fly with GeoServer",
    "section": "Testing GeoServer",
    "text": "Testing GeoServer\nGeoServer includes several data sources by default, and we can use these to test out our install. We’re mostly interested in WMS services which we can display on a leaflet map, though GeoServer can display WCS, WFS, WMS, and WMTS.\nBroadly speaking, GeoServer groups sets of layers together in workspaces, where the layers are the features we’re intrested in displaying on a map. And the layers themselves are built up from data stores. We can inspect workspaces in the main web interface by clicking the Data > Workpaces link:\n\nAny of the listed workspaces can then be accessed as WMS services programmatically via http://localhost/geoserver/<workspace>/wms? (and an XML file describing the service can be accessed via http://localhost/geoserver/<workspace>/wms?request=GetCapabilities). For example, if we wanted to display layers that are part of the topp workspace, we’d use http://localhost:8080/geoserver/topp/wms?. If we click on Data > Layers in the main web interface we see that there is, for example, a layer in the topp workspace called states:\n\nWe can display this in a leaflet map in R by running the following code:\nlibrary(leaflet)\n\nleaflet() %>%\n  addTiles(\n    urlTemplate = \"https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\"\n  ) %>%\n  leaflet::fitBounds(-125, 25, -67, 50) %>%\n  addWMSTiles(\n    \"http://localhost:8080/geoserver/topp/wms?\", \n    layers = \"states\",\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE)\n  )\nwhich yields the following:"
  },
  {
    "objectID": "posts/building-tile-services-on-the-fly-with-geoserver/index.html#an-r-client-library",
    "href": "posts/building-tile-services-on-the-fly-with-geoserver/index.html#an-r-client-library",
    "title": "Building Tile Services on-the-fly with GeoServer",
    "section": "An R client library",
    "text": "An R client library\nOur use case is to create styled tiles services, entirely from within R. To simplify the discussion, a simple R package is provided which:\n\nincludes geographies (as sf objects)\nincludes population counts (as a tidy data.frame)\nincludes an interface for (a subset of) the GeoServer REST API\nincludes utilities that simplify working with the GeoServer interface.\n\nWe won’t go into the details here, but rather just describe the relevant usage as it arises. Where possible, though, we will also consider the plain HTTP calls. But by way of example, the following lists the layers available in the topp workspace. First, using the REST API (via curl):\n$ curl -s -u admin:geoserver -X GET \\\n    http://localhost:8080/geoserver/rest/workspaces/topp/layers | jq '.'\n{\n  \"layers\": {\n    \"layer\": [\n      {\n        \"name\": \"states\",\n        \"href\": \"http://localhost:8080/geoserver/rest/workspaces/topp/layers/states.json\"\n      },\n      {\n        \"name\": \"tasmania_cities\",\n        \"href\": \"http://localhost:8080/geoserver/rest/workspaces/topp/layers/tasmania_cities.json\"\n      },\n      {\n        \"name\": \"tasmania_roads\",\n        \"href\": \"http://localhost:8080/geoserver/rest/workspaces/topp/layers/tasmania_roads.json\"\n      },\n      {\n        \"name\": \"tasmania_state_boundaries\",\n        \"href\": \"http://localhost:8080/geoserver/rest/workspaces/topp/layers/tasmania_state_boundaries.json\"\n      },\n      {\n        \"name\": \"tasmania_water_bodies\",\n        \"href\": \"http://localhost:8080/geoserver/rest/workspaces/topp/layers/tasmania_water_bodies.json\"\n      }\n    ]\n  }\n}\nand then using the provided R package:\nlibrary(geoserveR)\n\ngs <- GeoServer$new()\ntopp_layers <- gs$getLayers(\"topp\")\nstr(topp_layers)\nList of 5\n $ states                   :List of 2\n  ..$ name: chr \"states\"\n  ..$ href: chr \"http://localhost:8080/geoserver/rest/workspaces/topp/layers/states.json\"\n $ tasmania_cities          :List of 2\n  ..$ name: chr \"tasmania_cities\"\n  ..$ href: chr \"http://localhost:8080/geoserver/rest/workspaces/topp/layers/tasmania_cities.json\"\n $ tasmania_roads           :List of 2\n  ..$ name: chr \"tasmania_roads\"\n  ..$ href: chr \"http://localhost:8080/geoserver/rest/workspaces/topp/layers/tasmania_roads.json\"\n $ tasmania_state_boundaries:List of 2\n  ..$ name: chr \"tasmania_state_boundaries\"\n  ..$ href: chr \"http://localhost:8080/geoserver/rest/workspaces/topp/layers/tasmania_state_boundaries.json\"\n $ tasmania_water_bodies    :List of 2\n  ..$ name: chr \"tasmania_water_bodies\"\n  ..$ href: chr \"http://localhost:8080/geoserver/rest/workspaces/topp/layers/tasmania_water_bodies.json\""
  },
  {
    "objectID": "posts/building-tile-services-on-the-fly-with-geoserver/index.html#making-a-new-layer",
    "href": "posts/building-tile-services-on-the-fly-with-geoserver/index.html#making-a-new-layer",
    "title": "Building Tile Services on-the-fly with GeoServer",
    "section": "Making a new layer",
    "text": "Making a new layer\nPretty much any layer that exists in GeoServer can be made available as a service–WMS in our case. We’ll run through a couple of examples, but, broadly speaking, the steps are:\n\ncreate a workspace\nadd a datastore to the workspace\nadd a style via a styled layer descriptor (sld) file\nadd a layer to the workspace which references the data store and style\n\nWe’ll put all our examples in a single workspace named statsnz. We can create this in R as follows:\nlibrary(geserveR)\n\ngs <- GeoServer$new()\ngs$createWorkspace(\"statsnz\")"
  },
  {
    "objectID": "posts/building-tile-services-on-the-fly-with-geoserver/index.html#example---meshblock-with-labels",
    "href": "posts/building-tile-services-on-the-fly-with-geoserver/index.html#example---meshblock-with-labels",
    "title": "Building Tile Services on-the-fly with GeoServer",
    "section": "Example - meshblock with labels",
    "text": "Example - meshblock with labels\nMeshblocks are the building blocks of most other official geographies held by Stats NZ. There are approximately 50,000 of them and collectively they are quite large–the GeoPackage used here was 91MB, though we simplified this considerably using mapshaper to make things practical. To display them in R on a leaflet map:\nlibrary(geoserveR)\nlibrary(leaflet)\n\nleaflet() %>%\n  addTiles(urlTemplate = \"https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\") %>%\n  addPolygons(\n    data = mb2018, \n    color = \"#000000\", weight = 1, opacity = 1, fillOpacity = 0,\n    label = ~code\n  )\nThis yields something like the following:\n\nSaved as a stand-alone HTML file, this weighs in at over 15MB, and can be sluggish when rendering. In actuality, the features are oversimplified here, so that when zoomed in the shapes have lost far too much detail. But if we’d displayed them at full resolution, our HTML file would be over 100MB!\nTo make meshblocks available as a service, we first add a datastore. We can add this in R as follows:\ngs$createDatastore(mb2018, \"statsnz\", \"mb2018\")\nInternally, this creates a copy of the mb2018 feature class as a GeoPackage, and then uploads it to the server via the datastores endpoint. We then make a layer in R as follows:\nstyle <- import_template(\n  \"outline_with_label\",\n  name = \"mb2018_with_labels\", strokeColor = \"#000000\", strokeWidth = 1,\n  maxScale = 600000, geometryName = \"geom\", labelName = \"code\",\n  fontFamily = \"Arial\", fontSize = 9, fontStyle = \"normal\",\n  fontWeight = \"bold\", fontColor = \"#000000\",\n  haloSize = 1.5, haloColor = \"#FFFFFF\"\n)\n\ngs$createLayer(\"statsnz\", \"mb2018\", \"mb2018_with_labels\", style)\nInternally, we create a feature type, which is initially unconfigured. We then create a new style, and then we apply the style to the created layer. The variable style contains styling information in styled layer descriptor (SLD) format. The approach here isn’t terribly refined, but it is practical. The import_template function is used to import bundled templates, and replace placeholders with parameter values.\nEither way, having done all this, we can now access meshblocks as a WMS via http://localhost:8080/statsnz/wms? as follows:\nleaflet() %>%\n  fitBounds(lng1 = 164.45, lng2 = 179.35, lat1 = -48.52, lat2 = -33.22) %>%\n  addTiles(\n    urlTemplate = \"https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\"\n  ) %>%\n  addWMSTiles(\n    \"http://localhost:8080/geoserver/statsnz/wms?\", \n    layers = \"mb2018_with_labels\",\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE), \n    group = \"mb2018\"\n  ) %>%\n  addLayersControl(\n    overlayGroups = \"mb2018\",\n    options = layersControlOptions(collapsed = FALSE)\n  )\nVisually:\n\n\nBetter yet, saved as a stand-alone HTML file, this map is less than 500KB (though the various tiles still need to be transferred, so one can still burn through a bit of data just navigating around)."
  },
  {
    "objectID": "posts/building-tile-services-on-the-fly-with-geoserver/index.html#example---population-by-statistical-area-2",
    "href": "posts/building-tile-services-on-the-fly-with-geoserver/index.html#example---population-by-statistical-area-2",
    "title": "Building Tile Services on-the-fly with GeoServer",
    "section": "Example - population by statistical area 2",
    "text": "Example - population by statistical area 2\nConsider the following R code:\nlibrary(geoserveR)\nlibrary(leaflet)\nlibrary(sf)\nlibrary(dplyr)\n\ndata(sa22018)\ndata(popdata)\n\nmapdata <- sa22018 %>%\n  inner_join(filter(popdata, geography == \"sa22018\"), by = \"code\") %>%\n  filter(year == 2018)\n\npal <- colorNumeric(palette = \"Blues\", domain = mapdata$value)\n\nleaflet() %>%\n  addTiles(urlTemplate = \"https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\") %>%\n  addPolygons(data = mapdata, \n              fillColor = ~pal(value), opacity = 0, fillOpacity = 1,\n              label = ~sprintf(\"%s - %s\", label, value)) %>%\n  addLegend(position = \"bottomright\", pal = pal, values = mapdata$value)\nThis yields an interactive map as follows:\n\nThe resulting HTML file weighs in at 3.2MB–not enormous by any stretch, but still probably not something you want users to be fetching repeatedly over a mobile connection, for example. But this map just shows data for 2018, yet we have data for the years 1996, 2001, and 2006-2018–that’s 15 years all up. What if we want to make all this available on one map? In that case, the resulting stand-alone HTML file weighs in at over 40MB. (Of course, a hosted app could serve up a year based on user selection, but 3MB or so would still be transferred each time the year was switched).\nLet us instead see if we can create a workspace in GeoServer which contains a layer for each year. As a WMS, the layers will cease to be interactive (so no hovering tooltips), but we could at least render a label whenever the user zooms in close enough, so let’s do that too.\nFirst, we create a version of the SA2 feature class that has population counts:\nlibrary(dplyr)\nlibrary(reshape2)\n\ncounts <- popdata %>%\n  dplyr::filter(geography == \"sa22018\") %>%\n  dplyr::select(-geography)\n\ncounts_wide <- counts %>%\n  reshape2::dcast(code ~ year, value.var = \"value\") %>%\n  data.frame # this is deliberate so years are like X1996 rather than `1996`\n\nsa22018_with_counts <- sa22018 %>%\n  inner_join(counts_wide, by = \"code\")\nThen, we create a data store using the feature class created, and add a new layer for each of the columns:\ngs <- GeoServer$new()\n\ngs$createDatastore(sa22018_with_counts, \"statsnz\", \"sa22018_with_counts\")\n\nyears <- c(1996, 2001, 2006:2018)\n\nfor (year in years) {\n  col <- sprintf(\"X%s\", year)\n  name <- sprintf(\"sa22018_%s\", year)\n  style <- create_polygon_fills(sa22018_with_counts, col, \"geom\", \"label\")\n  gs$createLayer(\"statsnz\", \"sa22018_with_counts\", name, style) \n}\nAnd that’s it—we now have a bunch of new layers called sa22018_1996, and so on. There’s a little bit going on under the hood, of course, but either way we can now use the resulting WMS layers as follows:\nm <- leaflet() %>%\n  fitBounds(lng1 = 164.45, lng2 = 179.35, lat1 = -48.52, lat2 = -33.22) %>%\n  addTiles(\n    urlTemplate = \"https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\"\n  )\n\nfor (year in years) {\n  m <- m %>%\n    addWMSTiles(\n      \"http://localhost:8080/geoserver/statsnz/wms?\",\n      layers = sprintf(\"sa22018_%s\", year),\n      options = WMSTileOptions(format = \"image/png\", transparent = TRUE),\n      group = as.character(year)\n    ) %>%\n    hideGroup(as.character(year))\n}\n\nm %>%\n  addLayersControl(\n    overlayGroups = as.character(years),\n    options = layersControlOptions(collapsed = TRUE)\n  ) %>%\n  showGroup(as.character(tail(years, 1)))\nwhich yields:"
  },
  {
    "objectID": "posts/building-tile-services-on-the-fly-with-geoserver/index.html#appendix---api-calls",
    "href": "posts/building-tile-services-on-the-fly-with-geoserver/index.html#appendix---api-calls",
    "title": "Building Tile Services on-the-fly with GeoServer",
    "section": "Appendix - API calls",
    "text": "Appendix - API calls\nFor the sake of brevity, the examples above mostly focused on R calls using the provided R client library. For the sake of completeness, and for those who might want to do something similar without R, we include the equivalent functionality using direct API calls.\n\nCreating a workspace\nTo make the statsnz workspace, we ran:\nlibrary(geserveR)\n\ngs <- GeoServer$new()\ngs$createWorkspace(\"statsnz\")\nTo do this direclty, we use the workspaces endpoint:\ncurl \\\n  -u admin:geoserver \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"workspace\":{\"name\":\"statsnz\"}}' \\\n  -X POST \\\n  http://localhost:8080/geoserver/rest/workspaces\n\n\nCreating a datastore\nTo add the meshblock feature set as a datastore, we ran the following in R:\ngs$createDatastore(mb2018, \"statsnz\", \"mb2018\", \"mb2018.gpkg\")\nInternally, this results in mb2018 first being saved as a temporary GeoPackage, before being uploaded via the datastores endpoint. If we wanted to do this manually, we’d first create the GeoPackage in R:\nfname <- tempfile(fileext = \".gpkg\")\nst_write(regc2018, fname, \"mb2018\")\nand then we’d PUT via the datastores endpoint as follows (assuming fname has the value file4b82655cefdd.gpkg):\ncurl \\\n  -u admin:geoserver \\\n  -H \"Accept: application/json, text/xml, application/xml, */*\" \\\n  -H \"Content-Type: application/octet-stream\" \\\n  -T \"/tmp/RtmpbY5Tby/file4b82655cefdd.gpkg\" \\\n  -X PUT \\\n  \"http://localhost:8080/geoserver/rest/workspaces/statsnz/datastores/regc2018/file.gpkg?configure=none&update=overwrite&charset=UTF-8&filename=mb2018.gpkg\"\n\n\nCreating a layer\nTo create a meshblock layer with a simple outline and text label, we ran the following in R:\nstyle <- import_template(\n  \"outline_with_label\",\n  name = \"mb2018_with_labels\", strokeColor = \"#000000\", strokeWidth = 1,\n  maxScale = 600000, geometryName = \"geom\", labelName = \"code\",\n  fontFamily = \"Arial\", fontSize = 9, fontStyle = \"normal\",\n  fontWeight = \"bold\", fontColor = \"#000000\",\n  haloSize = 1.5, haloColor = \"#FFFFFF\"\n)\n\ngs$createLayer(\"statsnz\", \"mb2018\", \"mb2018_with_labels\", style)\nAssuming we have saved the content of the variable style in a file called style.sld, we could create the style on the server as follows:\ncurl \\\n  -u admin:geoserver \\\n  -H \"Accept: application/json, text/xml, application/xml, */*\" \\\n  -H \"Content-Type: application/vnd.ogc.sld+xml\" \\\n  --data \"@style.sld\" \\\n  -X POST \\\n  \"http://localhost:8080/geoserver/rest/workspaces/statsnz/styles?name=mb2018_with_label\"\nWe’d create the feature type as:\ncurl \\\n  -u admin:geoserver \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"featureType\":{\"name\":\"mb2018_with_labels\", \"nativeName\":\"mb2018\"}}' \\\n  -X POST \\\n  \"http://localhost:8080/geoserver/rest/workspaces/statsnz/datastores/mb2018/featuretypes\"\nand we’d add the style to the layer as:\ncurl \\\n  -u admin:geoserver \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"layer\":{\"defaultStyle\":{\"name\":\"statsnz:labelledpolygon\"}}}' \\\n  -X PUT \\\n  \"http://localhost:8080/geoserver/rest/workspaces/statsnz/layers/mb2018\""
  },
  {
    "objectID": "posts/deploying-shiny-apps/index.html",
    "href": "posts/deploying-shiny-apps/index.html",
    "title": "Deploying Shiny Applications",
    "section": "",
    "text": "An Example Application\nIt doesn’t really matter what our application is, but for illustration, let us simply use the following simple application throughout:\n\napp.R\n\n\nlibrary(shiny)\n\nui <- fluidPage(\n  titlePanel(\"Hello runif from Shiny!\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\n        inputId = \"nobs\",\n        label = \"Number of observations:\",\n        min = 1, max = 10000, value = 1000\n      )\n    ),\n    mainPanel(\n      plotOutput(outputId = \"distPlot\")\n    )\n  )\n)\n\nserver <- function(input, output) {\n  data <- reactive({\n    runif(input$nobs)\n  })\n\n  output$distPlot <- renderPlot({\n    hist(\n      data(), col = \"#75AADB\", border = \"white\",\n      xlab = \"x\",\n      main = \"distribution of random uniform numbers\")\n    })\n}\n\nshinyApp(ui = ui, server = server)\n\n\n\nThe application looks as follows:\n\n\n\nNGINX\nIt is easy enough to run applications locally on some specified or random port. In real-world applications, though, applications will usually be made available to external users via some sort of reverse proxy. NGINX is a great choice, and is easy enough to get going. As with many things these days, the easiest way to get up and running, at least for testing or development purposes, is via a container. There are official images, and we can get a copy easily (I’m using Docker, but things can be modified easily enough for other options, such as podman):\ndocker pull nginx\nAn instance suitable for local testing can be stood up easily by running something like:\ndocker run -d --rm --name nginx --network host nginx\nWe can gain access to a terminal in the container by running:\ndocker exec -it nginx /bin/bash\nThen, to test configuration changes we edit the file located at /etc/nginx/conf.d/default.conf, then reload NGINX by running:\nservice nginx reload\nEither way, if running properly, visiting localhost or localhost:80 should produce something like the following:\n\n\n\nshinyapps.io\nRStudio, creators of Shiny, offer a decent hosted service, shinyapps.io, which is both cheap and easy to use, and is highly recommended for many types of user. We can demonstrate deployment of our app using the free tier. First, users need to visit the shinyapps.io website and create an account if they haven’t already. Once signed in, open the dashboard, then click ‘Account’, then ‘Tokens’, and finally ‘Add Token’:\n\nClicking ‘Show’ will open a dialog, and the token can then safely be copied to your clipboard:\n\nAssuming the rsconnect package is installed, users can then paste and run this code directly in R. This links R to your shinyapps.io account, after which apps can be deployed easily from R. This is especially easy for RStudio users, where we just click a single button:\n\nUsers will then see a simple dialog as follows:\n\nAfter waiting a short while, a web browser should open with the running app:\n\nThose not using RStudio can deploy using the rsconnect::deployApp function, but we omit the details. shinyapps.io offers a few nice features, such as the ability to query logs from R, and manage resources assigned to apps via the shinyapps.io dashboard.\n\n\nShiny Server Open Source\nShiny Server Open Source is a free server application that lets users host multiple shiny apps easily from a single host. It is relatively easy to get up and running. For example, to install on Ubuntu, assuming you already have both R and the shiny package installed, we simply run:\n$ sudo apt-get install gdebi-core\n$ wget https://download3.rstudio.org/ubuntu-14.04/x86_64/shiny-server-1.5.17.973-amd64.deb\n$ sudo gdebi shiny-server-1.5.17.973-amd64.deb\nI prefer to isolate things from the host, and use containers where possible. One can get Shiny running via Docker as follows:\n$ docker pull rocker/shiny\n$ docker run -d --rm --name shiny-server --network host -v <path to apps>:/srv/shiny-server rocker/shiny\nIf all went well, Shiny Server will be running on port 3838 on the host, and you’ll be greeted with the following:\n\nTo make our app available via Shiny Server, all we need to do is copy the application to the /srv/shiny-server folder inside our container. For example, assuming our app was saved locally as ./runif/app.R, and this had been copied to /srv/shiny-server/runif/app.R in the container, we could browse to localhost:3838/runif to see:\n\nFinally, if NGINX is running on the same host, it easy enough to make Shiny Server available to the public using a location directive as follows:\nserver {\n  \n  location /shiny/ {\n    rewrite ^/shiny/(.*)$ /$1 break;\n    proxy_pass http://127.0.0.1:3838/;\n    proxy_redirect http://127.0.0.1:3838/ $scheme://$host/shiny/;\n    proxy_http_version 1.1;\n    proxy_set_header Upgrade $http_upgrade;\n    proxy_set_header Connection \"upgrade\";\n  }\n  \n}\nDoing this would mean that our app would be available at both localhost:3838/runif/ and localhost/shiny/runif/. Importantly, external users would be able to reach the app from https://<domain>/shiny/runif/.\n\n\n\nR -e ... + NGINX\nWe can start a Shiny application directly from the terminal with relative ease. For example, assuming our app is in a folder runif, we could start it on port 3001 as follows:\nnohup R -e \"shiny::runApp('runif', host = '0.0.0.0', port = 3001)\" > runif.log 2>&1 &\nThen, of course, the app will be available on port 3001 on the host:\n\nAs before, if NGINX is running on the same host, it easy enough to make the available to the public using a location directive as follows:\nlocation /runif/ {\n  proxy_pass http://localhost:3001/;\n  proxy_http_version 1.1;\n  proxy_set_header Upgrade $http_upgrade;\n  proxy_set_header Connection \"upgrade\";\n}\nIn this case, the app would then be available generally via localhost/ruif/ or https://<domain>/runif/:\n\n\n\nContainerising Shiny Apps\nIt is relatively straightforward to containerise an application. Consider the following Dockerfile:\n\nDockerfile\n\n\nFROM ubuntu:20.04\n\nENV DEBIAN_FRONTEND=noninteractive\nENV SHELL=/bin/bash\n\nRUN  apt-get update && apt-get -y dist-upgrade && \\\n  apt install -y --no-install-recommends \\\n    locales software-properties-common dirmngr wget lsb-release && \\\n  wget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | \\\n    tee -a /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc && \\\n  add-apt-repository \"deb https://cloud.r-project.org/bin/linux/ubuntu $(lsb_release -cs)-cran40/\" && \\\n  apt-get install -y --no-install-recommends r-base r-base-dev && \\\n  sed -i -e 's/# en_US.UTF-8 UTF-8/en_US.UTF-8 UTF-8/' /etc/locale.gen && \\\n  dpkg-reconfigure --frontend=noninteractive locales && \\\n  update-locale LANG=en_US.UTF-8 && \\ \n  alias make=\"/usr/bin/make -j 20\" && \\\n  R -e \"install.packages(c('shiny'))\" && \\\n  apt-get remove -y wget lsb-release r-base-dev && \\\n  apt-get autoremove -y && \\\n  apt-get clean -y && \\\n  rm -rf /var/lib/apt/lists/*\n\nEXPOSE 3838\n  \nCMD R -e \"shiny::runApp('/app', host = '0.0.0.0', port = 3838)\"\n\n\n\nWe can build this image as follows:\ndocker build -t shiny .\nTo start our test application on port 3001, say, we’d then simply run:\ndocker run --rm --name runifapp -p 3001:3838 -v <path to runif>/runif:/app shiny\n(Note that the Dockerfile described isn’t generally useful since we’d need to ensure considerably more R packages were available to be sure that any particular Shiny app would run correctly.)\n\n\nShinyProxy\nShinyProxy is similar to Shiny Server in that it can be used to deploy multiple shiny applications easily on a single host. However, while it is completely open source, it still supports a number of useful enterprise features, such as LDAP authentication. Under the hood, ShinyProxy runs applications by spinning up containers on demand, which is neat!\nInstallation is easy enough. Of course, Docker needs to be installed and running, but ShinyProxy needs TCP access in order to start containers. This varies by system, but for systems using systemd we can just run:\nsudo systemctl edit docker\nand then add the following lines in the appropriate place (comments make it clear where):\n[Service]\nExecStart=\nExecStart=/usr/bin/dockerd -H unix:// -D -H tcp://127.0.0.1:2375\nBut something like:\n\nThen we simply download a copy of ShinyProxy from the ShinyProxy downloads page. At the time of writing, the most recent version is 2.6.0. It is way beyond the scope of a short post such as this to cover ShinyProxy in great detail, so we consider just a very simple setup. Assume first that the Docker image, shiny, as described above exists. Further assume that the following file exists in the same directory as the ShinyProxy jar:\n\napplication.yml\n\n\nserver: \n  servlet:\n    context-path: /shinyproxy\nproxy:\n  title: My Shiny Proxy\n  logo-url: https://www.openanalytics.eu/shinyproxy/logo.png\n  landing-page: /\n  heartbeat-rate: 10000\n  heartbeat-timeout: 60000\n  port: 8081\n  authentication: none\n  docker:\n    cert-path: /home/none\n    url: http://localhost:2375\n    port-range-start: 20000\n  specs:\n  - id: runif\n    display-name: runif\n    description: Historgram of a random uniform sequence\n    container-image: shiny\n    container-volumes: [ \"</path/to/shiny/app>/runif:/app\" ]\n\nlogging:\n  file:\n    name: shinyproxy.log\n\n\n\nWe then start ShinyProxy by running:\njava -jar shinyproxy-2.6.0.jar \nGiven the config, ShinyProxy will then be running on port 8081. A landing page will be available at localhost:8081/shinyproxy:\n\nWe can click the runif link to get to our app, or just navigate directly to localhost:8081/shinyproxy/app/runif:\n\nAs in earlier examples, we can use NGINX to proxy easily enough. A sufficient location directive in this case would be (note that the location must match the context-path field from application.yml):\nlocation /shinyproxy/ {\n  proxy_pass http://localhost:8081/shinyproxy/;\n  proxy_http_version 1.1;\n  proxy_set_header Upgrade $http_upgrade;\n  proxy_set_header Connection \"upgrade\";\n}\nShinyProxy will then be available at localhost/shinyproxy/, or externally at https://<domain>/shinyproxy/:\n\n\n\nOur Container Revisited…\nIn the next example we’ll deploy a Shiny app to a cloud-hosted service, and we’ll do so using a container. We’ll want our container to be self contained, so we’d prefer our app to be copied into the image. So, let us define a new container which we’ll use for the rest of this blog, defined by the following Dockerfile (where, of course, our build context contains our app in a folder runif):\n\nDockerfile\n\n\nFROM ubuntu:20.04\n\nENV DEBIAN_FRONTEND=noninteractive\nENV SHELL=/bin/bash\n\nRUN  apt-get update && apt-get -y dist-upgrade && \\\n  apt install -y --no-install-recommends \\\n    locales software-properties-common dirmngr wget lsb-release && \\\n  wget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | \\\n    tee -a /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc && \\\n  add-apt-repository \"deb https://cloud.r-project.org/bin/linux/ubuntu $(lsb_release -cs)-cran40/\" && \\\n  apt-get install -y --no-install-recommends r-base r-base-dev && \\\n  sed -i -e 's/# en_US.UTF-8 UTF-8/en_US.UTF-8 UTF-8/' /etc/locale.gen && \\\n  dpkg-reconfigure --frontend=noninteractive locales && \\\n  update-locale LANG=en_US.UTF-8 && \\ \n  alias make=\"/usr/bin/make -j 20\" && \\\n  R -e \"install.packages(c('shiny'))\" && \\\n  apt-get remove -y wget lsb-release r-base-dev && \\\n  apt-get autoremove -y && \\\n  apt-get clean -y && \\\n  rm -rf /var/lib/apt/lists/*\n\nCOPY runif /app/\n\nEXPOSE 3838\n\nCMD R -e \"shiny::runApp('/app', host = '0.0.0.0', port = 3838)\"\n\n\n\n\n\nAzure App Services\nAzure is the suite of cloud services managed by Microsoft, and there are several viable options for doing something like deploying a Shiny app. We can deploy things manually on a remote virtual machine using approaches already outlined above, but one reasonable option is to deploy a container using Azure App Service. Note that most things can be done on Azure either through the Azure Portal using a largely point-and-click interface, or at the command-line using Azure CLI. We’ll use the portal here where possible, but the CLI will still be required. Either way, we assume interested readers have access to an Azure account (and, hence, the portal) already, and have also installed the CLI themselves.\nFirst, we need to create an Azure Container Registry. There’s a few ways to get there, but one way is to click ‘All services’…\n\n…then select ‘Containers’ from the ‘Categories’ menu, and then ‘Container registries’…\n\n…then click ‘Create’…\n\nand, finally, we are presented with a form which is straightforward to complete…\n\nAt this point, we need to push our container to the registry. We first need to connect to the container registry on whichever machine our image resides. To do this we first connect to Azure by runing:\naz login\nA browser window will open and await your Azure credentials, after which you’ll be logged in. Then, we connect to the registry:\naz acr login --name myregistry\nThen, to push the runifapp image:\n$ docker tag runifapp myregsitry.azurecr.io/test/runifapp\n$ docker push runifapp.azurecr.io/test/runifapp\nAfter this, we can deploy the image simply by browsing to the image and clicking ‘Deploy to web app’ from the available context menu (note you will also need to have an ‘App Service Plan’ available, but there’s only so much we can cover here!):\n\nIt can take a long time to deploy, but once up and running, you’ll be able to find an external link from the main resource page:\n\n\nNote that Shiny applications require WebSockets availability, so make sure this is enabled and save yourself a bunch of frustration!\n\n\n\nAWS\nTheir are a few options for deploying Shiny apps on AWS. Again, we could deploy manually on an EC2 instance, using containers or not. But otherwise we ought to be able to do this via Elastic Beanstalk, or else by first pushing our image to Elastic Container Registry, and then deploying via Elastic Container Service. I had a quick go at the latter, but the setup became reasonably involved, involving internet gateways and so on, so I decided it to leave it for this post. AWS App Runner looks like a much simpler option and I was able to deploy the app using this, only to find out App Runner does not support WebSockets and the deployed app didn’t work correctly. Hopefully Amazon add WebSocket support to App Runner soon."
  },
  {
    "objectID": "posts/r-spatial-snowflake/index.html",
    "href": "posts/r-spatial-snowflake/index.html",
    "title": "Loading Spatial Data to Snowflake from R",
    "section": "",
    "text": "I’ve recently started using Snowflake for work. I’m keen to enable more active use of geospatial data, and so I’ve been weighing up the merits of using something like Snowflake rather than, say, PostGIS. Snowflake is lacking in features compared to PostGIS–it has far fewer spatial functions, and a particular issue for me is the lack of support for any geodetic datum other than WGS84. On the other hand, if most of your other data will be stored in Snowflake, then having it all in one place greatly enhances the ability to mash things up in-place.\nAs part of my exploration, one thing I found particularly niggly was the load process itself. It is relatively easy to load data into PostGIS from a variety of data sources, either using the ogr2ogr tool provided as part of the GDAL suite; or utilities provided by PostGIS itself, such as shp2pgsql. By contrast, I’d found loading spatial data to Snowflake significantly less straightforward. I understand that FME Workbench can handle the job, but I don’t have access to it currently where I work, and don’t think I’d use it enough to justify its procurement.\nCuriously, my first attempt (which worked rather well), involved PostGIS. I’d first load a feature class to PostGIS, then I’d dump the results of a query into a CSV file, where I’d replace the geometry with well-known text (WKT) using the ST_AS_TEXT function. When copying data from CSV, Snowflake will happily convert WKT to its own GEOGRAPHY type if that’s what the schema demands, and so the method is effective enough. However, if exploring the use of Snowflake as an alternative to PostGIS, it does seem peculiar to involve PostGIS as part of the load process.\nAs an alternative, we can use either of the ODBC or JDBC drivers to insert records. We just need to find a client library or wrapper that we’re comfortable with. I found an approach using R which is both easy and performant, and so I thought it would be worth describing. The post will be brief, but hopefully useful to somebody."
  },
  {
    "objectID": "posts/r-spatial-snowflake/index.html#odbc",
    "href": "posts/r-spatial-snowflake/index.html#odbc",
    "title": "Loading Spatial Data to Snowflake from R",
    "section": "ODBC",
    "text": "ODBC\nIf using ODBC, R users will need to install the odbc package, as well as the Snowflake ODBC driver. See:\nODBC Driver"
  },
  {
    "objectID": "posts/r-spatial-snowflake/index.html#jdbc",
    "href": "posts/r-spatial-snowflake/index.html#jdbc",
    "title": "Loading Spatial Data to Snowflake from R",
    "section": "JDBC",
    "text": "JDBC\nIf using JDBC, R users will need to install the RJDBC package. This depends in turn on the rJava package, and a usable Java Development Kit will need to be present for this to work correctly. Otherwise, users simply need to download the JDBC driver. This can be fetched directly from Maven, for example via:\nsnowflake-jdbc >> 3.13.17\nThe ODBC driver will probably be a little faster than the JDBC driver. In saying that, if you work in a Windows environment, the ODBC driver will need to be installed via an MSI file, and this might require some effort in some enterprise settings. On the other hand, the JDBC driver has no such requirement, and is easy enough to use if you already have access to a functioning JDK."
  },
  {
    "objectID": "posts/a-brief-look-at-apache-sedona/index.html",
    "href": "posts/a-brief-look-at-apache-sedona/index.html",
    "title": "A Brief Look at Apache Sedona",
    "section": "",
    "text": "Apache Sedona, formerly GeoSpark, is a library that let’s us make spatial RDDs and DataFrames in Apache Spark, as well as to run spatial queries. Feature classes can get very large, and so being able to run various geoprocessing tasks in a distributed context seems worthwhile. So, in this post, we take a very brief look at Apache Sedona.\nThis post is not intended to provide comprehensive coverage of the features provided by Sedona–that would be pretty unrealistic for a short blog post. Rather, we will just run a couple of simple examples which will give some sense of both the overall interface and also performance relative to some other common tools.\nFor the purposes of illustration, we consider just two examples:\n\ndissolve a set of polygons via ST_Union_Aggr / ST_Union\nattach polygon attributes to a set of points using ST_Intersects\n\nThe polygons we will dissolve are Meshblock Higher Geographies 2021 (high definition) provided by Stats NZ, and the grouping variable used will be Regional Council. For the task of attaching polygon attributes to points, we borrow the meshblock code from the meshblock 2021 feature class above, and we attach it to NZ Street Address provided by Land Information New Zealand (LINZ).\nNeither of these tasks is massive, with each being comfortably achievable on a single, typically endowed laptop. In this setting, PostGIS is faster than Sedona, but Sedona offers a more scalable solution overall. Sedona can perform as well as PostGIS when using a cluster with several workers, and Sedona should scale to very large problems where vertical scaling of PostGIS would become challenging."
  },
  {
    "objectID": "posts/a-brief-look-at-apache-sedona/index.html#postgis-1",
    "href": "posts/a-brief-look-at-apache-sedona/index.html#postgis-1",
    "title": "A Brief Look at Apache Sedona",
    "section": "PostGIS",
    "text": "PostGIS\nAssuming we have stored the meshblock features in a PostGIS table called statsnz.meshblock2021, we can dissolve them by running the following SQL query:\nSELECT\n  regc2021_v, ST_Multi(ST_Union(geom)) as geom\nFROM \n  statsnz.meshblock2021\nGROUP BY \n  regc2021_v\nORDER BY\n  regc2021_v\nThis took 121 seconds."
  },
  {
    "objectID": "posts/a-brief-look-at-apache-sedona/index.html#r-sf",
    "href": "posts/a-brief-look-at-apache-sedona/index.html#r-sf",
    "title": "A Brief Look at Apache Sedona",
    "section": "R / sf",
    "text": "R / sf\nTo import the meshblock data in R, we run:\nlibrary(sf)\n\nsystem.time({\n  mbhg <- st_read(\"meshblock-higher-geographies-2021-high-definition.shp\")  \n})\nThis took 3 seconds, and the resulting data frame occupies 300MB in memory. To dissolve it, we run:\nrc <- mbhg %>%\n  group_by(REGC2021_V) %>%\n  summarise(n = n())\nThis took around 124 seconds, and the resulting data frame occupies 3.9MB in memory."
  },
  {
    "objectID": "posts/a-brief-look-at-apache-sedona/index.html#sedona---local",
    "href": "posts/a-brief-look-at-apache-sedona/index.html#sedona---local",
    "title": "A Brief Look at Apache Sedona",
    "section": "Sedona - Local",
    "text": "Sedona - Local\nWe first load our meshblock feature class as follows:\nimport org.apache.sedona.core.formatMapper.shapefileParser.ShapefileReader\n\nval mbrdd = ShapefileReader.readToPolygonRDD(\n  sc, // the cluster SparkContext\n  \"meshblock-higher-geographies-2021-high-definition\"\n)\nThe type of mbrdd is PolygonRDD, which is-a SpatialRDD. SpatialRDD has a standard RDD under the hood, and has methods for adding indexes, doing spatial joins, and so on. However, we can convert SpatialRDDs to DataFrames, and then run reasonably standard looking SQL queries on the result. This is probably going to be the most accessible option for those who aren’t terribly familiar with Spark or Java / Scala. To convert to DataFrame:\nimport org.apache.sedona.sql.utils.{Adapter, SedonaSQLRegistrator}\n\nSedonaSQLRegistrator.registerAll(spark)\n\nval mbdf = Adapter\n  .toDf(mbrdd, spark)\n  .repartition(32)\n  .persist\nThe result is a DataFrame where our geometry is stored as type Geometry in a column called geometry, and calling registerAll(spark) ensures that the Geometry type is understood by Spark. For example, we create a user-defined function which calculates the area of a polygon:\nimport org.locationtech.jts.geom.Geometry\nimport org.apache.spark.sql.functions.{col, udf}\n\nval f: (x: Geometry) => x.getArea\nand use it to derive an area column:\nmbdf\n  .withColumn(\"area\", f(col(\"geometry\")))\n  .select(\"MB2021_V1_\", \"REGC2021_V\", \"REGC2021_2\", \"area\")\n  .limit(10)\n  .show\n+----------+----------+------------------+--------------------+\n|MB2021_V1_|REGC2021_V|        REGC2021_2|                area|\n+----------+----------+------------------+--------------------+\n|   0973300|        03|    Waikato Region|   68346.86758174733|\n|   4002221|        06|Hawke's Bay Region|  42925.980450310104|\n|   4008822|        13| Canterbury Region|   37448.43920328615|\n|   2311100|        13| Canterbury Region|3.8148438963027686E7|\n|   2815503|        13| Canterbury Region|  24410.008602414582|\n|   0221305|        02|   Auckland Region|    884246.943146247|\n|   1152900|        03|    Waikato Region| 4.621434800254852E7|\n|   4011098|        13| Canterbury Region|   8207.470212947434|\n|   4011489|        13| Canterbury Region|1.0198131361068603E7|\n|   2830201|        14|      Otago Region|   96059.66604287364|\n+----------+----------+------------------+--------------------+\nEither way, we now create a view from the DataFrame which we can use from our SQL context:\nmbdf.createOrReplaceTempView(\"mb\")\nand then use to write standard looking spatial SQL queries. For example, we now add an area column in an arguably more straightforward manner:\nspark\n  .sql(\"\"\" \n    SELECT \n      MB2021_V1_, \n      REGC2021_V, \n      REGC2021_2, \n      ST_AREA(geometry) as area\n    FROM\n      mb\n    LIMIT 10\n  \"\"\")\n  .show\n+----------+----------+------------------+--------------------+\n|MB2021_V1_|REGC2021_V|        REGC2021_2|                area|\n+----------+----------+------------------+--------------------+\n|   0973300|        03|    Waikato Region|   68346.86758174733|\n|   4002221|        06|Hawke's Bay Region|  42925.980450310104|\n|   4008822|        13| Canterbury Region|   37448.43920328615|\n|   2311100|        13| Canterbury Region|3.8148438963027686E7|\n|   2815503|        13| Canterbury Region|  24410.008602414582|\n|   0221305|        02|   Auckland Region|    884246.943146247|\n|   1152900|        03|    Waikato Region| 4.621434800254852E7|\n|   4011098|        13| Canterbury Region|   8207.470212947434|\n|   4011489|        13| Canterbury Region|1.0198131361068603E7|\n|   2830201|        14|      Otago Region|   96059.66604287364|\n+----------+----------+------------------+--------------------+\nMore importantly, we can dissolve the meshblocks by region as desired as follows:\nval rc = spark.sql(\n  \"\"\"\n  SELECT \n    REGC2021_V, \n    ST_Union_Aggr(geometry) as geometry \n  FROM \n    mb \n  GROUP BY\n    REGC2021_V \n  ORDER BY\n    REGC2021_V\n  \"\"\" \n)\nSpark SQL execution is lazy, so we need to trigger an action in order to see how long this actually takes. I like to write a little function I can re-use for timing purposes as follows:\ndef timeit[T](block: =>T): (T, Double) = {\n  val startTime = System.nanoTime\n  val res: T = block\n  (res, (System.nanoTime - startTime) / 1e9)\n}\nSo:\nval (_, t) = timeit { rc.show }\nt: Double = 152.024319529\nAt 152 seconds this is a little slower than PostGIS, but pretty reasonable all the same. However, there is a pretty big caveat here. Spark is pretty sophisticated, and in practice performance will vary a lot depending on how things are configured. We can vary the number of executors, the number of cores each executor can use, driver memory, and so on, and so on. In this case we did not spend any time tuning these parameters, but we did deliberately repartition the input meshblock features into 36 chunks, and this has a dramatic effect on performance. The following chart shows runtime as a function of the number of partitions:\n\nThe machine this task was run on has 12 cores, and the best performance seems to be roughly where there are 3 times as many partitions as cores.\nNote that those familiar with Spark could attempt to do things a bit more… primitively. Looking again at the original object, mbrdd, we can get the underlying RDD as:\nmbrdd.getRawSpatialRDD.rdd\nThis is essentially just a collection of objects of type Geometry. We can call a method getUserData on a Geometry object to get the attributes associated with a feature as a single tab-delimited string. The 24th field is the regional council code, and we can use this to create a pair RDD:\nval pairrdd = mbrdd\n  .getRawSpatialRDD.rdd\n  .groupBy(x => x.getUserData.toString.split(\"\\t\")(24))\nWe can then fold the geometries in each key-value pair naively as follows:\nval rc = pairrdd\n  .map(x => {\n    val y = x._2\n    (x._1 -> y.tail.foldLeft(y.head)(_.union(_)))\n  })\nThis does in fact give us what we want, however the approach turns out to be way too naive, taking 45 minutes all up! That said, it would scale if we threw enough workers at it. Still, at times this sort of approach will be useful."
  },
  {
    "objectID": "posts/a-brief-look-at-apache-sedona/index.html#sedona---elastic-mapreduce",
    "href": "posts/a-brief-look-at-apache-sedona/index.html#sedona---elastic-mapreduce",
    "title": "A Brief Look at Apache Sedona",
    "section": "Sedona - Elastic MapReduce",
    "text": "Sedona - Elastic MapReduce\nWhen deployed on Elastic MapReduce, the code required is the same, with very minor differences. In this case, we naively set the number of partitions for each input feature class to be the number of cores times the number of workers. In addition, when using EMR we store our source shapefiles in the Hadoop filesystem. A cluster with 6 workers was able to dissolve the features as required in 122 seconds, and a cluster with 12 workers dissolved the features in 108 seconds."
  },
  {
    "objectID": "posts/a-brief-look-at-apache-sedona/index.html#postgis-3",
    "href": "posts/a-brief-look-at-apache-sedona/index.html#postgis-3",
    "title": "A Brief Look at Apache Sedona",
    "section": "PostGIS",
    "text": "PostGIS\nAssuming we have address points stored in a PostGIS table called linz.address, and meshblock as before in statsnz.meshblock2021, we can borrow attributes from each address’ enclosing meshblock as follows:\nSELECT \n  addr.address_id, \n  addr.full_add_1,  \n  mb.mb2021_v1_, \n  addr.geom \nFROM\n  linz.address addr, \n  statsnz.meshblock2021 mb \nWHERE \n  st_intersects(addr.geom, mb.geom)\nThis took 193 seconds. Note that to ensure this query can be executed efficiently, it is necessary to ensure spatial indexes are created for both input features ahead of time. For example:\nCREATE INDEX address_geom_idx ON linz.address using GIST(geom);\nCREATE INDEX meshblock2021_geom_idx ON statsnz.meshblock2021 using GIST(geom);"
  },
  {
    "objectID": "posts/a-brief-look-at-apache-sedona/index.html#r-sf-1",
    "href": "posts/a-brief-look-at-apache-sedona/index.html#r-sf-1",
    "title": "A Brief Look at Apache Sedona",
    "section": "R / sf",
    "text": "R / sf\nTo import the address data in R, we run:\nsystem.time({\n  addr <- st_read(\"nz-street-address.shp\")  \n})\nThis took 40 seconds, and the resulting data frame occupies 1.5GB in memory. To intersect this with the meshblock feature class:\nsf::st_intersection(\n  select(addr, ADDRESS_ID, FULL_ADD_1), \n  select(mbhg, MB2021_V1_)\n) \nThis took 3194 seconds, or 53 minutes. This is slow because there is no way to make use of a spatial index, and to find which meshblock an address is in will require a full scan of the meshblock feature class for each address. Note, though, that it would be easy to parallelise this task simply by breaking the address input into ranges and intersecting each concurrently."
  },
  {
    "objectID": "posts/a-brief-look-at-apache-sedona/index.html#sedona---local-1",
    "href": "posts/a-brief-look-at-apache-sedona/index.html#sedona---local-1",
    "title": "A Brief Look at Apache Sedona",
    "section": "Sedona - Local",
    "text": "Sedona - Local\nTo create a DataFrame containing addresses, we run:\nval addrpath = \"shp/linz/nz-street-address\"\nval addrrdd = ShapefileReader.readToGeometryRDD(sc, addrpath)\nval addr = Adapter.toDf(addrrdd, spark).repartition(32).persist\naddr.createOrReplaceTempView(\"addr\")\nAnd to intersect this with the meshblock DataFrame, we run:\nval addrmb = spark\n  .sql(\"\"\"\n  SELECT \n    addr.ADDRESS_ID, \n    addr.FULL_ADD_1, \n    mb.MB2021_V1_, \n    addr.geometry \n  FROM \n    addr, \n    mb  \n  WHERE \n    st_intersects(addr.geometry, bb.geometry)\n  \"\"\")\nThis took 268 seconds. This is reasonable, but still a little slower than using PostGIS. As for the PostGIS query, it is necessary to ensure that spatial indexes are used in order for this query to run efficiently. One way of doing this is to ensure we set the Spark config sedona.global.index to true. Without doing this, the query will run much slower.\nAs before, we can get similar results using RDDs directly. For example, we can produce a new pair RDD that contains meshblocks as a key, and all the points within the meshblock as values in the following way:\nimport org.apache.sedona.core.enums.IndexType\nimport org.apache.sedona.core.spatialOperator.JoinQuery\n\naddrrdd.spatialPartitioning(GridType.KDBTREE)\nmbrdd.spatialPartitioning(addrrdd.getPartitioner())\naddrrdd.buildIndex(IndexType.QUADTREE, true)\nmbrdd.buildIndex(IndexType.QUADTREE, true)\n\nval result = JoinQuery.SpatialJoinQuery(addrrdd, mbrdd, true, false)\nAgain, those familiar with Spark already will find objects like this relatively easy to work with. For example, to print a list of meshblock and address ID pairs:\nresult.rdd.flatMap(x => {\n  val mbid = x._1.getUserData.toString.split(\"\\t\")(1)\n  val addrids = x._2.asScala.map(_.getUserData.toString.split(\"\\t\")(1))\n  addrids.map((mbid, _))\n}).take(10).foreach(println)\n(4003499,2099993)\n(4003499,2099992)\n(4003499,2099994)\n(1619000,2099934)\n(2817502,2010408)\n(3031002,2099960)\n(4008278,2099958)\n(4007449,2100082)\n(4006988,2099944)\n(1618904,2099935)"
  },
  {
    "objectID": "posts/a-brief-look-at-apache-sedona/index.html#sedona---elastic-mapreduce-1",
    "href": "posts/a-brief-look-at-apache-sedona/index.html#sedona---elastic-mapreduce-1",
    "title": "A Brief Look at Apache Sedona",
    "section": "Sedona - Elastic MapReduce",
    "text": "Sedona - Elastic MapReduce\nWhen deployed on Elastic MapReduce, the code required is the same, with very minor differences. In this case, we again naively set the number of partitions for each input feature class to be the number of cores times the number of workers. A cluster with 6 workers was able to dissolve the features as required in 154 seconds, and a cluster with 12 workers dissolved the features in 110 seconds"
  },
  {
    "objectID": "posts/a-brief-look-at-apache-sedona/index.html#sbt-console",
    "href": "posts/a-brief-look-at-apache-sedona/index.html#sbt-console",
    "title": "A Brief Look at Apache Sedona",
    "section": "sbt console",
    "text": "sbt console\nOne of the easier ways to get running with Spark is to set up a basic sbt project. We can then use the sbt console to run code interactively, or else output a jar file which can be used with spark-shell. To get going, all we need is a build.sbt file with the following content:\nname := \"\"\"sedonatest\"\"\"\nversion := \"0.0.1\"\n\nscalaVersion := \"2.12.13\"\nscalacOptions += \"-Ydelambdafy:inline\"\n\nlibraryDependencies ++= Seq(\n  \"org.apache.spark\" %% \"spark-core\" % \"3.0.1\", \n  \"org.apache.spark\" %% \"spark-sql\" % \"3.0.1\", \n  \"org.apache.sedona\" %% \"sedona-core-3.0\" % \"1.0.0-incubating\",\n  \"org.apache.sedona\" %% \"sedona-sql-3.0\" % \"1.0.0-incubating\",\n  \"org.apache.sedona\" %% \"sedona-viz-3.0\" % \"1.0.0-incubating\",\n  \"org.apache.sedona\" %% \"sedona-viz-3.0\" % \"1.0.0-incubating\",\n  \"org.locationtech.jts\" % \"jts-core\" % \"1.18.1\",\n  \"org.geotools\" % \"gt-main\" % \"24.0\",\n  \"org.geotools\" % \"gt-referencing\" % \"24.0\",\n  \"org.geotools\" % \"gt-epsg-hsql\" % \"24.0\"\n)\n\nresolvers ++= Seq(\n  \"Open Source Geospatial Foundation Repository\" at \"https://repo.osgeo.org/repository/release/\",\n  \"Apache Software Foundation Snapshots\" at \"https://repository.apache.org/content/groups/snapshots\",\n  \"Java.net repository\" at \"https://download.java.net/maven/2\"\n)\nFrom within the project folder, we first run sbt to get an interactive build shell, and then console to get to the Scala REPL. Run this way, the Spark and Sedona libraries will both be available.\nUnlike spark-shell, sbt console will not provide a SparkSession instance. Assuming we’re running in standalone mode, we can create one as follows:\nval spark: SparkSession = SparkSession\n    .builder\n    .master(\"local[*]\")\n    .appName(\"sedonatest\")\n    .config(\n      \"spark.serializer\", \n      \"org.apache.spark.serializer.KryoSerializer\"\n    ) \n    .config(\n      \"spark.kryo.registrator\", \n      \"org.apache.sedona.viz.core.Serde.SedonaVizKryoRegistrator\"\n    ) \n    .config(\"sedona.global.index\",\"true\")\n    .getOrCreate\nAnd, of course, to get a SparkContext:\nval sc = spark.sparkContext"
  },
  {
    "objectID": "posts/a-brief-look-at-apache-sedona/index.html#spark-shell",
    "href": "posts/a-brief-look-at-apache-sedona/index.html#spark-shell",
    "title": "A Brief Look at Apache Sedona",
    "section": "spark-shell",
    "text": "spark-shell\nIf we have access to a Spark cluster and wish to use spark-shell instead, then we’d modify the dependencies in build.sbt as follows:\nlibraryDependencies ++= Seq(\n  \"org.apache.spark\" %% \"spark-core\" % \"3.0.1\" % \"provided\", \n  \"org.apache.spark\" %% \"spark-sql\" % \"3.0.1\" % \"provided\", \n  \"org.apache.sedona\" %% \"sedona-core-3.0\" % \"1.0.0-incubating\",\n  \"org.apache.sedona\" %% \"sedona-sql-3.0\" % \"1.0.0-incubating\",\n  \"org.apache.sedona\" %% \"sedona-viz-3.0\" % \"1.0.0-incubating\",\n  \"org.apache.sedona\" %% \"sedona-viz-3.0\" % \"1.0.0-incubating\",\n  \"org.locationtech.jts\" % \"jts-core\" % \"1.18.1\",\n  \"org.geotools\" % \"gt-main\" % \"24.0\",\n  \"org.geotools\" % \"gt-referencing\" % \"24.0\",\n  \"org.geotools\" % \"gt-epsg-hsql\" % \"24.0\"\n)\nWe’d typically make a so-called ‘fat jar’, and adding % \"provided\" ensures the Spark dependencies, which will be present already, aren’t included in the archive. Note that we’re using Spark 3.0.1 here, so adjust this as required to match whatever is installed in your cluster.\nTo make a fat jar, we ensure ./project/assembly.sbt is present with the following content:\naddSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.14.10\")\nWe can then run sbt assembly from the project root, producing, in this case, ./target/scala-2.12/sedonatest-assembly-0.0.1.jar. We then start spark-shell as follows:\nspark-shell --jars sedonatest-assembly-0.0.1.jar\nIf we wish the jar file to have a different name, sedonatest.jar for example, we’d add the following to build.sbt:\nassemblyJarName in assembly := \"sedonatest.jar\"\nNote that if we get duplicate dependency errors when running assembly, then we can add something like the following to build.sbt:\nassemblyMergeStrategy in assembly := {\n  case PathList(\"META-INF\", xs @ _*) => MergeStrategy.discard \n  case x => MergeStrategy.first\n}\nElastic MapReduce uses YARN as a resource manager, so we also omit the following line when creating our SparkSession:\n  .master(\"local[*]\")"
  },
  {
    "objectID": "posts/routing-in-r-using-osrm/index.html",
    "href": "posts/routing-in-r-using-osrm/index.html",
    "title": "Routing in R Using the Open Source Routing Machine (OSRM)",
    "section": "",
    "text": "Overview\nEdit 2020-07-16: The original version of this post included a bespoke decoder for geometries encoded as Google polylines. The post has been updated to use the googlePolylines package instead.\nI often find myself needing to establish the travel time or distance between arrays of addresses. In the past I have used ArcMap’s Network Analyst tool, but have found the syntax to be clunky at best, and the performance to be very mediocre. And, besides, I am often working in R and sometimes it’s nice to be able to do everything in the one environment, rather than doing the routing in Python, say, and then using the results in R.\nThe open source routing machine is a very fast routing engine which can be accessed via an HTTP API, which means it can be queried relatively easy from most languages, including R. And while public servers are available for use, it is also relatively easy to set up locally resulting in excellent throughput due to the lack of latency.\n\n\nPrerequisites\nThe following R packages are required:\n\nrjson for reading web service results in JSON format\nbitops used to decode polylines (has convenient bitshift operators, etc.)\nsp for spatial projections, etc.\nleaflet for rendering leaflet maps in a browser\ngooglePolylines for converting routes returned from OSRM to sp-compatible paths\n\n\n\nGeocoding addresses\nThe [Google Geocoding API](https://developers.google.com/maps/documentation/geocoding/] is used to geocode addresses. Usage is free, but the free license is limited to 2500 requests per day, at a rate no faster than 5 per second. There are free alternatives, but I’ve not found any that are satisfactorally accurate for New Zealand addresses. That said, interesting alternatives worth keeping an eye on are:\n\ndata science toolkit - doesn’t work at all for New Zealand\nNominatim - finds addresses but probably only okay if approximate locations are acceptable.\n\nThe following code results in two addresses being geocoded which we will use as an origin and a destination later when using a routing service.\n\nlibrary(rjson)\n\n# use HERE API to geocode a start point...\nget_location <- function(\n  search_text,\n  app_id = params$app_id, \n  app_code = params$app_code\n) {\n  service <- \"https://geocoder.api.here.com/6.2/geocode.json\"\n  \n  query <- sprintf(\n    \"%s?app_id=%s&app_code=%s&searchtext=%s\",\n    service, app_id, app_code,\n    gsub(\" \", \"%20\", search_text, fixed = TRUE)\n  )\n\n  res1 <- jsonlite::fromJSON(query, simplifyVector = FALSE)\n  res2 <- res1$Response$View[[1]]$Result[[1]]$Location\n\n  list(\n    lng = res2$DisplayPosition$Longitude, \n    lat = res2$DisplayPosition$Latitude, \n    address = res2$Address$Label\n  )\n}\n\n# start point...\n(o <- get_location(\"25 Edelweiss Grove, Timberlea, Upper Hutt\"))\n\n$lng\n[1] 175.1085\n\n$lat\n[1] -41.10678\n\n$address\n[1] \"25 Edelweiss Grv, Timberlea, Upper Hutt 5018, New Zealand\"\n\n# ...and destination\n(d <- get_location(\"1 Pipitea Street, Wellington\"))\n\n$lng\n[1] 174.7811\n\n$lat\n[1] -41.27568\n\n$address\n[1] \"1 Pipitea St, Thorndon, Wellington 6011, New Zealand\"\n\n\n\n\nOpen Source Routing Machine (OSRM)\nOpen Source Routing Machine is an open source route solver. It is written in C++ and runs on Linux (maybe other platforms, but stick with Linux), and is very fast. There is a nice web demo which uses the service as a back-end here. The back-end service is available to the public at https://router.project-osrm.org. Details, including usage policy, is available here.\nThe code below shows how to find a route between the origin and destination locations found above:\n\nurl <- paste0(\n  \"https://router.project-osrm.org/route/v1/driving/\",\n  o$lng,\",\",o$lat,\";\",d$lng,\",\",d$lat,\"?overview=full\"\n)\n\nroute <- jsonlite::fromJSON(url, simplifyVector = FALSE)\n\nAgain, assuming a route was successfully found, route will now contain a list including, among other things, time in seconds to traverse the route, distance in metres, and the route geometry stored in encoded polyline algorithm format.\n\nroute$routes[[1]]$duration\n\n[1] 1744.9\n\nroute$routes[[1]]$distance\n\n[1] 35344.3\n\nroute$routes[[1]]$geometry\n\n[1] \"~t{yFozwk`@[hAo@rAOKMEOAO?MByBnBD^Tj@DNDVNNr@|APh@Lp@p@~DBVV\\\\V\\\\XVzB~A^RNDV@\\\\AfAOVCXBPFJFTPh@v@XLTBj@ALBLHLNNT|BzDfAhBFL@LALAJGLiBhCyAnBcChDMRIPSp@Mh@G\\\\E\\\\ET`BLhAFnBRn@D^Fd@Lf@PTJXPZRLJTVVZJJNVHPLTTb@P`@LNT`@Tl@HPFNHRXz@XbAZ`B`@xAXbATlANjAHd@FbAHrA@P@HJbBHzAL|AJxAHjAJjA@PTfC\\\\bDXhC\\\\dDZlC\\\\`DRjBd@jDNpALtAL`BBRB`@VjELzB?zAN`EL`DNpDH~AFpBHtBBhABjA@v@?fBAxACvAGbFAfA@`AB`ABt@H`EFnCLvCHhBHzFBz@Bn@@lABz@?jAAxAC|BOrG?`BCvDAjD?|E?pAAdLB~DFfDN~CRvCPrBPlBZbCZpB@JJZ\\\\~AjA~EHVFXPt@Z`An@hBv@lBjAjCp@tAl@lA`AdBfAbB~AdC|BnDpC~DfBjCz@rALZP^Tn@Dd@Jh@Dj@Bt@?l@Cl@EVMdAIlAErABjAJlALl@f@vAVl@Xh@`AhBn@rAZp@hAdDz@zCt@jC~DvNf@`B^lAvA`DVl@d@fAbAnB~@bBzA`CxDdGbDjEfC|DzBjDfBlCh@x@~@zAfAlBfAtB`AnB|@~BZ~@l@fBd@rA\\\\hAz@tC|@tCr@pBh@zA|@vBr@jBfAdCh@dAh@hAbAtBz@`Bl@rAbBzCr@xAp@|Ad@hAhAxCv@vBdA`Dj@zAd@dAr@dB|@nBr@xATb@p@hA~@zAfBlC`ClDhB~Bt@fAl@|@f@`Ad@x@\\\\v@Tl@d@pAf@bBH\\\\j@pBb@~An@zBj@lCt@jD`@hBbA`Dx@fCfBfF`@jA`@hAj@nAn@pAt@rA^l@bApAtBlCJNLNLNJLLNLLLNLLLLNLLLLLLJNLLJNJNLLJNHxCtBNJLJNJNJLLNJLLLJNLLLLLLLLLLNLLLNLLLNJNLNLNJNJNLPJNJNLPJPJNJPHPJPlNhWnCbFJRJRHPJRJRJRHRJRHRHRHRJTHRHTHRHTFRHTHTFRHTnBzGHTFTHTFTHTHRFTHTHRHTHRHTHRJRHTHRJRHRJRHRJRJRJPHRJRJPJRJPJRLPJPJRJPLPJPLPJPLNLPJPLNLPLNLPLNLNLNLNLNLNLNNNLLNNLNLLNLNNzDpDLLNLLLLLLNLLLNJLLNLLLNJNLNLNJNLNJPJNLNJPJNJPJN~DxGLNJPJPJNJPJNLPJNJPJNLNJPJNLNJNxHdKhBdCpBnCxB`DzAjClAfClB|Dv@vA`A|AjA|AvAbBtCnDpCbDfAzAbA`BnA`ChAbCt@~Av@xA|@|A|@nAn@t@|@`ArBxB|B|BvE|E|BhCvAxAfAlApA`BbBhCnCtErC`FbBtCv@|ApBnE|AnDlDfIvDrItAbD`ApBfB`DjBfDz@zAjArBr@hADJHLd@|@|@pBHNj@rAx@dBZl@Zl@rAfCx@|ApB~DvBpEvB|EpCxFpAxCBDLZjApCtC`HxBbFL\\\\N\\\\`@|@~@tBpAfCbApBl@pAl@pAl@zAl@zAXn@d@fA`AvB~ApDnCzFjD`HTb@N`@Vh@nAxCHNHTXt@^~@d@nAJZJVPj@|@`CnAfDnDjJpAjD`@bA^t@vBnDn@rAt@xA|AfDbA~Bz@xBt@dBdAbCf@pAjCjHxBzFv@~Bh@pAt@nAdAlBt@fA|@rAnAlB~BlDx@tAbCrDvA~Bn@dAbAzBr@rARf@^|@j@~Ah@vAh@zAXfAVv@ZdAZ~@h@xA\\\\z@\\\\`A`A`Cp@fBLd@j@|AZfAx@xCjApEhB|Gv@pCv@jCfCjHt@pBb@hAh@vAhAdDj@hBf@`B`@zAz@bDv@dDvAdG~AlHf@xBRbALx@Jz@Hx@Fv@Bp@BdA@`AAp@?VAX?HA^C\\\\C\\\\CXI|@Eb@CVA\\\\?T@XBVF^Lh@R|@^~ApCrL|AxG~AhH^`Bb@|AtAhD^dAZhATnATlAj@~CxBvMrAnIZdBXbA^bAl@tAp@dAv@~@|@x@nCbCh@l@^l@p@lAb@bAd@~A~@lE\\\\|A\\\\fAd@jAj@`A^h@`@f@`GvGlCrDlCtExG~M~IfR~ExH`ElIfFxKrBjElBjE`AnBpArCx@bBjAtBjAnBlBhCfBdC~@rAr@bAp@fAj@~@x@zAn@hAx@vAx@~A`AfBfBrDnArCdFbL~@pBj@lAn@dAj@|@b@n@j@n@v@|@`AbAt@~@h@r@dA|Av@xA`@|@`@bAlAvCj@`Bj@tB`@jBd@pCd@xCf@dCr@fCbAbDfBrFb@nAl@`Br@`BjAbCrBfDrAhBz@bA|@~@fAdAz@t@vApAv@t@h@h@f@n@^n@h@bA`@bAt@tBn@`B^`Ad@~@NXVZf@n@j@l@pCfCpDdDv@t@hIpJz@`AzAtA~AxAtIvHl@p@p@~@Zh@xFnIh@p@TRZLVFV?ZAp@Ox@ULGhAm@l@YtAs@ZQr@]BCj@YXOVMPInAo@BaDBqA?GAGAIFCDAB???\"\n\n\nWe then call a function to convert the encoded route to a SpatialLines object:\n\npath <- googlePolylines::decode(route$routes[[1]]$geometry)[[1]]\n\n\n\n\n\n\n\n\n\n\n\n\nUsing leaflet to make a nicer map\nIt is relatively easy to make a nice interactive map. Here we draw a simple leaflet map, and overlay the origin and destination points, as well as the route between.\n\nlibrary(leaflet)\n\n#make a string to nicely label the route\ns <- route$routes[[1]]$duration\nkms <- round(route$routes[[1]]$distance/1000, 1)\nroutelabel <- paste0(s%/%60, \"m \", s%%60, \"s , \", kms, \"kms\")\n\n#create a basic map\nm <- leaflet(width=\"100%\") %>% \n  addTiles()  %>% \n  addPolylines(\n    lng = path$lon,\n    lat = path$lat,\n    popup = routelabel, \n    color = \"#000000\", \n    opacity = 1, \n    weight = 3\n  ) %>%\n  addMarkers(lng=o$lng, lat=o$lat, popup=o$address) %>%\n  addMarkers(lng=d$lng, lat=d$lat, popup=d$address)\n\n\n\n\n\n\n\nIt’s also relatively straight forward to use different base maps, and a nice demo of some other providers can be found here. For example:\n\nrequire(leaflet)\n\nleaflet(width=\"100%\") %>% \n  addTiles(\n    urlTemplate='https://stamen-tiles-{s}.a.ssl.fastly.net/watercolor/{z}/{x}/{y}.png',\n    attribution = sprintf(\n      \"Map tiles by %s\", \n      '<a href=\"http://stamen.com\">Stamen Design</a>'\n    )\n  ) %>%\n  addTiles(\n    urlTemplate = 'https://stamen-tiles-{s}.a.ssl.fastly.net/toner-hybrid/{z}/{x}/{y}{r}.png'\n  ) %>%\n  addPolylines(\n    lng = path$lon,\n    lat = path$lat,\n    popup=routelabel, \n    color = \"#000000\", \n    opacity = 1, \n    weight = 3\n  ) %>%\n  addMarkers(lng = o$lng, lat = o$lat, popup = o$address) %>%\n  addMarkers(lng = d$lng, lat = d$lat, popup = d$address)"
  },
  {
    "objectID": "posts/serving-geospatial-features-with-mapnik-and-tilestache/index.html",
    "href": "posts/serving-geospatial-features-with-mapnik-and-tilestache/index.html",
    "title": "Serving Geospatial Features with Mapnik and TileStache",
    "section": "",
    "text": "When making maps for display on the web, the size of the various objects being rendered can quickly become overwhelming. For example, a polygon feature class containing New Zealand Meshblocks, a set of roughly 40,000 mutually exclusive and exhaustive polygons, is more than 100MB when stored as a shapefile, but much larger uncompressed and rendered in a browser window.\nThere are various approaches one can take to attempt to deal with this issue, though useful solutions tend to depend on server-side processing. Essentially, one fetches and renders only as much data as is needed to populate the current viewport. One way of doing this is to use a tile server whereby a map is represented as a grid made of individual tiles, and each tile is an image which can be rendered and served on request. For example:\n\nIn this document we consider two main tools: Mapnik, which is used to render the individual tiles; and TileStache, which is used to serve the tiles.\nMapnik can render tiles dynamically, thus avoiding the need to cache all the required tiles in advance. Maps themselves are styled via an XML file which provides a good deal of control over map layout, and can be modified on demand.\nBest of all, every tool used in this document is completely free. So, in principle, a production quality server could be created, with the only costs incurred being the cost of the host server itself. At current rates, a Linux instance with 2 cores and 8GB of RAM could be created on AWS with an annual cost of as little as $900."
  },
  {
    "objectID": "posts/serving-geospatial-features-with-mapnik-and-tilestache/index.html#mapnik-tilestache",
    "href": "posts/serving-geospatial-features-with-mapnik-and-tilestache/index.html#mapnik-tilestache",
    "title": "Serving Geospatial Features with Mapnik and TileStache",
    "section": "Mapnik & TileStache",
    "text": "Mapnik & TileStache\nBoth Mapnik and TileStache are in the standard repositories, so installing is as simple as:\nsudo apt-get install libmapnik libmapnik-dev mapnik-utils python-mapnik tilestache\nTileStache also required the Python imaging library. To install this:\nsudo pip install -U Pillow\n\nTesting the installation\nFor development purposes TileStache can be started via the tilestache-server command, which causes TileStache to run on port 8080 by default.\nWhen starting, tilestache-server looks for a file called tilestache.cfg, so we create one as follows:\n{\n  \"layers\": \n  {\n    \"osm\":\n    {\n      \"provider\": {\"name\": \"proxy\", \"url\": \"http://tile.openstreetmap.org/{Z}/{X}/{Y}.png\"}\n    }\n  }\n}\nTo start TileStache, run:\ntilestache-server -c <path to config>/tilestache.cfg\nwhich will then make the server available at http://localhost:8080:\n\nIn this case, tiles from openstreetmap.org are available at /osm/{z}/{x}/{y}.png:"
  },
  {
    "objectID": "posts/serving-geospatial-features-with-mapnik-and-tilestache/index.html#uwsgi-nginx-setup-optional",
    "href": "posts/serving-geospatial-features-with-mapnik-and-tilestache/index.html#uwsgi-nginx-setup-optional",
    "title": "Serving Geospatial Features with Mapnik and TileStache",
    "section": "uWSGI + Nginx Setup (optional)",
    "text": "uWSGI + Nginx Setup (optional)\nThe testing section above describes how one can run TileStache and Mapnik together using the tilestache-server command, and is fine for development purposes. However, for a more performant, robust solution, we instead use WSGI.\nThere are a few options we could employ here, but we will use uWSGI to run TileStache, and NGINX to reverse proxy.\n\nuWSGI\nOne of the easier ways to install uWSGI is to use pip:\nsudo apt-get install python-pip\nsudo pip install -U uwsgi\nNow, create a file called /etc/tilestache/tilestache-uwsgi.ini as follows (note we have decided to use port 9090 rather than the default of 8080):\n[uwsgi]\nhttp-socket = :9090\neval = import TileStache; application = TileStache.WSGITileServer(\"/etc/tilestache/tilestache.cfg\")\nGiven this file, TileStache can then be started so that it is available at http://localhost:9090 via:\nuwsgi --ini /etc/tilestache/tilestache-uwsgi.ini\n\n\nStarting uWSGI automatically on boot\nWe might also want to set things up so that the service starts automatically when the system is booted. Debian uses System V-style init. I’m not all that experienced with this, but I got things working by creating /etc/init.d/tilestache as follows:\n#!/bin/sh\n\n### BEGIN INIT INFO\n# Provides:          tilestache\n# Required-Start:    \n# Required-Stop:     \n# Default-Start:     2 3 4 5\n# Default-Stop:      0 1 6\n# Short-Description: Start TileStache.\n# Description:       Start TileStache.\n### END INIT INFO\n\nDAEMON=/usr/local/bin/uwsgi\nNAME=\"tilestache\"\n\nRUN_DIR=/var/run\nPIDFILE=$RUN_DIR/tilestache.pid\n\ncase \"$1\" in\n    start)\n      printf \"%-50s\" \"Starting $NAME...\"\n      PID=`$DAEMON --ini /etc/tilestache/tilestache-uwsgi.ini > /dev/null 2>&1 & echo $!`\n      if [ -z $PID]; then\n        printf \"%s\\n\" \"Fail\"\n      else\n        echo $PID > $PIDFILE\n        printf \"%s\\n\" \"done.\"\n      fi\n    ;;\n\n    stop)\n      printf \"%-50s\" \"Stopping $NAME...\"\n      PID=`cat $PIDFILE`\n      if [ -f $PIDFILE ]; then \n        kill -HUP $PID\n        printf \"%s\\n\" \"done.\"\n        rm -f $PIDFILE\n      else\n        printf \"%s\\n\" \"no such process.\"\n      fi\n      echo \"done.\"\n    ;;\n\n    restart)\n      $0 stop\n      $0 start\n    ;;\n\n    *)\n    echo \"Usage: $0 {start|stop|restart}\"\n    exit 1\n    ;;\nesac\n\nexit 0\nand running:\nsudo update-rc.d tilestache defaults\nFor those unfamiliar with Linux, this also means the service can be stopped and restarted at will with commands like:\nsudo service tilestache start\nsudo service tilestache stop\nsudo servcee tilestache restart\nUbuntu, and possily other Linux distributions, now uses upstart rather than System V-like init. It’s a simpler process to set up, so the details are left to the reader if this applies (typically, create /etc/init/tilestache.conf rather than /etc/init.d/tilestache, where tilestache.conf makes use of much simpler syntax–see upstart for details).\nNote that uWSGI has something called uWSGI Emperor. This makes it possible to control multiple services from a single uWSGI instance– just create ini files like tilestache-uwsgi.ini above. This certainly has some appeal.\n\n\nNGINX\nIf you don’t have NGINX, it can be installed as follows:\nsudo apt-get install nginx\nTo give the service a nice endpoint, and potentially make it available externally, we add the following to /etc/nginx/sites-available/default:\nserver{\n\n  ...\n  \n  location /tiles/ {\n    include uwsgi_params;\n    uwsgi_pass 127.0.0.1:9090;\n  }\n  \n  ...\n\n}\nApparently NGINX and uWSGI work together. I couldn’t quite figure this out, but it is, apparently, possible to do something like:\nserver{\n\n  ...\n  \n  location /tiles/ {\n    include uwsgi_params;\n    uwsgi_pass 127.0.0.1:9090;\n  }\n  \n  ...\n\n}\nTo test the config and restart NGINX, issue:\nsudo service nginx configtest && sudo service nginx reload\nAt this point, the service would now be availabe on a standard port at http://localhost/tiles/ or http://<domain name>/tiles/ if the server is available externally."
  },
  {
    "objectID": "posts/serving-geospatial-features-with-mapnik-and-tilestache/index.html#tilemill",
    "href": "posts/serving-geospatial-features-with-mapnik-and-tilestache/index.html#tilemill",
    "title": "Serving Geospatial Features with Mapnik and TileStache",
    "section": "TileMill",
    "text": "TileMill\nSetting up a Mapnik source for use with TileStache requires a Mapnik XML file, and these can be tricky to produce by hand; and iterative refinement is also cumbersome. So, it is also useful to install TileMill which provides a nice editor which can be used to create maps, and export the settings as a Mapnik XML file.\n\nInstallation\nTileMill was originally produced by Mapbox, though it seems they no longer actively develop it (instead, they encourage the purchase of Mapbox Studio, which is excellent, but not free–enterprises and professionals might want to consider it). That said, the project has been forked, and active development is continuing. We choose a fork called TileOven. To install it:\ncd ~/Downloads\ngit clone https://github.com/florianf/tileoven.git\ncd tileoven\nnpm install\ncd\nsudo mv ~/Downloads/tileoven /usr/local/\nOf course, this means one must also have both git and node.js. To install git:\nsudo apt-get install git\nTo install a newer version of node than is in the repositories, run:\ncd ~/Downloads\nwget https://nodejs.org/dist/v6.9.1/node-v6.9.1-linux-x64.tar.xz\ntar -xvf node-v6.9.1-linux-x64.tar.xz\nsudo mv node-v6.9.1-linux-x64 /usr/local/node\necho \"export PATH=$PATH:/usr/local/node/bin\" >> ~/.bashrc\nsource ~/.bashrc\n\n\nTesting the installation\nTo run TileOven, issue:\n/usr/local/tileoven/index.js\nThen browse to http://localhost:9090 in a web browser and you’ll see something like:\n\n\n\nAccessing TileMill remotely\nTileMill is currently built to work strictly locally. This makes it somewhat awkward to put it on a server and access it from a remote client. There are ways, though none is neccesarily ideal.\nIn one method, the variable listenHost can be set to match the server’s public IP address in the file /usr/local/tileoven/lib/config.defaults.json. This is far from secure.\nAlternatively, we could rely on SSH tunneling. The TileMill UI runs on port 20009, and the tile server runs on port 20008; so we just need to forward these two ports to get access. On a system with a proper SSH client such as Linux, we can do something like:\nssh hostname -l username -L 20009:localhost:20009 20008:localhost:20008\nThen we would simply point our browser at http://localhost:20009 to get access. From a security perspective, this isn’t necessarily an ideal option either, however. On my machine, I created a user called gisuser and added them to a group called gis. I then made sure this user had limited access, and used this account to start the TileMill server.\nThings are a bit trickier on a Windows platform since it doesn’t ship with an SSH client. Either way, we will describe how this can work using PuTTY. I installed using the installer found https://the.earth.li/~sgtatham/putty/latest/x86/putty-0.67-installer.msi.\nNow, let’s assume we have a server running TileMill which can be accessed on a network by the name debian, or by the IP address 192.168.20.22 (which is true for my local network). We start PuTTY and enter either the hostname or IP address as follows:\n\nThen, navigate to Connection→SSH→Auth→Tunnels, and enter ports 20008 and 20009 as required:\n\nClick the connect button, and you will be asked to enter a username and, and a terminal will open as follows:\n\nThe terminal isn’t terribly important in this instance since we just want to acces TileMill. Simply open a browser and point it at http://localhost:20009 as follows:\n\nThis was done using a remote machine on the same local network as the TileMill server, but the same process applies over the web–just use the public IP or fully qualified name of the server. For further detail, see:\nAccessing TileMill from remote install"
  },
  {
    "objectID": "posts/serving-geospatial-features-with-mapnik-and-tilestache/index.html#postgresql-postgis",
    "href": "posts/serving-geospatial-features-with-mapnik-and-tilestache/index.html#postgresql-postgis",
    "title": "Serving Geospatial Features with Mapnik and TileStache",
    "section": "PostgreSQL / PostGIS",
    "text": "PostgreSQL / PostGIS\nWhile we can render tiles directly from shapefiles and rasters, it is probably better to pull spatial data from a spatially enabled database. PostgreSQL is a high quality, enterprise ready relational database management system which is freely available; and PostGIS is an extender which adds spatial capabilities. We will describe how to install these products, as well as populate tables from shapefiles and rasters.\n\nInstall PostgreSQL and PostGIS\nTo install PostgreSQL and PostGIS, simply run:\nsudo apt-get install postgresql postgis odbc-postgresql\nThe last package, odbc-postgresql, is not strictly necessary, but it will often be useful to access Postgres via ODBC, and this package provides the necessary driver. If you would also like a JDBC driver, you can do:\nsudo apt-get install libpostgresql-jdbc-java\n\n\nCreate a spatially-enabled database\nWe create a single database called gis, and we grant read access to a user called gisuser with the creative password password. It makes sense to group spatial data into collections by creating schemas as appropriate. In this case, we create a schema called geography in which we place standard geographies (meshblock, Area Unit, Territorial Authority, and Regional Council) as multipolygons. Similarly, we cteate a schema called raster in which we will place all raster data.\nTo gain access to an interactive SQL shell as root:\nsudo -u postgres psql\nAnd to create the database, run the following:\nCREATE DATABASE gis;\n\\c gis\nCREATE EXTENSION postgis;\nCREATE EXTENSION postgis_topology;\nCREATE SCHEMA geography;\nWe can confirm the spatial extensions are present as follows:\nsudo -u postgres psql -U postgres -d gis -c \"SELECT postgis_version()\"\nwhich should return something like:\n            postgis_version            \n---------------------------------------\n 2.1 USE_GEOS=1 USE_PROJ=1 USE_STATS=1\n(1 row)\nFinally, we give access to the data to user gisuser as follows:\nCREATE USER gisuser WITH ENCRYPTED PASSWORD 'password';\nGRANT CONNECT ON DATABASE gis TO gisuser;\nGRANT USAGE ON SCHEMA geography TO gisuser;\nGRANT USAGE ON SCHEMA raster TO gisuser;\nGRANT SELECT ON ALL TABLES IN SCHEMA geography TO gisuser;\nGRANT SELECT ON ALL TABLES IN SCHEMA raster TO gisuser;\nNote that PostGIS supports rasters, but as far as I can tell, TileMill and Mapnik cannot read PostGIS rasters. So, we won’t bother loading any rasters to PostGIS here, and will instead just reference the GeoTIFF files directly.\n\n\nPopulate the database\nIt is then relatively easy to load data into the database. To load the shapefile <path>/MB2013.* into the database, for example:\nshp2pgsql -I -s 2193 <path>/REGC2013.shp geography.REGC2013 | \\\n   sudo -u postgres psql -U postgres -d gis\nAnd that’s it! Again, we can conduct some simple checks. For example, the average area of a meshblock can be calculated by appealing to the ST_Area function as follows:\npsql -U gisuser -d gis \\ \n   -c \"SELECT sum(ST_Area(geom)) / sum(1) as ave_area FROM geography.MB2013\"\nwhich, after being prompted for a password, returns:\n     ave_area     \n------------------\n 5829825.22609488\n(1 row)"
  },
  {
    "objectID": "posts/serving-geospatial-features-with-mapnik-and-tilestache/index.html#shapefiles",
    "href": "posts/serving-geospatial-features-with-mapnik-and-tilestache/index.html#shapefiles",
    "title": "Serving Geospatial Features with Mapnik and TileStache",
    "section": "Shapefiles",
    "text": "Shapefiles\nLet’s start with the river shapefile. In the TileMill UI, select ‘Projects’ from the sidebar, and then click the ‘+ New project’. Fill in the resulting dialog as follows:\n\nThe default map canvas will contain a modestly styled countries feature class which you can safely delete–click the layers control (), hover over #countries, and click delete ().\nNext, click the ‘+ Add layer’ button, and fill in the resulting dialog as in the following screenshot, before clicking the ‘Save & Style’ button:\n\nNow edit the style.mss tab so it contains:\nMap {\n  background-color: transparent;\n}\n\n#river {\n  line-color:#594;\n  line-width:0.5;\n  polygon-opacity:1;\n  [Tot_reach >=     0]{ polygon-fill:#EFF3FF; }\n  [Tot_reach >=  3000]{ polygon-fill:#BDD7E7; }\n  [Tot_reach >=  6000]{ polygon-fill:#6BAED6; }\n  [Tot_reach >=  9000]{ polygon-fill:#3182BD; }\n  [Tot_reach >= 12000]{ polygon-fill:#08519C; }\n}\nUpon clicking the save button, the map should update and (after a little panning and zooming) looks as follows:\n\nTo serve a map like this via TileStache, save style.mss as a Mapnik XML file by clicking the Export button and selecting ‘Mapnik XML’. Save the file as /etc/tilestache/river.xml. Then, edit /etc/tilestache/tilestache.cfg to include a river layer as follows:\n{\n  \"cache\":\n  {\n    \"name\": \"Disk\",\n    \"path\": \"file:///tmp/cache\",\n    \"umask\": \"0000\",\n    \"dirs\": \"portable\"\n  },\n  \"layers\":\n  {\n    \"river\":\n    {\n      \"provider\": {\"name\": \"mapnik\", \"mapfile\": \"river.xml\"}\n    },\n    \n    ...\n    \n  }\n}\nThis newly created layer should now be accessible via <base>/river/{z}/{x}/{y}.png. As described above, I set TileStache to be accessible via http://localhost/tiles. The leaflet package in R provieds us with a convenient way of testing things are working:\nlibrary(leaflet)\n\nleaflet(width=\"100%\", height=500) %>% addTiles() %>% \n  fitBounds(166.70047, -34.45676, 178.52966, -47.06345) %>%\n  addTiles(urlTemplate = \"http://localhost/tiles/river/{z}/{x}/{y}.png\")\n\n(Of course, you get a live preview of the tiles in TileMill, but we want to confirm that we can server these via TileStache. That is, we want to confirm that we can access the tile server remotely with a ‘Google Maps-style URL’.)"
  },
  {
    "objectID": "posts/serving-geospatial-features-with-mapnik-and-tilestache/index.html#postgis",
    "href": "posts/serving-geospatial-features-with-mapnik-and-tilestache/index.html#postgis",
    "title": "Serving Geospatial Features with Mapnik and TileStache",
    "section": "PostGIS",
    "text": "PostGIS\nNow that we’re familiar with the general layout, we’ll be a little lighter on detail. This time, create a project called composite and, as before, delete the default countries layer.\nWe will add each of MB2013, AU2013, TA2013, and REGC2013 from PostGIS tables, though it would be just as easy to add them via shapefiles. They are all entered the same way, so we show the setup for just the one, MB2013, as follows:\n\nAfter all 4 layers are added, edit style.mss so it contains the following:\nMap {\n  background-color: transparent;\n  maximum-extent: \"-19691655, -5989537, 19879128, -4081645\";\n}\n\n#MB2013 {\n  ::shape{\n    [zoom >= 15]{\n      line-color:#000000;\n      line-width:0.5;\n      line-opacity: 1;\n      polygon-opacity: 0;\n      polygon-fill: transparent;\n    }\n  }\n  ::label{\n    [zoom >= 15]{\n      text-name: [code];\n      text-face-name: 'Arial Black Regular';\n      text-fill: #036;\n      text-size: 9;\n      text-halo-fill: fadeout(white, 30%);\n      text-halo-radius: 2.5;\n      text-avoid-edges: false;\n      text-allow-overlap: true;\n      text-placement: interior;\n    }\n  }\n}\n\n#AU2013 {\n  ::shape{\n    [zoom >= 12][zoom < 15]{\n      line-color:#000000;\n      line-width:0.5;\n      line-opacity: 1;\n      polygon-opacity: 0;\n      polygon-fill: transparent;\n    }\n  }\n  ::label{\n    [zoom >= 12][zoom < 15]{\n      text-name: [label] + \"\\n(\" + [code] + \")\";\n      text-face-name: 'Arial Black Regular';\n      text-fill: #036;\n      text-size: 9;\n      text-halo-fill: fadeout(white, 30%);\n      text-halo-radius: 2.5;\n      text-avoid-edges: false;\n      text-allow-overlap: true;\n      text-placement: interior;\n    }\n  }\n}\n\n#TA2013 {\n  ::shape{\n    [zoom > 9][zoom < 12]{\n      line-color:#000000;\n      line-width:0.5;\n      line-opacity: 1;\n      polygon-opacity: 0;\n      polygon-fill: transparent;\n    }\n  }\n  ::label{\n    [zoom > 9][zoom < 12]{\n      text-name: [label] + \"\\n(\" + [code] + \")\";\n      text-face-name: 'Arial Black Regular';\n      text-fill: #036;\n      text-size: 9;\n      text-halo-fill: fadeout(white, 30%);\n      text-halo-radius: 2.5;\n      text-avoid-edges: false;\n      text-allow-overlap: true;\n      text-placement: interior;\n    }\n  }\n}\n\n#REGC2013 {\n  ::shape{\n    [zoom >= 0][zoom <= 9]{\n      line-color:#000000;\n      line-width:0.5;\n      line-opacity: 1;\n      polygon-opacity: 0;\n      polygon-fill: transparent;\n    }\n  }\n  ::label{\n    [zoom >= 0][zoom <= 9]{\n      text-name: [label];\n      text-face-name: 'Arial Black Regular';\n      text-fill: #036;\n      text-size: 9;\n      text-halo-fill: fadeout(white, 30%);\n      text-halo-radius: 2.5;\n      text-avoid-edges: false;\n      text-allow-overlap: true;\n      text-placement: interior;\n    }\n  }\n}\nAs before, ws export to Mapnik XML and save as /etc/tilestache/composite.xml, and edit /etc/tilestache/tilestache.cfg to inculde the new layer:\n{\n  \"cache\":\n  {\n    \"name\": \"Disk\",\n    \"path\": \"file:///tmp/cache\",\n    \"umask\": \"0000\",\n    \"dirs\": \"portable\"\n  },\n  \"layers\":\n  {\n  \n    ...\n    \n    \"composite\":\n    {\n      \"provider\": {\"name\": \"mapnik\", \"mapfile\": \"composite.xml\"}\n    }\n  }\n}\nI won’t profess to being very artistic here, but this map will show regional council areas (REGC2013) when zoomed out, and will change to display more detailed classifications as we zoom in. Again, it is easy to view the map in R:\nlibrary(leaflet)\n\nleaflet() %>% addTiles() %>%\n  fitBounds(166.70047, -34.45676, 178.52966, -47.06345) %>%\n  addTiles(urlTemplate = \"http://localhost/tiles/composite/{z}/{x}/{y}.png\")"
  },
  {
    "objectID": "posts/serving-geospatial-features-with-mapnik-and-tilestache/index.html#rasters",
    "href": "posts/serving-geospatial-features-with-mapnik-and-tilestache/index.html#rasters",
    "title": "Serving Geospatial Features with Mapnik and TileStache",
    "section": "Rasters",
    "text": "Rasters\nFinally, create a new project called nleach, and add the Nitrogen_Leaching_20150903.tif as follows:\n\nThen edit style.mss as follows:\nMap {\n  background-color: transparent;\n}\n\n#nleach {\n  raster-opacity:1;\n  raster-colorizer-default-mode: linear;\n  raster-colorizer-default-color: transparent;\n  raster-colorizer-stops:\n    stop(-0.000005, #FFFFB2)\n    stop(13.402771, #FECC5C)\n    stop(26.805547, #FD8D3C)\n    stop(40.208324, #F03B20)\n    stop(53.611100, #BD0026);\n}\nExport to Mapnik XML and save as /etc/tilestache/nleach.xml. Then add the layer to /etch/tilestache/tilestache.cfg:\n{\n  \"cache\":\n  {\n    \"name\": \"Disk\",\n    \"path\": \"file:///tmp/cache\",\n    \"umask\": \"0000\",\n    \"dirs\": \"portable\"\n  },\n  \"layers\":\n  {\n  \n    ...\n    \n    \"composite\":\n    {\n        \"provider\": {\"name\": \"mapnik\", \"mapfile\": \"composite.xml\"},\n        \"projection\": \"spherical mercator\"\n    }\n  }\n}\nAgain, we can visualise the result in R:\nlibrary(leaflet)\n\nleaflet() %>% addTiles() %>%\n  fitBounds(166.70047, -34.45676, 178.52966, -47.06345) %>%\n  addTiles(urlTemplate = \"http://localhost/tiles/nleach/{z}/{x}/{y}.png\")"
  },
  {
    "objectID": "posts/using-postgis-as-a-spatial-backend-for-r/index.html",
    "href": "posts/using-postgis-as-a-spatial-backend-for-r/index.html",
    "title": "Using PostGIS as a Spatial Backend for R",
    "section": "",
    "text": "R is a wildly extensible tool, and that extensibility means it can be used in a surprising array of domains. This versatility is great, but there are times when R is just not the best tool for the job–at least not by itself. Geospatial applications are a good example. Packages such as rgdal, sf, and raster make R quite usable in this domain, but R’s largely in-memory approach can make geoprocessing tasks involving large spatial objects a little challenging. In this blog, we look at how we can leverage PostgreSQL and the PostGIS extension to usefully complement R, mostly by offloading large geoprocessing tasks, and as a library for storing large feature classes for shared use across a potentially large userbase."
  },
  {
    "objectID": "posts/using-postgis-as-a-spatial-backend-for-r/index.html#vector-data",
    "href": "posts/using-postgis-as-a-spatial-backend-for-r/index.html#vector-data",
    "title": "Using PostGIS as a Spatial Backend for R",
    "section": "Vector data",
    "text": "Vector data\nWe can load vector data to PostGIS using the ogr2ogr program provided by gdal. For example, in the following we load a GeoPackage called meshblock-2020-generalised.gpkg to statsnz.meshblock2020:\nogr2ogr \\\n  -f PostgreSQL PG:\"dbname='gis' user='gisuser' password='gisuser'\" \\\n  meshblock-2020-generalised.gpkg \\\n  -nln statsnz.meshblock2020\nIn this case the geodatabase has a single layer, so we don’t need to specify it explicitly. And note that on occasion it might be necessary to specify the type explicitly, for example by adding -nlt MULTIPOLYGON.\nWe can also load vector data using the shp2pgsql utility that is provided with PostGIS. It only works for shapefiles, however. So, to load the data this way we’d first convert it to a shapefile as follows:\nogr2ogr \\\n  -f 'ESRI Shapefile' mb2020.shp \\\n  meshblock-2020-generalised.gpkg\nand then use shp2pgsql to load it to a PostGIS table:\nshp2pgsql -s 2193 mb2020.shp statsnz.meshblock2020 | psql -d gis\nNote that shp2pgsql actually just creates a large SQL script, and that is executed in the same way as any other. ogr2ogr can also be used to create a SQL script which can be loaded directly to our database:\nogr2ogr -f PGDUMP mb2020.sql \\\n  meshblock-2020-generalised.gpkg \\\n  -nln statsnz.meshblock2020\nActually, this approach should be preferred since transferring to shapefile as an intermediate step will cause attribute names to be truncated / normalised, and if the GeoPackage contains indexes, comments, and so forth, they will be retained.\nFinally, one could load the data via R itself, though it probably isn’t the best approach in general. Still, to load the same GeoPackage:\nlibrary(sf)\n\nconn <- RPostgreSQL::dbConnect(\n  \"PostgreSQL\", host = \"localhost\", port = 5432,\n  dbname = \"gis\", user = \"gisuser\", password = \"gisuser\"\n)\n\nmb <- st_read(\"meshblock-2020-generalised.gpkg\")\nst_write(mb, conn, c(\"statsnz\", \"meshblock2020\"))\nDBI::dbDisconnect(conn)"
  },
  {
    "objectID": "posts/using-postgis-as-a-spatial-backend-for-r/index.html#raster-data",
    "href": "posts/using-postgis-as-a-spatial-backend-for-r/index.html#raster-data",
    "title": "Using PostGIS as a Spatial Backend for R",
    "section": "Raster data",
    "text": "Raster data\nGDAL only offers read-only support for PostGIS currently (I think), so there are two options for loading rasters: using the raster2pgsql command-line tool provided by PostGIS, or using the rpostgis::pgWriteRast function in R.\nOne advantage of the raster2pgsql approach is that we can wildcard a large number of input files. For example, for this blog, the Wellington aerial imagery was downloaded as an archive containing 1667 jpeg files. These can be loaded as linz.wellington-010m-urban-aerial-2017 as follows:\nraster2pgsql \\\n  -I -C -e -Y -F -s 2193 -t 100x100 -l 2,4 \\\n  *.jpg linz.wellington_010m_2017 | psql -U gisuser -d gis\nThere are some interesting options here. Using -e means each file will be run as an independent query, rather than all running as a single transaction. Using -l 2,4 means that lower resolution (1/2 and 1/4) versions of the raster will be added to new tables called o_2_wellington_010m_2017 and o_4_wellington_010m_2017, respectively. See Raster Data Management, Queries, and Applications.\nLoading the data via R is possible, though not as straightforward. In this case, while not recommended, we could do something like:\nlibrary(rpostgis)\n\nconn <- RPostgreSQL::dbConnect(\n  \"PostgreSQL\", host = \"localhost\", port = 5432,\n  dbname = \"gis\", user = \"gisuser\", password = \"gisuser\"\n)\n\nfiles <- dir(\"/home/cmhh/Downloads/shp/linz\", full.names = TRUE)\nfiles <- files[grepl(\"*.jpg$\", files)]\nl <- lapply(files, function(f) {\n  pgWriteRast(\n    conn, \n    c(\"linz\", \"wellington_010m_2017\"), \n    stack(f), \n    append = TRUE\n  )\n})\n\nDBI::dbDisconnect(conn)"
  },
  {
    "objectID": "posts/using-postgis-as-a-spatial-backend-for-r/index.html#transforming-reprojecting-and-simplifying-features",
    "href": "posts/using-postgis-as-a-spatial-backend-for-r/index.html#transforming-reprojecting-and-simplifying-features",
    "title": "Using PostGIS as a Spatial Backend for R",
    "section": "Transforming / reprojecting and simplifying features",
    "text": "Transforming / reprojecting and simplifying features\nConsider the osm.gis_osm_landuse_a_free_1 feature class. This is reasonably large, consisting of some 373272 features, and totalling 327MB as a shapefile on disk. The features themselves are polygons that describe various types of land use, a section of which looks as follows:\n\nAssume we wish to do the following:\n\nread the shapefile\nconvert to EPSG 2193 (New Zealand Tranverse Mercator 2000)\nsimplify with 1 metre tolerance.\n\nTo do this in R, we might do the following:\nlibrary(sf)\n\nlanduse_nztm_1m <- \n  st_read(\"gis_osm_landuse_a_free_1.shp\") %>%\n  st_transform(2193) %>%\n  st_simplify(preserveTopology = TRUE, dTolerance = 1)\nPeak memory use during this operation was approximately 2.5GB, and the operation took 63 seconds. This same task can be conducted from R, but leaning heavily on PostGIS, by pulling the results of a SQL query as follows:\nconn <- RPostgreSQL::dbConnect(\n  \"PostgreSQL\", host = \"localhost\", port = 5432,\n  dbname = \"gis\", user = \"gisuser\", password = \"gisuser\"\n)\n  \nlanduse_nztm <- st_read(\n  conn, \n  query = \"\n    SELECT \n      gid, osm_id, code, fclass, name, \n      ST_Simplify(ST_Transform(wkb_geometry, 2193), 1.0, FALSE) as geom \n    FROM \n      osm.gis_osm_landuse_a_free_1\n  \"\n)\nPeak memory usage throughout was 1.5GB, and the total execution time was 19.4 seconds. In this case, then, PostGIS consumed 1GB (40%) less memory, and was 43.6 seconds (69%) faster than an in-memory approach using the sf package."
  },
  {
    "objectID": "posts/using-postgis-as-a-spatial-backend-for-r/index.html#dissolving-features",
    "href": "posts/using-postgis-as-a-spatial-backend-for-r/index.html#dissolving-features",
    "title": "Using PostGIS as a Spatial Backend for R",
    "section": "Dissolving features",
    "text": "Dissolving features\nA feature class can be dissolved by some categorical attribute using the sf package simply by performing a grouped aggregate, in this case with dplyr:\nlibrary(dplyr)\n\nmb_dissolved_sf <- \n  st_read(\"meshblock-2020-generalised.gpkg\") %>%\n  group_by(LANDWATER_NAME) %>%\n  summarise(n = n())\nPeak memory use for this was about 530MB, and total execution time was 65.1 seconds.\nmb_dissolved <- st_read(\n  conn, \n  query = \"\n    SELECT\n      landwater_name, ST_Multi(ST_Union(geom)) as geom\n    FROM \n      statsnz.meshblock2020\n    GROUP BY \n      landwater_name\n  \"\n)\nPeak memory use for this was about 480MB, and total execution time was 67.9 seconds. Importantly, peak memory usage for R itself was much smaller at 132.3MB.\nThe result either way can be plotted using the geom_sf geometry for ggplot2. Before dissolving we have:\nlibrary(ggplot2)\n\nggplot() + \n  geom_sf(\n    data = st_read(\"meshblock-2020-generalised.gpkg\"), \n    aes(fill = LANDWATER_NAME)\n  ) +\n  guides(fill = guide_legend(title = \"\"))\n\nAfter dissolving we have:\nggplot() + \n  geom_sf(\n    data = mb_dissolved_sf, \n    aes(fill = LANDWATER_NAME), linetype = \"blank\"\n  ) +\n  guides(fill = guide_legend(title = \"\"))"
  },
  {
    "objectID": "posts/using-postgis-as-a-spatial-backend-for-r/index.html#clipping-a-large-raster",
    "href": "posts/using-postgis-as-a-spatial-backend-for-r/index.html#clipping-a-large-raster",
    "title": "Using PostGIS as a Spatial Backend for R",
    "section": "Clipping a large raster",
    "text": "Clipping a large raster\nThe Wellington 0.10m Urban Aerial Photos (2017) raster was downloaded in JPEG format. Unpacked, the ‘raster’ is 5.4GB, and is made up of 1667 individual JPEG files, each about 3.5MB in size. Collectively, they look as follows:\n\nIn principle, these can be imported into R using the raster package. For example, to create a raster from the first 2 JPEG files:\nf <- dir(\"/home/cmhh/Downloads/shp/linz\", full.names = TRUE)\nf <- f[grepl(\"^.+(.jpg)$\", f)]\nr <- merge(brick(f[1]), brick(f[2]))\nwhich yields:\n\nIn practice, however, this approach is excruciatingly slow, and far from practical. (Loading the raster to PostGIS is also excruciatingly slow, but it only needs to be done once, and then many users can use the result.) Now consider this same raster with the statsnz.meshblock2020 feature class overlaid:\n\nAssume that we just wish to use a single meshblock, 2122700, to mask the raster. This can be done in R using the gdalUtils::gdalwarp function (which is just a wrapper for the gdalwarp command-line tool) as follows:\nlibrary(gdalUtils)\n\ngdalwarp(\n  srcfile = \"\n    PG: dbname='gis' host=127.0.0.1 port=5432 \n    user='gisuser' password='gisuser' mode=2 \n    schema='linz' column='rast' \n    table='wellington_010m_2017'\n  \",\n  dstfile = \"2122700.tif\",\n  s_src = \"EPSG:2193\", \n  t_srs = \"EPSG:2193\",\n  multi = TRUE,\n  cutline = \"\n    PG:dbname='gis' host=127.0.0.1 port=5432 \n    user='gisuser' password='gisuser'\n  \",\n  csql = \"\n    select geom \n    from statsnz.meshblock2020 \n    where mb2020_v1_00 = '2122700'\n  \",\n  crop_to_cutline = TRUE,\n  dstnodata = \"nodata\"\n)\nNote that this took only a few seconds in practice, and outputs a single TIF file which can be read into R using raster::stack(\"2122700.tif\"), and looks as follows:"
  },
  {
    "objectID": "posts/using-postgis-as-a-spatial-backend-for-r/index.html#installing-docker",
    "href": "posts/using-postgis-as-a-spatial-backend-for-r/index.html#installing-docker",
    "title": "Using PostGIS as a Spatial Backend for R",
    "section": "Installing Docker",
    "text": "Installing Docker\nDocker is relatively easy to install on Linux systems. Reasonably new versions will sometimes be available from standard repositories, but a newer version can be obtained for most distributions by following the instructions:\nInstall Docker Engine\nWhile Windows users can run Linux containers via Docker Desktop, they are strongly encouraged to enable the Windows Subsystem for Linux version 2 (WSL2), and instead install Docker on Linux. For a rundown on using Docker via WSL2 see:\nDocker on Windows with Windows Subsystem for Linux 2\nI have no experience with Docker on a Mac, but Docker Desktop is available. It has apparently had performance issues in the past, though newer versions might be much better. The release notes for version 2.3.5.0 (2020-08-21) have the following promising inclusion:\n\nDocker Desktop now uses gRPC-FUSE for file sharing by default. This has much faster file sharing and uses much less CPU than osxfs, especially when there are lots of file events on the host. To switch back to osxfs, go to Preferences > General and disable gRPC-FUSE."
  },
  {
    "objectID": "posts/using-postgis-as-a-spatial-backend-for-r/index.html#installing-postgresql-and-postgis",
    "href": "posts/using-postgis-as-a-spatial-backend-for-r/index.html#installing-postgresql-and-postgis",
    "title": "Using PostGIS as a Spatial Backend for R",
    "section": "‘Installing’ PostgreSQL and PostGIS",
    "text": "‘Installing’ PostgreSQL and PostGIS\nThere are Docker images available which contain both PostgreSQL and PostGIS, though it is instructive to create one. First, create a file called Dockerfile with the following content:\nFROM ubuntu:20.04\n\nENV DEBIAN_FRONTEND=noninteractive\nENV SHELL=/bin/bash\n\nRUN  apt-get update && apt-get -y dist-upgrade && \\\n  apt-get install -y --no-install-recommends \\\n    wget gnupg2 ca-certificates gdal-bin sudo vim && \\\n  sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt focal-pgdg main\" > /etc/apt/sources.list.d/pgdg.list' && \\\n  wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - && \\\n  apt-get update && apt-get install -y --no-install-recommends postgresql-13 postgresql-13-postgis-3 postgis && \\\n  apt-get clean && \\\n  rm -rf /var/lib/apt/lists/*\n\nRUN service postgresql start && \\\n  sudo -u postgres psql -c 'create database gis;' && \\\n  sudo -u postgres psql -d gis -c 'create extension postgis;' && \\\n  sudo -u postgres psql -d gis -c 'create extension postgis_raster;' && \\\n  sudo -u postgres psql -d gis -c 'create extension postgis_sfcgal;' && \\\n  sudo -u postgres psql -d gis -c 'create extension postgis_topology;' && \\\n  sudo -u postgres psql -d gis -c \"SET postgis.gdal_enabled_drivers = 'ENABLE_ALL';\" && \\\n  sudo -u postgres psql -c 'create user gisuser;' && \\\n  sudo -u postgres psql -c \"alter user gisuser with encrypted password 'gisuser';\" && \\\n  sudo -u postgres psql -c 'grant all privileges on database gis to gisuser;' && \\\n  printf \"\\tlisten_addresses='*'\\t\" >> /etc/postgresql/13/main/postgresql.conf && \\\n  sed -i -E '/local +all +all +peer/ s/peer/md5/' /etc/postgresql/13/main/pg_hba.conf && \\\n  sed -i -E '/host +all +all +127.0.0.1\\/32 +md5/ s/127.0.0.1\\/32/0.0.0.0\\/0   /' /etc/postgresql/13/main/pg_hba.conf && \\\n  sed -i -E '/host +all +all +::1\\/128 +md5/ s/::1\\/128/::0\\/0  /' /etc/postgresql/13/main/pg_hba.conf &&\\ \n  printf \"localhost:5432:gis:gisuser:gisuser\" >> /root/.pgpass && \\\n  chmod 0600 /root/.pgpass\n\nEXPOSE 5432\n\nCMD service postgresql start && \\\n  tail -f /dev/null  \nTo build the image, change into the directory containing Dockerfile and run:\ndocker build -t postgis .\nIf we wish to persist our database, we can create a volume as so:\ndocker volume create pgdata\nThen, to run an instance with the volume mounted, as well as a folder containing all the required source features (if desired):\ndocker run \\\n  -d --rm --name postgis \\\n  -p 5432:5432 \\\n  -v /path/to/downloaded/data:/data \\\n  -v pgdata:/var/lib/postgresql/13/main \\\n  postgis\nTo gain terminal access to the running container, run:\ndocker exec -it postgis bash"
  },
  {
    "objectID": "posts/using-postgis-as-a-spatial-backend-for-r/index.html#installing-r-and-required-spatial-dependencies",
    "href": "posts/using-postgis-as-a-spatial-backend-for-r/index.html#installing-r-and-required-spatial-dependencies",
    "title": "Using PostGIS as a Spatial Backend for R",
    "section": "‘Installing’ R and required spatial dependencies",
    "text": "‘Installing’ R and required spatial dependencies\nTo create a Linux container with R and RStudio Server, create a file called Dockerfile with content as follows:\nFROM ubuntu:20.04\n\nARG rstudio_version=1.3.1093\nENV DEBIAN_FRONTEND=noninteractive\nENV SHELL=/bin/bash\nENV RSTUDIO_VERSION=$rstudio_version\n\n# install necessary packages\nRUN  apt-get update && apt-get -y dist-upgrade && \\\n  apt-get install -y --no-install-recommends \\\n    gnupg2 ca-certificates gdebi-core wget odbc-postgresql libblas3 \\\n    grass gdal-bin libgdal-dev libgeos-dev libproj-dev proj-bin proj-data && \\\n  sed -i -e 's/# en_US.UTF-8 UTF-8/en_US.UTF-8 UTF-8/' /etc/locale.gen && \\\n  dpkg-reconfigure --frontend=noninteractive locales && \\\n  update-locale LANG=en_US.UTF-8 && \\\n  wget -qO- \"https://yihui.org/gh/tinytex/tools/install-unx.sh\" | sh -s - --admin --no-path && \\\n  mv /root/.TinyTeX /usr/local/TinyTex && \\\n  /usr/local/TinyTex/bin/*/tlmgr path add && \\\n  apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9 && \\\n  printf \"deb https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/\" >> /etc/apt/sources.list && \\\n  apt-get update && apt-get install -y --no-install-recommends r-base && \\\n  wget https://download2.rstudio.org/server/bionic/amd64/rstudio-server-${RSTUDIO_VERSION}-amd64.deb && \\\n  gdebi rstudio-server-${RSTUDIO_VERSION}-amd64.deb && \\\n  rm rstudio-server-${RSTUDIO_VERSION}-amd64.deb && \\\n  apt-get clean && \\\n  rm -rf /var/lib/apt/lists/* && \\\n  R -e \"install.packages(c('renv', 'rgdal', 'rgeos', 'sf', 'leaflet', 'RPostgreSQL', 'rpostgis'))\" && \\\n  wget https://download2.rstudio.org/server/bionic/amd64/rstudio-server-${RSTUDIO_VERSION}-amd64.deb && \\\n  gdebi --non-interactive rstudio-server-${RSTUDIO_VERSION}-amd64.deb && \\\n  rm rstudio-server-${RSTUDIO_VERSION}-amd64.deb\n\n# add user for demo purposes\nRUN adduser --disabled-password --gecos \"\" guest && \\\n  usermod --password $(openssl passwd -1 guest) guest && \\\n  usermod -aG sudo guest \n\nEXPOSE 8787\n\nCMD service rstudio-server start && \\\n  tail -f /dev/null\nTo build this container, simply change into the directory containing Dockerfile and run:\ndocker build -t rstudio .\nThe image can be started by running:\ndocker run -d --rm --name rstudio -p 8787:8787 rstudio\nWhen done, RStudio will be available via a web browser at http://localhost:8787."
  },
  {
    "objectID": "posts/agent-based-simulations-with-akka/index.html",
    "href": "posts/agent-based-simulations-with-akka/index.html",
    "title": "Agent Based Simulations with Akka",
    "section": "",
    "text": "In this post we describe a relatively simple agent-based simulation. The simulation itself is a rough approximation of a household survey, consisting of a large collection of dwellings, in turn surveyed in a strictly face-to-face setting by one of a number of independent field collectors.\nWe describe the simulation in terms of the actor model, though we do so in a way that is largely agnostic with respect to programming language or actor framework. Still, a concrete implementation is useful for illustration, and one is provided here. This is implemented using Akka, an implementation of the actor model on the JVM, and so some jargon specific to Akka will appear here and there. Nevertheless, it should be easy enough to implement something similar in other frameworks"
  },
  {
    "objectID": "posts/agent-based-simulations-with-akka/index.html#coordinator",
    "href": "posts/agent-based-simulations-with-akka/index.html#coordinator",
    "title": "Agent Based Simulations with Akka",
    "section": "Coordinator",
    "text": "Coordinator\nThere is a single coordinator, an instance of Coordinator, which is our entry-point for the actor system itself. On start-up, the coordinator spawns 6 area coordinators, and its sole responsibility after set-up is to listen for messages from outside the system, and pass them to the appropriate area coordinator. The internal state of the coordinator is just a list of the area coordinators spawned."
  },
  {
    "objectID": "posts/agent-based-simulations-with-akka/index.html#areacoordinator",
    "href": "posts/agent-based-simulations-with-akka/index.html#areacoordinator",
    "title": "Agent Based Simulations with Akka",
    "section": "AreaCoordinator",
    "text": "AreaCoordinator\nThe simulation as configured contains 6 area coordinators, which are instances of AreaCoordinator, and which divide New Zealand up as follows:\n\nArea coordinators are responsible for spawning field collectors and for deciding which field collectors will spawn particular dwellings. The internal state of an area coordinator is a list of the the collectors spawned, and a map storing the assignment of dwellings to collectors."
  },
  {
    "objectID": "posts/agent-based-simulations-with-akka/index.html#fieldcollectoractor",
    "href": "posts/agent-based-simulations-with-akka/index.html#fieldcollectoractor",
    "title": "Agent Based Simulations with Akka",
    "section": "FieldCollectorActor",
    "text": "FieldCollectorActor\nField collectors are instances of FieldCollectorActor, and are spawned by area coordinators. They are assigned a certain number of dwellings, after which it is their responsibility to set about getting responses from those cases. They will typically be sent a message instructing them to simulate a day’s work, assuming a particular starting day and time. This will involve placing all unfinished cases in some efficient driving order, and then visiting each in turn. A field collector will send a dwelling an interview request, and react accordingly. In all cases, time will be advanced by a random amount appropriate for the type of response, and interviewers will stop working once some maximum amount of work has been exceeded.\nThe internal state of a field collector contains information about itself (id, address, location, name) as well as a configuration object that contains:\n\ncases it has been assigned\noverall summaries of each case (complete, incomplete, etc.)\nwork items for the current day\nthe dwelling being surveyed\nthe current individual being surveyed\nthe simulated date and time of day\nthe total kilometres travelled in the current day\nthe total minutes spent working in the current day"
  },
  {
    "objectID": "posts/agent-based-simulations-with-akka/index.html#dwellingactor",
    "href": "posts/agent-based-simulations-with-akka/index.html#dwellingactor",
    "title": "Agent Based Simulations with Akka",
    "section": "DwellingActor",
    "text": "DwellingActor\nDwellings are instances of DwellingActor, and are spawned by the field collector responsible. Once they are spawned, they will in turn spawn a random set of individuals to function as the dwelling’s residents. A dwelling can respond to interview requests from the field collector which spawned them, avoiding contact, refusing, or providing a full response. After a full response, the dwelling actor will pass the references for any individuals it spawned to the field collector.\nThe internal state of a dwelling actor contains information about itself (id, address, location), along with a list of all individuals spawned. A dwelling is also aware of whether or not it has responded (that is, sent a complete questionnaire response to its parent collector)."
  },
  {
    "objectID": "posts/agent-based-simulations-with-akka/index.html#individualactor",
    "href": "posts/agent-based-simulations-with-akka/index.html#individualactor",
    "title": "Agent Based Simulations with Akka",
    "section": "IndividualActor",
    "text": "IndividualActor\nIndividuals function as dwelling residents, and are instances of IndividualActor. All they can do is respond to requests for an interview from field collectors, avoiding contact, refusing, or providing a full response. Individuals are only discovered by field collectors after getting a response from the dwelling that spawned them.\nThe internal state of an individual contains information about itself (name, age, sex, date of birth), and a reference to its parent dwelling. Like dwellings, individuals are also aware of whether or not they have responded (i.e., provided a questionnaire response to a field collector)."
  },
  {
    "objectID": "posts/agent-based-simulations-with-akka/index.html#eventrecorder",
    "href": "posts/agent-based-simulations-with-akka/index.html#eventrecorder",
    "title": "Agent Based Simulations with Akka",
    "section": "EventRecorder",
    "text": "EventRecorder\nThe system has a single instance of EventRecorder, and this actor is responsible for persisting information. This might not reflect idiomatic, or best practice; but it works well enough in this case. For example, when a field collector gets a response from an individual, the individual will send a message to the field collector indicating the type of response, but also a JSON string representing a questionnaire response if fully responding. The response itself is then sent, along with identifiers for the dwelling and individual, to the event recorder to persist. Currently, the event recorder stores the following:\n\n\n\n\n\n\n\n\ntable\nfrom\npurpose\n\n\n\n\ncollectors\nAreaCoordinator\nA list of all spawned collectors.\n\n\ndwellings\nFieldCollectorActor\nA list of all spawned dwellings.\n\n\ndwelling_assignment\nFieldCollector\nCollector and dwelling pairs reflecting work allocation.\n\n\ndwelling_response\nFieldCollector\nCompleted dwelling questionnaires.\n\n\nindividual_response\nFieldCollector\nCompleted individual questionnaires.\n\n\ninterview\nFieldCollector\nAll attempted contacts and their outcome.\n\n\ntrips\nFieldCollector\nAll car trips made by field collectors."
  },
  {
    "objectID": "posts/agent-based-simulations-with-akka/index.html#spawn-event-recorder-and-area-coordinators",
    "href": "posts/agent-based-simulations-with-akka/index.html#spawn-event-recorder-and-area-coordinators",
    "title": "Agent Based Simulations with Akka",
    "section": "Spawn Event Recorder and Area Coordinators",
    "text": "Spawn Event Recorder and Area Coordinators\nThe actor system starts with a single coordinator, which is created as follows:\nval system = ActorSystem(Coordinator(conf.dbpath()), \"collectionsim\")\nThis coordinator spawns a single event recorder and 6 area coordinators when created. The code looks as follows:\nval eventRecorder: ActorRef[EventRecorderCommand] =\n  context.spawn(EventRecorder(dbpath, 100), \"eventrecorder\")\n\nval areaCoordinators: Map[Area, ActorRef[AreaCoordinatorCommand]] = \n  area.areas.map(x => \n    (x, context.spawn(AreaCoordinator(eventRecorder), s\"areacoordinator@${x.toString}\"))\n  ).toMap\nNote that dbpath is the name of the SQLite database which will be created, and 100 is the batch size for writing to the database. That is, a buffer is maintained that contains up to 100 SQL queries–when the buffer is full, the queries are committed to the database."
  },
  {
    "objectID": "posts/agent-based-simulations-with-akka/index.html#spawn-field-collectors",
    "href": "posts/agent-based-simulations-with-akka/index.html#spawn-field-collectors",
    "title": "Agent Based Simulations with Akka",
    "section": "Spawn Field Collectors",
    "text": "Spawn Field Collectors\nField collector actors are ultimately spawned by passing a message to the coordinator, which is defined as follows:\ncase class FieldCollector(\n  id: Int, address: String, location: Coordinates, area: String\n) extends CoordinatorCommand with AreaCoordinatorCommand\nThis message is passed to the coordinator, the coordinator then forwards it to the correct area coordinator, and the area coordinator then spawns a new collector. The area coordinator also sends a message to the event recorder to be persisted. Visually:\n\n(Note I got a Wacom tablet recently, which is awesome, but I’m new to any sort of digital drawing process, and the hand-eye coordination is taking some time. So, this is the best I can do for now! Sorry.)\nCollectors can be provided by a CSV (or gzipped CSV) file, and an iterator can be used to read such files row-by-row, converting each row to an instance of FieldCollector, before being passed to the coordinator. For example, the simulation source includes a file, interviewers.csv.gz, consisting of 100 locations, and the first few rows look as follows:\n\n\n\n\n\n\n\n\n\n\naddress_id\nfull_address\nregion\nlng\nlat\n\n\n\n\n236594\n258 Hereford Street, Christchurch Central, Christchurch\n13\n172.6452\n-43.53221\n\n\n1535707\n26 Harris Road, Pokuru\n3\n175.2201\n-38.02346\n\n\n617660\n125 Westminster Avenue, Tamatea, Napier\n6\n176.8741\n-39.50367\n\n\n582666\n49 Cambridge Terrace, Kaiti, Gisborne\n5\n178.0384\n-38.67728\n\n\n2096666\n206/155 Beach Road, Auckland Central, Auckland\n2\n174.7741\n-36.84973\n\n\n992032\n14 Glen Close, Glen Eden, Auckland\n2\n174.6365\n-36.91012\n\n\n\nWe can spawn a new collector for each row by running something like:\nimplicit val csvConfig = CsvConfig(\"address_id\", \"full_address\", \"lng\", \"lat\", \"region\")\n\nval cit = FieldCollectorIterator(\"interviewers.csv.gz\")\n\nwhile (cit.hasNext) {\n  system ! cit.next()\n}\nFor interest, the interviewer locations look as follows:"
  },
  {
    "objectID": "posts/agent-based-simulations-with-akka/index.html#spawn-dwellings",
    "href": "posts/agent-based-simulations-with-akka/index.html#spawn-dwellings",
    "title": "Agent Based Simulations with Akka",
    "section": "Spawn Dwellings",
    "text": "Spawn Dwellings\nThe process of spawning a dwelling actors is triggered by sending a message to the coordinator, defined as follows:\ncase class Dwelling(\n  id: Int, address: String, location: Coordinates, area: String\n) extends CoordinatorCommand with AreaCoordinatorCommand with FieldCollectorCommand\nThis message is passed to the coordinator, the coordinator then forwards it to the correct area coordinator, and the area coordinator then decides which of its collectors to forward the message to. Additionally, when a dwelling is spawned, the field collector sends a message to the event recorder to be persisted. Visually:\n\nThe rule for deciding which specific field collector will be sent a particular dwelling is currently very simple, and works as follows:\n\nfilter the list of collectors to the set with less than $n$ cases already assigned\nassign the dwelling to the closest (according to a routing service) remaining collector.\n\nLike collectors, dwellings can be provided by a CSV (or gzipped CSV) file, and an iterator can be used to read such files row-by-row, converting each row to an instance of Dwelling, and passing it to the coordinator. For example, say we had a file called dwellings.csv.gz, and the first few rows looked as follows:\n\n\n\n\n\n\n\n\n\n\naddress_id\nfull_address\nregion\nlng\nlat\n\n\n\n\n2186256\n75B Insoll Avenue, Enderley, Hamilton\n03\n175.2944\n-37.76214\n\n\n814042\n7 Bertram Street, Hillcrest, Rotorua\n04\n176.2322\n-38.14669\n\n\n695912\n114 Naylor Street, Hamilton East, Hamilton\n03\n175.3075\n-37.79751\n\n\n1283168\n46 Mako Street, Oneroa, Waiheke Island\n02\n175.0076\n-36.78458\n\n\n336865\n85 Wither Road, Witherlea, Blenheim\n18\n173.9541\n-41.53808\n\n\n336339\n1/11 Milford Street, Witherlea, Blenheim\n18\n173.9540\n-41.53505\n\n\n\nThen we could spawn a new collector for each row by running something like:\nimplicit val csvConfig = CsvConfig(\"address_id\", \"full_address\", \"lng\", \"lat\", \"region\")\n\nval dit = DwellingIterator(\"dwellings.csv.gz\")\n\nwhile (dit.hasNext) {\n  system ! dit.next()\n}\nA number of sample files are provided along with the simulation source. For example sample1_01.csv.gz and sample1_02.csv.gz both consist of roughly 1600 locations each, but one is the result of a two-stage selection and one is a simple random sample, the result being the latter is more spatially dispersed. Spatially, the two samples look as follows:\n\nNote that these two samples give us the opportunity to look a little at the plausibility of our simulation outcomes. All else constant, we’d expect collectors to have to travel more to service the non-clustered sample, and probably require more days to finish their workload overall as a result."
  },
  {
    "objectID": "posts/agent-based-simulations-with-akka/index.html#spawn-individuals",
    "href": "posts/agent-based-simulations-with-akka/index.html#spawn-individuals",
    "title": "Agent Based Simulations with Akka",
    "section": "Spawn Individuals",
    "text": "Spawn Individuals\nImmediately after being spawned, a dwelling actor will then spawn a random collection of individual actors, or else remain empty / vacant. The process proceeds as follows…\nFirstly, with some probability (default of 0.1), we decide the dwelling is empty, and do nothing.\nFor non-empty dwellings, the first step is to assign a household type. This is done randomly from the following discrete distribution (the default probabilities are drawn for the 2018 Census):\n\n\n\nhousehold type\nprobability\n\n\n\n\none-person household\n0.2274\n\n\none-family household\n0.6862\n\n\ntwo-family household\n0.0352\n\n\nother multi-person household\n0.0512\n\n\n\nIn the case of a one-person household, a single individual is randomly generated. This process involves generating a random name, sex, and a random birth-date which makes the person at least 18 years of age (using the current system time&endash;this could, of course, be modified). In the case of a multi-person household, we simply generate a random number of individuals in the same way.\nIn the case of a one-family household, we randomly generate a ‘family’, and for a two-family household, we randomly generate two. To generate a family, we first generate a random family type according to the following distribution:\n\n\n\nfamily type\nprobability\n\n\n\n\ncouple only\n0.3725\n\n\ncouple only and others\n0.0376\n\n\ncouple with children\n0.3985\n\n\ncouple with children and others\n0.0373\n\n\none parent with children\n0.1247\n\n\none parent with children and others\n0.0293\n\n\n\nChildren are simply random individuals with a random birth date which makes them less than 15-years of age, and who have a specified family name. The process for generating couples is very naive, being more illustrative than definitive&endash;we simply generate a male and female with a common surname, and who are within $\\pm$ 5 years of age of each other."
  },
  {
    "objectID": "posts/agent-based-simulations-with-akka/index.html#simulating-a-days-work",
    "href": "posts/agent-based-simulations-with-akka/index.html#simulating-a-days-work",
    "title": "Agent Based Simulations with Akka",
    "section": "Simulating a Day’s Work",
    "text": "Simulating a Day’s Work\nWe kick off a simulated day of work by passing the coordinator a message defined as follows:\ncase class RunDay(datetime: LocalDateTime) \nextends CoordinatorCommand with AreaCoordinatorCommand with FieldCollectorCommand\nThis message is then forwarded to each area coordinator, and each area coordinator then forwards it to each of their collectors. For example:\nsystem ! RunDay(LocalDateTime.of(2022,01,10,9,0))\n\nEach field collector will then proceed to work through their assigned dwelling cases, assuming a start time of 9:00 am on January 10, 2022. For each field collector, this means…\nA field collector first subsets cases to those that are active. The active cases are placed in driving order by appealing to a routing service, and attempted one-by-one. This means travelling to the address of each case, attempting to interview the dwelling and any residents, and then moving to the next. The total minutes spent interviewing and total distance travelled are tracked, and if the maximum allowable minutes an interviewer can work in any day has been exceeded, the interviewer will return home, rather than travel to the next case. Each time a field collector travels to a new address, a record representing the trip, including origin and destination coordinates, departure and arrival time, and distance travelled is sent to the event recorder to persist. Likewise, all attempts to interview a dwelling or individual–the time the attempt is made, along with the outcome–is logged with the event recorder.\nIf a dwelling response has not been previously obtained, then the field collector sends an AttemptInterview message to the dwelling, and the dwelling responds with DwellingRefusal, DwellingNoncontact, or DwellingResponse. If DwellingRefusal, the case is marked as complete, and no further attempts will be made. If DwellingNoncontact, the collector moves on to the next case. If DwellingResponse, the dwelling will also send a list of any individuals it has spawned, and the collector will then attempt those in turn. Along with the list of individuals, a dwelling will also send a payload representing a completed household questionnaire (just a random string for now), which the collector will then forward to the event recorder to persist. If a dwelling response has been previously obtained, then the collector will already have a list of available individuals, and so they will be attempted directly, and no message will be sent to the dwelling.\n\nLike dwellings, individuals will each be sent an AttemptInterview message, and they will respond with any of IndividualRefusal, IndividualNoncontact, or IndividualResponse. If IndividualRefusal, the individual will be marked as complete, and no further attempts will be made. If IndividualNoncontact, the collector will move on to the next individual, but the non-contact will remain active, and so future interview attempts will be made. If IndividualResponse, a payload representing a completed personal questionnaire will be sent as part of the message, which the collector will then forward to the event recorder to persist. The individual will be marked as complete, and no future interviews will be attempted.\n \nWhen finished with a particular dwelling, field collectors send themselves a NextItem message. When received, collectors will remove the current case from the pool of outstanding work, and send an AttemptInterview message to the next available case (to the dwelling if no response has yet been received, or to the next unfinished individual). When moving to the next case, collectors increment a dayKms variable (if travelling), and a dayMins variable as appropriate.\nAt any point, ff there is no more work, or if dayMins exceeds some pre-configured amount of time in the day, the collector sends themselves the GoHome message. When this message is received, the route home is calculated and dayKms and dayMins are incremented, the pool of work is emptied, and no further messages are sent."
  },
  {
    "objectID": "posts/web-maps-and-tiles-with-qgis/index.html",
    "href": "posts/web-maps-and-tiles-with-qgis/index.html",
    "title": "Web Maps and Tiles with QGIS",
    "section": "",
    "text": "In an earlier post, we looked at how to style a map using TileMill (actually, using a fork called TileOven), render tiles with Mapnik, and finally serve tiles using TileStache.\nServing tiles in this way is useful, avoiding the need to bundle potentially large features with an online map or web app. At the same time, the outlined process is a little involved.\nIn this post we look at the open source Quantum GIS. QGIS provides functionality to publish maps as WMS / WFS / WCS services, and so we look at how this is done.\nQGIS is made extensible via plugins, and this is a strong feature of the product. We also look at a plugin called QMetaTiles which can make a set of static tiles which can be combined with a web server to be used as a standard tile service."
  },
  {
    "objectID": "posts/web-maps-and-tiles-with-qgis/index.html#qgis-qgis-server",
    "href": "posts/web-maps-and-tiles-with-qgis/index.html#qgis-qgis-server",
    "title": "Web Maps and Tiles with QGIS",
    "section": "QGIS + QGIS Server",
    "text": "QGIS + QGIS Server\nTo install, simply run:\nsudo apt-get install qgis qgis-server\nThe version of QGIS available will likely be an older, LTS version. If a newer version is desired, simply follow the instructions provided on the QGIS website. In the case of Ubuntu / Debian Linux:\nQGIS Installers"
  },
  {
    "objectID": "posts/web-maps-and-tiles-with-qgis/index.html#fastcgi-nginx",
    "href": "posts/web-maps-and-tiles-with-qgis/index.html#fastcgi-nginx",
    "title": "Web Maps and Tiles with QGIS",
    "section": "fastcgi + NGINX",
    "text": "fastcgi + NGINX\nTo deploy QGIS maps as a service, a CGI-enabled web server is required. For illustration here we use NGINX and FastCGI. But Apache is relatively easy to use on both Linux and Windows users. Windows users might find it easy to use IIS and FastCGI for IIS.\nRegardless, to install both NGINX and fastcgi:\nsudo apt-get install nginx fcgiwrap\nTo enable CGI, modify the NGINX default config (probably something like /etc/nginx/sites-available/default) so it looks as follows:\n...\n\nserver {\n\n  ...\n\n  location /cgi-bin/ {\n    gzip off;\n    include fastcgi_params;\n    fastcgi_param SCRIPT_FILENAME /usr/lib$fastcgi_script_name;\n    fastcgi_pass unix:/var/run/fcgiwrap.socket;\n  }\n   \n  ...\n\n}\n\n...\nWhen done, test the config and restart NGINX if all is well:\nsudo /etc/init.d/nginx configtest && sudo /etc/init.d/nginx reload\nTo conduct a quick test, we can make a little CGI script. On my machine, I created a file called /usr/lib/cgi-bin/hello.cgi with the following content:\n#!/usr/bin/python\nprint \"Content-type: text/html\\n\\n\"\nprint \"<html><head><title>Hello, World!</title></head>\\n\"\nprint \"<body><h1>Hello, World!</h1></body></html>\\n\"\nThe file needs to be made readable and executable, so:\nsudo chmod go+rx /usr/lib/cgi-bin/hello.cgi\nThen, browsing to http://localhost/test/hello.cgi will yield something like:"
  },
  {
    "objectID": "posts/web-maps-and-tiles-with-qgis/index.html#polygons",
    "href": "posts/web-maps-and-tiles-with-qgis/index.html#polygons",
    "title": "Web Maps and Tiles with QGIS",
    "section": "Polygons",
    "text": "Polygons\nWe start QGIS, and we load 4 polygon feature classes:\n\nMB2013\nAU2013\nTA2013\nREGC2013\n\nThe project is saved as ~/Maps/2013/2013.qgs, and each of the included feature classes is saved in the same directory as the project. In addition we also place a copy of /usr/lib/cgi-bin/qgis-mapserve.fcgi in the directory.\n\nOnly basic styling is applied–simple black outlines for each feature class, and labels only visible at certain scales.\n\n\nNext, to enable the map as a web service, open the the project properties (CTRL + SHIFT + P), and select ‘OWS Server’. There, check the ‘Service capabilites’ box, and also the ‘Use layer ids as names’ box. To enable WFS, also check each box in the ‘Published’ column in the table under the section labelled ‘WFS capabilities (also influences DXF export)’. Click the ‘Launch’ button in the ‘Test configuration’ section to confirm everything is okay, the click ‘OK’, and save the project.\n\nAt this point, the service isn’t yet published. To do this, first ensure that the ‘Save paths’ option in the project properties is set to ‘relative’:\n\nThen, copy the project to /usr/lib/cgi-bin/, which is where the webserver is configured to run CGI scripts from (of course, modify this as you see fit if you have decided, for safety reasons or otherwise, to serve CGI content from a different location):\nsudo cp -R ~/Maps/2013 /usr/lib/cgi-bin/\nsudo chmod -R go+rx /usr/lib/cgi-bin/2013\nIf all is well, you should get a response at the following URI:\nhttp://localhost/cgi-bin/2013/qgis_mapserv.fcgi?service=wms&request=getcapabilities\nThe result should be an XML file describing the service. It is, of course, possible to get images directly from the service given the right set of parameters. For example:\nhttps://localhost/cgi-bin/2013/qgis_mapserv.fcgi?&SERVICE=WMS&REQUEST=GetMap&VERSION=1.1.1&LAYERS=REGC2013&STYLES=&FORMAT=image%2Fpng&TRANSPARENT=true&HEIGHT=256&WIDTH=256&SRS=EPSG%3A2193&BBOX=1089971.21440,4747987.36420,2470042.12700,6194308.49260\nyields the following thumbnail:\n\nAs a further example, we use R to create a leaflet map containing each layer:\nlibrary(leaflet)\n\nuri <- \"https://localhost/cgi-bin/2013/qgis_mapserv.fcgi?\"\n\nleaflet() %>%\n  addTiles() %>%\n  fitBounds(167, -48, 179, -34) %>%\n  addWMSTiles(\n    baseUrl=uri, \n    layers=\"REGC2013\",\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE),\n    group = \"Regional Council\"\n  ) %>%\n  addWMSTiles(\n    baseUrl=uri,\n    layers=\"TA2013\",\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE),\n    group = \"Territorial Authority\"\n  ) %>%\n  addWMSTiles(\n    baseUrl=uri,\n    layers=\"AU2013\",\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE),\n    group = \"Area Unit\"\n  ) %>%\n  addWMSTiles(\n    baseUrl=uri,\n    layers=\"MB2013\",\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE),\n    group = \"Meshblock\"\n  ) %>%\n  addLayersControl(\n    baseGroups = c(\n      \"Regional Council\", \"Territorial Authority\",\n      \"Area Unit\", \"Meshblock\"\n    ),\n    options = layersControlOptions(collapsed = FALSE)\n  )"
  },
  {
    "objectID": "posts/web-maps-and-tiles-with-qgis/index.html#rasters",
    "href": "posts/web-maps-and-tiles-with-qgis/index.html#rasters",
    "title": "Web Maps and Tiles with QGIS",
    "section": "Rasters",
    "text": "Rasters\nWe start QGIS and load the raster Nitrogen_Leaching_20150903.tif. The project is saved as ~/Maps/nitrogen.qgs, and both the raster and a copy of /usr/lib/cgi-bin/qgis-mapserve.fcgi is placed in the same directory.\n\nSome basic styling is applied. Specifically, the colouring is changed from greyscale to psuedo-colour (yellow to red).\n\nMaking the map available as a web service is similar to before, except we check features under the ‘WCS’ heading rather than ‘WFS’.\n\nAs before, we copy the project to the web server root directory, and alter permissions to make it readable (and executable):\nsudo cp -R ~/Maps/nitrogen /usr/lib/cgi-bin/\nsudo chmod -R go+rx /usr/lib/cgi-bin/nitrogen\nAgain, if the service is functioning correctly, there should be at the following URL:\nhttp://localhost/cgi-bin/nitrogen/qgis_mapserv.fcgi?service=wms&request=getcapabilities\nAnd, again, it should bepossible to get images directly from the service given the right set of parameters. For example:\nhttps://localhost/cgi-bin/nitrogen/qgis_mapserv.fcgi?&SERVICE=WMS&REQUEST=GetMap&VERSION=1.1.1&LAYERS=nitrogen&STYLES=&FORMAT=image%2Fpng&HEIGHT=256&WIDTH=256&SRS=EPSG%3A2193&BBOX=1089971.21440,4747987.36420,2470042.12700,6194308.49260\nyields the following thumbnail:\n\nAs a further example, we use R to create a leaflet map containing each layer:\nlibrary(leaflet)\n\nuri <- \"https://localhost/cgi-bin/nitrogen/qgis_mapserv.fcgi?\"\n\nleaflet() %>%\n  addTiles() %>%\n  fitBounds(167, -48, 179, -34) %>%\n  addWMSTiles(\n    baseUrl=uri, \n    layers=\"nitrogen\",\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE),\n    group = \"nitrogen\"\n  )%>%\n  addLayersControl(\n    overlayGroups = c(\"nitrogen\"),\n    options = layersControlOptions(collapsed = FALSE)\n  )"
  },
  {
    "objectID": "posts/online-learning-with-akka/index.html",
    "href": "posts/online-learning-with-akka/index.html",
    "title": "Online Learning with Akka",
    "section": "",
    "text": "In a previous post, we looked at how we could consume messages in a Kafka topic, and use those messages for online training of a classifier. In this post, we look at how Akka could be used to solve the same problem, in broadly the same way. Akka is an implementation of the actor model, and is particularly well suited for building event-driven, reactive systems since the actors involved communicate exclusively by sending and reacting to messages."
  },
  {
    "objectID": "posts/online-learning-with-akka/index.html#serialisation-deserialisation",
    "href": "posts/online-learning-with-akka/index.html#serialisation-deserialisation",
    "title": "Online Learning with Akka",
    "section": "Serialisation / Deserialisation",
    "text": "Serialisation / Deserialisation\nAkka is designed to be scalable, and our actor system can be spread across multiple JVM instances, on disparate servers. For actors to communicate with each other outside a single JVM instance, we would need to serialise our messages. Custom serialisers are easy enough to make–we simply create a class which mixes in akka.serialization.Serializer, and then we add entries in the akka.actor.serializers and akka.actor.serialization-bindings section of our configuration (see Serialization) to make use of them. However, in this example we limit ourselves to a single JVM instance, and so we do not need to worry."
  },
  {
    "objectID": "posts/online-learning-with-akka/index.html#model-our-messages",
    "href": "posts/online-learning-with-akka/index.html#model-our-messages",
    "title": "Online Learning with Akka",
    "section": "Model Our Messages",
    "text": "Model Our Messages\nIn order to send labelled images, we’ll re-use the MnistRecord type from the Kafka implementation, so, please excuse the duplication…\nThe MNIST database consists of handwritten images of 28x28 pixels, each assigned a label from 0 to 9. The image data is easily read from the provided files as an array of bytes in row-major order, so we define a simple case class as follows (the class has a number of methods, but we omit the details here):\ncase class Image(data: Array[Byte], rows: Int, cols: Int, byrow: Boolean)\nLabelled images can then be modelled simply, also using a case class, as follows:\ncase class MnistRecord(image: Image, label: Int)\nThe MNIST data has been bundled as a resource in the provided sbt project, and a basic, single-use iterator is provided also. For example, to read the training data:\n\nscala> val testIt = MnistIterator(MnistType.TRAIN)\nval testIt: org.cmhh.MnistIterator = <iterator>\n\nThe rest of our messages will be modelled as singleton objects, or very simple case classes:\nimport akka.actor.typed.ActorRef\nimport org.deeplearning4j.nn.multilayer.MultiLayerNetwork\n\nobject messages {\n  sealed trait CoordinatorCommand \n  case object RequestAccuracy extends CoordinatorCommand\n  case class RequestSendImages(n: Int) extends CoordinatorCommand\n  case class RequestClassifier(f: String) extends CoordinatorCommand\n  case object Stop extends CoordinatorCommand with ProducerCommand with ConsumerCommand\n  case class Accuracy(value: Double, n: Int) extends CoordinatorCommand\n  case class Classifier(value: MultiLayerNetwork, f: String)  extends CoordinatorCommand\n  \n  sealed trait ProducerCommand\n  case class SendImages(sendTo: ActorRef[ConsumerCommand], n: Int) extends ProducerCommand\n\n  sealed trait ConsumerCommand\n  case class Image(image: MnistRecord) extends ConsumerCommand\n  case class SendAccuracy(replyTo: ActorRef[Accuracy]) extends ConsumerCommand\n  case class SendClassifier(replyTo: ActorRef[Classifier], f: String) extends ConsumerCommand \n}\nAll our producer does is send \\(n\\) images to the consumer, and it does this when sent a SendImages object. In this case, the producer sends messages but does not expect any sort of reply. The producer will also terminate when sent the Stop object, but also once it has traversed the full MNIST training set once (again, we’re simulating online learning here–it would be straightforward to implement other training scenarios, traversing the training set as many times as required, for example, and in a random order). The full implementation is as follows:\nimport akka.actor.typed.scaladsl.Behaviors\nimport akka.actor.typed.scaladsl.LoggerOps\nimport akka.actor.typed.Behavior\n\nobject Producer {\n  import messages._\n\n  private val images = MnistIterator(MnistType.TRAIN)\n\n  def apply(): Behavior[ProducerCommand] = \n    Behaviors.receive[ProducerCommand]{ (context, message) => \n      message match {\n        case SendImages(r, n) => \n          def loop(i: Int): Behavior[ProducerCommand] = \n            if (!images.hasNext) {\n              println(\"All images sent.  Shutting down.\")\n              Behaviors.stopped\n            } else if (i == n) {\n              Behaviors.same\n            } else {\n              r ! Image(images.next())\n              loop(i + 1)\n            }\n          loop(0)\n        case Stop => \n          println(\"Producer is dead!\")\n          Behaviors.stopped\n      }\n    }\n}\nOur consumer is a little more complicated, but is still relatively simple. It can receive messages as follows:\n\nImage - when received, it will update its private classifier\nSendAccuracy - when received, it will send an Accuracy object to the sender\nSendClassifier - when received, it will send a Classifier object to the sender\n\nThe full implementation of the consumer actors is as follows:\nimport akka.actor.typed.scaladsl.Behaviors\nimport akka.actor.typed.scaladsl.LoggerOps\nimport akka.actor.typed.Behavior\nimport org.nd4j.evaluation.classification.Evaluation\n\nobject Consumer {\n  import messages._\n\n  private val network = model.cnn()\n  private val mnistTest = new MnistDataSetIterator(100, MnistType.TEST)\n\n  def apply(): Behavior[ConsumerCommand] = run(0)\n\n  def run(n: Int): Behavior[ConsumerCommand] = \n    Behaviors.receive{ (context, message) => message match {\n      case Image(rec) =>\n        val im = rec.toNd4j\n        network.fit(im._1, im._2)\n        run(n + 1)\n      case SendAccuracy(replyTo) =>\n        replyTo ! Accuracy(accuracy, n)\n        Behaviors.same\n      case SendClassifier(replyTo, f) =>\n        replyTo ! Classifier(network.clone, f)\n        Behaviors.same\n      case Stop => \n        Behaviors.stopped {() => \n          println(\"Consumer is dead!\")\n        }\n    }\n  }\n\n  def accuracy: Double = {\n    val eval = network.evaluate[Evaluation](mnistTest)\n    eval.accuracy\n  }\n}\nNote that actors can be created with an object-oriented or functional style, and we’ve opted for the functional approach here. So in the case of the consumer, we’ve tracked the number of total training images received, mostly to demonstrate how one might go about maintaining state–we could use mutable variables, but instead we define our Behavior recursively.\nRather than send messages directly to either of our producer or consumer actors, we communicate exclusively with a coordinator actor. We send the coordinator any of the following:\n\nRequestSendImage - coordinator will tell the producer to send Images to the consumer\nRequestAccuracy - coordinator will tell the consumer to send an Accuracy object\nRequestClassifier - coordinator will tell the consumer to send a Classifier object\nStop - tell both the consumer and producer to Stop\n\nimport akka.actor.typed.scaladsl.Behaviors\nimport akka.actor.typed.scaladsl.LoggerOps\nimport akka.actor.typed.Behavior\nimport java.io.File\n\nobject Coordinator {\n  import messages._\n\n  def apply(): Behavior[CoordinatorCommand] = Behaviors.setup { context => \n    val consumer = context.spawn(Consumer(), \"consumer\")\n    val producer = context.spawn(Producer(), \"producer\")\n\n    Behaviors.receiveMessage { message => {\n      message match {\n        case RequestSendImages(n) =>\n          producer ! SendImages(consumer, n)\n        case RequestAccuracy => \n          consumer ! SendAccuracy(context.self)\n        case RequestClassifier(f) =>\n          consumer ! SendClassifier(context.self, f)\n        case Accuracy(accuracy, n) => \n          println(f\"accuracy: %%1.4f, training images seen: %%05d\".format(accuracy, n))\n        case Classifier(n, f) =>\n          n.save(new File(f))\n        case Stop =>\n          consumer ! Stop\n          producer ! Stop\n      }\n      Behaviors.same\n    }}\n  }\n}"
  },
  {
    "objectID": "posts/online-learning-with-akka/index.html#summary",
    "href": "posts/online-learning-with-akka/index.html#summary",
    "title": "Online Learning with Akka",
    "section": "Summary",
    "text": "Summary\nIn this post we demonstrated the basic usage of Kafka’s typed actors. The specific use-case for our actor system was to react to new messages in real-time, using them to update a classifier. There are a number of extensions that we could consider to make this more interesting, of course–we could develop web interfaces for interacting with out actors, and we could look other parts of the Akka ecosystem such as Akka streams (though the goal here was specifically to use the actor model)."
  },
  {
    "objectID": "posts/too-much-shiny/index.html#shiny-with-bundled-data",
    "href": "posts/too-much-shiny/index.html#shiny-with-bundled-data",
    "title": "Too Much Shiny?!",
    "section": "Shiny with Bundled Data",
    "text": "Shiny with Bundled Data\nThe complete source for our first attempt is relatively simple, and looks as follows:\n\nui.Rserver.Rglobal.R\n\n\nshinyUI(fluidPage(\n  titlePanel(\"Labour Market Time Series Data\"),\n\n  sidebarLayout(\n    sidebarPanel(width = 4,\n      selectizeInput(\n        \"subject\", \"select subject:\", subject_choices\n      ),\n      selectizeInput(\n        \"group\", \"select group:\", c()\n      ),\n      selectizeInput(\n        \"series\", \"select series:\", c(), multiple = TRUE\n      )\n    ),\n\n    mainPanel(width = 8,\n      plotlyOutput(\"plot\")\n    )\n  )\n))\n\n\nshinyServer(function(input, output, session) {\n  groups <- reactive({\n    req(input$subject)\n    unique(data[\n      subject_code == input$subject,\n      .(group_code, group_description)\n    ])\n  })\n\n  series <- reactive({\n    req(input$subject, input$group)\n    unique(data[\n      subject_code == input$subject & group_code == input$group,\n      c(\"series_reference\", sprintf(\"series_title_%d\", 1:5)),\n      with = FALSE\n    ])\n  })\n\n  values <- reactive({\n    req(input$subject, input$group, input$series)\n\n    data[\n      subject_code == input$subject & group_code == input$group &\n        series_reference %in% input$series\n    ]\n  })\n\n  observeEvent(groups(), {\n    group_choices <- setNames(\n      groups()$group_code,\n      groups()$group_description\n    )\n\n    updateSelectizeInput(session, \"group\", choices = group_choices)\n  })\n\n  observeEvent(series(), {\n    labels <- apply(series(), 1, function(z) {\n      z[-1] |> (\\(x) {x[x != \"\"]})() |> paste(collapse = \", \")\n    })\n\n    subject_choices <- setNames(\n      series()$series_reference,\n      sprintf(\"%s - %s\", series()$series_reference, labels)\n    )\n\n    updateSelectizeInput(session, \"series\", choices = subject_choices)\n  })\n\n  output$plot <- renderPlotly({\n    req(values())\n    if (is.null(values())) return(NULL)\n    if (nrow(values()) == 0) return(NULL)\n\n    p <- ggplot(\n      data = values(),\n      aes(x = period, y = data_value, col = series_reference)\n    ) +\n      geom_line()\n\n    ggplotly(p)\n  })\n})\n\n\nlibrary(shiny)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(plotly)\n\ndata <- readRDS(\"../../data/labour_market.rds\")\n\nsubjects <- unique(data[, .(subject_code, subject_description)])\n\nsubject_choices <- setNames(\n  subjects$subject_code,\n  sprintf(\"%s - %s\", subjects$subject_code, subjects$subject_description)\n)\n\n\n\nand the running application looks as follows:"
  },
  {
    "objectID": "posts/too-much-shiny/index.html#creating-a-supporting-data-service",
    "href": "posts/too-much-shiny/index.html#creating-a-supporting-data-service",
    "title": "Too Much Shiny?!",
    "section": "Creating a Supporting Data Service",
    "text": "Creating a Supporting Data Service\nAs described, the first cut of our Shiny application has bundled with it a large data frame, but we are only ever interested in a small subset at any point in time. The subset we are interested in is determined by client selections, and the filtered data is made available via a reactive expression. For example, in our application, users can select a subject, then a group, and then up to \\(n\\) series references, and we then use the selected values to filter the bundled data:\nvalues <- reactive({\n  req(input$subject, input$group, input$series)\n\n  data[\n    subject_code == input$subject & group_code == input$group &\n      series_reference %in% input$series\n  ]\n})\nThat is, calling values() in our code will always return the correct subset, and any reactive block of code that depends on it will be re-evaluated whenever the underlying inputs change. Here we plot the result using standard plotting functions, thus separating the logic for deciding what we plot and how we plot. This is a reasonable practice in Shiny–that we access data via reactive expressions where possible–keeping such concerns separate from other logic, and helping keep our applications modular. But if we follow such a practice when authoring Shiny applications, we then have a reasonably obvious path for replacing the bundled data with a service–we simply create a service with end-points that roughly match our reactive expressions. Thus, the only changes we would need to make to our existing application would be to each of the reactive expressions which access the original bundled data. In this case, a sufficient service is fully described as follows:\n\nlabour_market_service.Rrun_labour_market_service.R\n\n\n#* @apiTitle Labour market data service\n#* @apiDescription Data service for HLFS, QES, and LCI time series data.\n\nlibrary(data.table)\ndata <- readRDS(\"../data/labour_market.rds\")\nsubjects <- unique(data[, c(\"subject_code\", \"subject_description\")])\n\n#* @filter cors\n#* turn this off in production!!\ncors <- function(res) {\n    res$setHeader(\"Access-Control-Allow-Origin\", \"*\")\n    plumber::forward()\n}\n\n#* Return complete set of subjects\n#* @get /subjects\n#* @serializer json\nfunction() {\n  subjects\n}\n\n#* Return complete set of groups for selected subject\n#* @get /groups/<subjectCode>\n#* @serializer json\nfunction(subjectCode) {\n  unique(data[\n    subject_code == subjectCode,\n    c(\"subject_code\", \"group_code\", \"group_description\")\n  ])\n}\n\n#* Return complete set of series references for selected subject and group\n#* @get /series/<subjectCode>/<groupCode>\n#* @serializer json\nfunction(subjectCode, groupCode) {\n  unique(data[\n    subject_code == subjectCode & group_code == groupCode,\n    c(\"subject_code\", \"group_code\", \"group_description\",\n      \"series_reference\", \"units\", \"magnitude\",\n      sprintf(\"series_title_%d\", 1:5)), with = FALSE\n  ])\n}\n\n#* Return data for specific references\n#* @param seriesReference:[string] Series reference, e.g. \"HLFQ.SAA1AZ\"\n#* @get /values\n#* @serializer json\nfunction(seriesReference) {\n  data[series_reference %in% seriesReference]\n}\n\n#* Return data for specific references\n#* @param seriesReference:[string] Series reference, e.g. \"HLFQ.SAA1AZ\"\n#* @get /values/<subjectCode>/<groupCode>\n#* @serializer json\nfunction(subjectCode, groupCode, seriesReference) {\n  data[\n    subject_code == subjectCode & group_code == groupCode &\n      series_reference %in% seriesReference\n  ]\n}\n\n\nlibrary(plumber)\npr(\"labour_market_service.R\") |> pr_run(host = \"0.0.0.0\", port = 3001)\n\n\n\nAs an aside, the performance of this service is actually surprisingly good. This is essentially because we’ve used the data.table package which is extremely fast. As an example, here is the result of fetching a single series using siege\nsiege -b -c 3 -t 10s \"http://localhost:3001/series?seriesReference=HLFQ.SAA1AZ\"\n** SIEGE 4.0.7\n** Preparing 3 concurrent users for battle.\nThe server is now under siege...\nLifting the server siege...\nTransactions:                   6216 hits\nAvailability:                 100.00 %\nElapsed time:                   9.12 secs\nData transferred:               0.33 MB\nResponse time:                  0.00 secs\nTransaction rate:             681.58 trans/sec\nThroughput:                     0.04 MB/sec\nConcurrency:                    2.98\nSuccessful transactions:           0\nFailed transactions:               0\nLongest transaction:            0.06\nShortest transaction:           0.00\n(Note that performance slowed if the number of concurrent workers exceeded 3. This is possibly because my machine has just 6 cores / 12 threads, and data.table was using 6 threads by default already. In saying that, CPU utilisation appeared low while the test was running.)"
  },
  {
    "objectID": "posts/too-much-shiny/index.html#shiny-with-a-data-service-and-no-bundled-data",
    "href": "posts/too-much-shiny/index.html#shiny-with-a-data-service-and-no-bundled-data",
    "title": "Too Much Shiny?!",
    "section": "Shiny with a Data Service (and No Bundled Data)",
    "text": "Shiny with a Data Service (and No Bundled Data)\nGiven an available service, we can easily modify our application, removing all dependencies on the bundled data. Before listing the complete source, however, we lift out a single reactive expression for illustration–series, in this case:\n\noldnew\n\n\nseries <- reactive({\n  req(input$subject, input$group)\n  unique(data[\n    subject_code == input$subject & group_code == input$group,\n    c(\"series_reference\", sprintf(\"series_title_%d\", 1:5)),\n    with = FALSE\n  ])\n})\n\n\nseries <- reactive({\n  req(input$subject, input$group)\n  url <- sprintf(\"%s/series/%s/%s\", service, input$subject, input$group)\n  response <- setDT(jsonlite::fromJSON(url))\n\n  if (length(response) == 0) return(NULL)\n\n  response[\n    ,c(\"series_reference\", sprintf(\"series_title_%d\", 1:5)),\n    with = FALSE\n  ]\n})\n\n\n\nAgain, our application is mostly unchanged besides these reactive expressions (we remove the data dependency in global.R, and replace it with the path to our service). The full source is as follows:\n\nui.Rserver.Rglobal.Rconfig.yml\n\n\nshinyUI(fluidPage(\n  titlePanel(\"Labour Market Time Series Data\"),\n\n  sidebarLayout(\n    sidebarPanel(width = 4,\n      selectizeInput(\n        \"subject\", \"select subject:\", subject_choices\n      ),\n      selectizeInput(\n        \"group\", \"select group:\", c()\n      ),\n      selectizeInput(\n        \"series\", \"select series:\", c(), multiple = TRUE\n      )\n    ),\n\n    mainPanel(width = 8,\n      plotlyOutput(\"plot\")\n    )\n  )\n))\n\n\nshinyServer(function(input, output, session) {\n  groups <- reactive({\n    req(input$subject)\n    url <- sprintf(\"%s/groups/%s\", service, input$subject)\n    setDT(jsonlite::fromJSON(url))\n  })\n\n  series <- reactive({\n    req(input$subject, input$group)\n    url <- sprintf(\"%s/series/%s/%s\", service, input$subject, input$group)\n    response <- setDT(jsonlite::fromJSON(url))\n\n    if (length(response) == 0) return(NULL)\n\n    response[\n      ,c(\"series_reference\", sprintf(\"series_title_%d\", 1:5)),\n      with = FALSE\n    ]\n  })\n\n  values <- reactive({\n    req(input$subject, input$group, input$series)\n\n    url <- sprintf(\n      \"%s/values/%s/%s?%s\",\n      service,\n      input$subject,\n      input$group,\n      paste(sprintf(\"seriesReference=%s\", input$series), collapse = \"&\")\n    )\n\n    response <- setDT(jsonlite::fromJSON(url))\n\n    if (length(response) == 0)\n      return(NULL)\n    else\n      response[, period := as.Date(period, format = \"%Y-%m-%d\")]\n  })\n\n  observeEvent(groups(), {\n    group_choices <- setNames(\n      groups()$group_code,\n      groups()$group_description\n    )\n\n    updateSelectizeInput(session, \"group\", choices = group_choices)\n  })\n\n  observeEvent(series(), {\n    labels <- apply(series(), 1, function(z) {\n      z[-1] |> (\\(x) {x[x != \"\"]})() |> paste(collapse = \", \")\n    })\n\n    subject_choices <- setNames(\n      series()$series_reference,\n      sprintf(\"%s - %s\", series()$series_reference, labels)\n    )\n\n    updateSelectizeInput(session, \"series\", choices = subject_choices)\n  })\n\n  output$plot <- renderPlotly({\n    req(values())\n    if (is.null(values())) return(NULL)\n    if (nrow(values()) == 0) return(NULL)\n\n    p <- ggplot(\n      data = values(),\n      aes(x = period, y = data_value, col = series_reference)\n    ) +\n      geom_line()\n\n    ggplotly(p)\n  })\n})\n\n\nlibrary(shiny)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(config)\n\nservice <- config::get(\"service\")\n\nsubjects <- jsonlite::fromJSON(sprintf(\"%s/subjects\", service))\n\nsubject_choices <- setNames(\n  subjects$subject_code,\n  sprintf(\"%s - %s\", subjects$subject_code, subjects$subject_description)\n)\n\n\ndefault:\n  service: \"http://localhost:3001\"\n\n\n\nAnd note our application starts up a little faster now since we do not have to wait for the bundled data to be read into memory. And, as noted in the overview, we have disentangled our application and the data completely, and so we can update our data service without needing to modify our application or redeploy it. Cool."
  },
  {
    "objectID": "posts/too-much-shiny/index.html#vue.js-application-with-a-data-service",
    "href": "posts/too-much-shiny/index.html#vue.js-application-with-a-data-service",
    "title": "Too Much Shiny?!",
    "section": "Vue.js Application with a Data Service",
    "text": "Vue.js Application with a Data Service\nFollowing the trend, the source code for a client-side Vue application is longer and more verbose than the Shiny code, but not unmanageably so. The complete code is:\n\nindex.htmlmain.jsstyles.css\n\n\n<html>\n<head>\n  <title>Labour Market Time Series Data</title>\n  <link rel=\"stylesheet\" href=\"https://unpkg.com/vue-select@3.0.0/dist/vue-select.css\">\n  <link rel=\"stylesheet\" href=\"./styles.css\" />\n</head>\n\n<body>\n<div id=labmktapp>\n  <div class=\"title\"><h1>Labour Market Time Series Data</h1></div>\n  <div class=\"body\">\n    <div class=\"controls\">\n      <p class=\"label\">select subject:</p>\n      <v-select v-model=\"selectedSubject\" :options=\"subjects\" id=\"subject-select\"></v-select>\n\n      <p class=\"label\">select group:</p>\n      <v-select v-model=\"selectedGroup\" :options=\"groups\" id=\"group-select\"></v-select>\n\n      <p class=\"label\">select series:</p>\n      <v-select v-model=\"selectedSeries\" :options=\"series\" id=\"series-select\" multiple></v-select>\n      </select>\n    </div>\n    <div class=\"plotcontainer\">\n      <div id=\"labmktplot\"></div>\n    </div>\n  </div>\n</div>\n\n<script src=\"https://cdn.jsdelivr.net/npm/vue@2/dist/vue.js\"></script>\n<script src=\"https://unpkg.com/vue-select@3.0.0\"></script>\n<script src=\"https://unpkg.com/axios/dist/axios.min.js\"></script>\n<script src='https://cdn.plot.ly/plotly-2.8.3.min.js'></script>\n<script src='./main.js'></script>\n\n</body>\n</html>\n\n\nconst server = \"http://localhost:3001\";\n\nVue.component('v-select', VueSelect.VueSelect);\nvar app = new Vue({\n  el: \"#labmktapp\",\n  data: {\n    subjects: [],\n    selectedSubject: {},\n    groups: [],\n    selectedGroup: {},\n    series: [],\n    selectedSeries: [],\n    values: [],\n    layout: {\n      xaxis: {title: {text: \"date\"}},\n      legend: {orientation: \"h\", yanchor: \"bottom\", y: -0.25},\n      margin: {t: 0}\n    },\n    config: { responsive: true }\n  },\n  mounted: function() {\n    const self = this\n    axios\n      .get(`${server}/subjects`)\n      .then(function (response) {\n        self.subjects = response.data.map(x => {\n          let obj = {}\n          obj[\"code\"] = x.subject_code\n          obj[\"label\"] = `${x.subject_code} - ${x.subject_description}`\n          return obj\n        })\n        self.selectedSubject = self.subjects[0]\n        self.selectedGroup = {}\n        self.selectedSeries = []\n      })\n      .catch(function (error) {\n        console.log(error)\n      })\n  },\n  computed: {\n    trace: function() {\n      const refs = this.values\n        .map(x => {return x.series_reference})\n        .filter((v,i,a) => a.indexOf(v) == i)\n\n      res = refs.map(ref => {\n        const sub = this.values.filter(x => x.series_reference == ref)\n        const xs = sub.map(x => {return x.period})\n        const ys = sub.map(x => {return x.data_value})\n        obj = {}\n        obj[\"type\"] = \"scatter\"\n        obj[\"mode\"] = \"lines\"\n        obj[\"name\"] = ref\n        obj[\"x\"] = xs\n        obj[\"y\"] = ys\n        return obj\n      })\n\n      return res\n    }\n  },\n  watch: {\n    selectedSubject: function() {\n      const self = this\n      const url = `${server}/groups/${self.selectedSubject[\"code\"]}`\n      axios\n        .get(url)\n        .then(function(response) {\n          data = response.data.map(x => {\n            obj = {}\n            obj[\"code\"] = x.group_code\n            obj[\"label\"] = x.group_description\n            return obj\n          })\n          self.groups = data\n          self.selectedGroup = data[0]\n        })\n        .catch(function (error) {\n          console.log(error)\n        })\n    },\n    selectedGroup: function() {\n      const self = this\n      axios\n        .get(`${server}/series/${self.selectedSubject[\"code\"]}/${self.selectedGroup[\"code\"]}`)\n        .then(function (response) {\n          self.series = response.data.map(x => {\n            obj = {}\n            obj[\"code\"] = x.series_reference\n            obj[\"label\"] = x.series_reference + \" - \" + self.titles(x)\n            return obj\n          })\n          self.selectedSeries = []\n        })\n        .catch(function (error) {\n          console.log(error)\n        })\n    },\n    selectedSeries: function() {\n      const self = this\n      if (self.selectedSeries.length == 0) {\n        self.values = []\n      } else {\n        const refQuery = self.selectedSeries.map(x => {\n          return `seriesReference=${x.code}`\n        }).join('&')\n\n        const url = `${server}/values/${self.selectedSubject[\"code\"]}/` +\n          `${self.selectedGroup[\"code\"]}?${refQuery}`\n\n        axios\n          .get(url)\n          .then(function(response) {\n            self.values = response.data\n          })\n          .catch(function (error) {\n            console.log(error)\n          })\n      }\n    },\n    trace: function() {\n      if (this.trace.length == 0) {\n        this.hidePlot()\n      } else {\n        this.unhidePlot()\n      }\n      this.updatePlot()\n    }\n  },\n  methods: {\n    titles: function(s) {\n      var res = s.series_title_1\n      if (s.series_title_2 != \"\") res = res + \", \" + s.series_title_2\n      if (s.series_title_3 != \"\") res = res + \", \" + s.series_title_3\n      if (s.series_title_4 != \"\") res = res + \", \" + s.series_title_4\n      if (s.series_title_5 != \"\") res = res + \", \" + s.series_title_5\n      return res\n    },\n    hidePlot: function() {\n      document.getElementById(\"labmktplot\").style.display = \"none\"\n    },\n    unhidePlot: function() {\n      document.getElementById(\"labmktplot\").style.display = \"block\"\n    },\n    updatePlot: function() {\n      Plotly.newPlot('labmktplot', this.trace, this.layout, this.config)\n    }\n  }\n})\n\n\n@import url('https://fonts.googleapis.com/css2?family=Open+Sans&display=swap');\n\n#labmktapp * {\n  font-family: \"Open Sans\", verdana, arial, sans-serif;\n}\n\n#labmktapp {\n  width: 100%;\n}\n\n#labmktapp p {\n  margin: 0;\n}\n\n#labmktapp .title h1 {\n  font-size: 25px;\n  font-weight: 500;\n}\n\n#labmktapp .controls {\n  display: inline-block;\n  width: calc(40% - 30px);\n  vertical-align: top;\n  border: 1px solid #999;\n  border-radius: 5px;\n  background-color: #eee;\n  padding: 10px;\n}\n\n#labmktapp .plotcontainer {\n  display: inline-block;\n  width: 60%;\n}\n\n#labmktapp .v-select .vs__dropdown-toggle {\n  border-color: #999!important;\n  margin-bottom: 20px;\n}\n\n#labmktapp .v-select .vs__dropdown-toggle, #labmktapp .vs__dropdown-menu {\n  background: #FFF;\n  font-size: 80%;\n}\n\n#labmktapp p.label {\n  font-size: 80%;\n  font-weight: 700;\n  margin-bottom: 10px;\n}\n\n\n\nUnlike earlier examples, we cannot embed the Vue application directly here since it now depends on a data service which we have not deployed. Either way, the resulting application looks as follows:"
  },
  {
    "objectID": "posts/serving-postgis-features/index.html",
    "href": "posts/serving-postgis-features/index.html",
    "title": "Serving PostGIS Features Over HTTP",
    "section": "",
    "text": "I love spatial data, and I use it a LOT. My typical use case mostly involves downloading a feature class from some online source, and then using it entirely locally. If I just want to look at it, I’ll drop it in QGIS, but oftentimes I’ll load the features into a (containerised) PostGIS-enabled database. Unfortunately, many of the features I’m interested in are harder to download than I’d like. Take the Stats NZ Geographic Data Service, for example. Let’s say we simply wish to get a local copy of the Regional Council 2022 Clipped (generalised) feature class. We can’t download this directly (or include a link in a blog-post or tutorial)–instead we:\n\ncreate an account if we don’t have one already\nensure we have logged in\nselect the feature class\nclick a download link\nselect the output format and coordinate reference in a dialog\nclick ‘accept terms and create download’ link\nwait (sometimes for a long time) for our download to be created\ndownload feature class via the returned link\n\nThere is a programmatic interface, but this is even less accessible, and requires the following steps:\n\ncreate an account if we don’t have one already\ncreate an API key (making sure it has the right functionality selected, also)\ncreate a download via the exports/layers endpoint\npoll the returned endpoint until the state becomes complete\ndownload feature class via the embedded download link\n\nSuch services are useful for less straightforward applications, such as providing WMTS layers which can be used in leaflet or openlayers. But in the situation outlined, it would be more convenient to simply be able to download the data without fanfare via a direct link. For example, it would be extremely easy (and cheap) to simply dump the more popular features classes in an S3 bucket in a handful of common formats. But we could still make features available directly, in the preferred format, via a web service…"
  },
  {
    "objectID": "posts/serving-postgis-features/index.html#a-quick-demo",
    "href": "posts/serving-postgis-features/index.html#a-quick-demo",
    "title": "Serving PostGIS Features Over HTTP",
    "section": "A Quick Demo",
    "text": "A Quick Demo\nA containerised version of the service can be started by running the following (note that Git large file storage is required in order to fetch the example feature classes, and sbt is required to build the service itself):\n$ git lfs clone git@github.com:cmhh/featureserver.git\n$ cd featureserver\n$ sbt assembly\n$ docker-compose up -d\nThe server will be running on localhost:9002. To list available features, we GET localhost:9002/featureserver/listFeatures:\n\nThen, for example, to download regional_council_2022 as a GeoPackage file we’d GET:\nhttp://localhost:9002/featureserver/getFeatureClass?catalog=gis&schema=statsnz&name=regional_council_2022\nThis will result in a file named regional_council_2022.gpkg.zip which can be unzipped locally, or happily used directly in QGIS:\nVideo\nNow lets look in a little more detail (again, full source here)…"
  },
  {
    "objectID": "posts/serving-postgis-features/index.html#get-some-data",
    "href": "posts/serving-postgis-features/index.html#get-some-data",
    "title": "Serving PostGIS Features Over HTTP",
    "section": "Get some data",
    "text": "Get some data\nI downloaded a number of feature classes from Stats NZ Geographic Data Service as GeoPackages, and loaded them to a https://github.com/cmhh/featureserver/tree/main/data repository:\n\n\n\n\n\n\n\n\n\nfile\noriginal source\n\n\n\n\ncommunityboard2022.gpkg\nCommunity Board 2022 Clipped (generalised)\n\n\nconstituency2022.gpkg\nConstituency 2022 Clipped (generalised)\n\n\nmaoriconstituency2022.gpkg\nMāori Constituency 2022 Clipped (generalised)\n\n\nmaoriward2022.gpkg\nMāori Ward 2022 Clipped (generalised)\n\n\nmeshblock2022.gpkg\nMeshblock 2022 Clipped (generalised)\n\n\nregionalcouncil2022.gpkg\nRegional Council 2022 Clipped (generalised)\n\n\nstatisticalarea12022.gpkg\nStatistical Area 1 2022 Clipped (generalised)\n\n\nstatisticalarea22022.gpkg\nStatistical Area 2 2022 Clipped (generalised)\n\n\nsubdivision2022.gpkg\nTerritorial Authority 2022 Clipped (generalised)\n\n\nta2022.gpkg\nTerritorial Authority 2022 Clipped (generalised)\n\n\ntalb2022.gpkg\nTerritorial Authority Local Board 2022 Clipped (generalised)\n\n\nurbanrural2022.gpkg\nUrban Rural 2022 Clipped (Generalised)\n\n\nward2022.gpkg\nWard 2022 Clipped (Generalised)"
  },
  {
    "objectID": "posts/serving-postgis-features/index.html#get-a-running-postgis-instance",
    "href": "posts/serving-postgis-features/index.html#get-a-running-postgis-instance",
    "title": "Serving PostGIS Features Over HTTP",
    "section": "Get a running PostGIS instance",
    "text": "Get a running PostGIS instance\nAs noted, I like to run PostGIS on-demand in a Docker container. We can do this easily enough by creating Dockerfile with the following content:\nFROM ubuntu:20.04\n\nENV DEBIAN_FRONTEND=noninteractive\nENV SHELL=/bin/bash\n\nRUN  apt-get update && apt-get -y dist-upgrade && \\\n  apt-get install -y --no-install-recommends \\\n    wget gnupg2 ca-certificates gdal-bin sudo vim \\\n    libgdal-dev libgeos-dev libproj-dev libsqlite3-dev libudunits2-dev && \\\n  sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt focal-pgdg main\" > /etc/apt/sources.list.d/pgdg.list' && \\\n  wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | apt-key add - && \\\n  apt-get update && apt-get install -y --no-install-recommends postgresql-14 postgresql-14-postgis-3 postgis && \\\n  sed -i -e 's/# en_US.UTF-8 UTF-8/en_US.UTF-8 UTF-8/' /etc/locale.gen && \\\n  dpkg-reconfigure --frontend=noninteractive locales && \\\n  update-locale LANG=en_US.UTF-8 && \\\n  rm -rf /var/lib/apt/lists/* \n\nRUN service postgresql start && \\\n  sudo -u postgres psql -c 'create database gis;' && \\\n  sudo -u postgres psql -d gis -c 'create extension postgis;' && \\\n  sudo -u postgres psql -d gis -c 'create extension postgis_raster;' && \\\n  sudo -u postgres psql -d gis -c 'create extension postgis_sfcgal;' && \\\n  sudo -u postgres psql -d gis -c 'create extension postgis_topology;' && \\\n  sudo -u postgres psql -d gis -c \"SET postgis.gdal_enabled_drivers = 'ENABLE_ALL';\" && \\\n  sudo -u postgres psql -c 'create user gisuser;' && \\\n  sudo -u postgres psql -c \"alter user gisuser with encrypted password 'gisuser';\" && \\\n  sudo -u postgres psql -c 'grant all privileges on database gis to gisuser;' && \\\n  printf \"\\tlisten_addresses='*'\\t\" >> /etc/postgresql/14/main/postgresql.conf && \\\n  sed -i -E '/local +all +all +peer/ s/peer/md5/' /etc/postgresql/14/main/pg_hba.conf && \\\n  sed -i -E '/host +all +all +127.0.0.1\\/32 +scram-sha-256/ s/127.0.0.1\\/32/0.0.0.0\\/0   /' /etc/postgresql/14/main/pg_hba.conf && \\\n  sed -i -E '/host +all +all +::1\\/128 +scram-sha-256/ s/::1\\/128/::0\\/0  /' /etc/postgresql/14/main/pg_hba.conf &&\\\n  printf \"localhost:5432:gis:gisuser:gisuser\" >> /root/.pgpass && \\\n  chmod 0600 /root/.pgpass\n\nRUN adduser --disabled-password --gecos \"\" gisuser && \\\n  usermod --password $(openssl passwd -1 gisuser) gisuser && \\\n  usermod -aG sudo gisuser\n\nEXPOSE 5432\n\nENTRYPOINT service postgresql start && \\\n  tail -f\nWe build this and run an instance as follows:\n$ docker build -t postgis <path to Dockerfile>/.\n$ docker run -d --rm --name postgis -v data:/data -p 5433:5432 postgis\nWe deliberately mount the data folder holding each of our GeoPackages, and the we run commands along the lines of:\nPGPASSWORD=gisuser psql -U gisuser -d gis -c 'create schema statsnz;'\n\nogr2ogr \\\n  -f PostgreSQL PG:\"dbname='gis' user='gisuser' password='gisuser'\" \\\n  regionalcouncil2022.gpkg \\\n  -nln statsnz.regional_council_2022\n\nogr2ogr \\\n  -f PostgreSQL PG:\"dbname='gis' user='gisuser' password='gisuser'\" \\\n  maoriconstituency2022.gpkg \\\n  -nln statsnz.maori_constituency_2022\nWe can automate this further (and, obviously be safer with passwords and root priviliges), but this is good for illustration."
  },
  {
    "objectID": "posts/serving-postgis-features/index.html#save-postgis-table-as-files-on-disk",
    "href": "posts/serving-postgis-features/index.html#save-postgis-table-as-files-on-disk",
    "title": "Serving PostGIS Features Over HTTP",
    "section": "Save PostGIS table as ‘files’ on disk",
    "text": "Save PostGIS table as ‘files’ on disk\nWe first write a function with the following signature:\ndef command(\n  catalog: String, schema: String, table: String, \n  path: String, format: Option[Format], epsg: Option[Int], simplify: Option[Double]\n): Result[String]\nResult is a simple custom type, similar to Try, except that the happy path is further differentiated into Empty and NonEmpty, and we do this so that we can send different HTTP codes in each case (while still using pattern matching). The command function first confirms the requested feature class exists, returning a valid ogr2ogr command inside a NonEmpty Result if it does, and an Empty Result if not. If the PostGIS database is unreachable, the result will be a Throwable inside an Error Result. But, as an example (the output isn’t as pretty–I added some line breaks!):\nutils.command(\n  \"gis\", \"statsnz\", \"regional_council_2022\", \n  \"blah\", \"geopackage\", Some(4326), Some(100)\n) match { \n  case NonEmpty(cmd) => println(cmd) \n  case _ => ()\n}\nogr2ogr -f \"GPKG\" \"blah/regional_council_2022.gpkg\" \\\n  PG:\"host=localhost port=5433 user=gisuser password=gisuser dbname=gis\" \\\n  \"statsnz.regional_council_2022\" \\\n  -t_srs \"EPSG:4326\" -simplify 100.0 -nlt MULTIPOLYGON \\\n  -nln \"regional_council_2022\" -overwrite\nThis command can be run as a process easily enough:\nimport sys.process._\nval p = Process(s\"\"\"$cmd\"\"\")\np.!!\nWe then zip the results using the java.util.zip library using the following function:\nimport java.nio.file.Files\nimport java.io.{File, FileInputStream, FileOutputStream}\nimport java.util.zip.{ZipEntry, ZipOutputStream}\nimport com.typesafe.config.{Config, ConfigFactory}\nimport scala.util.Try\n\ndef zip(folder: String, outfile: String): Try[String] = Try {\n  val dir: File = new File(folder)\n  val files = dir.listFiles().toList\n  val fos: FileOutputStream = new FileOutputStream(outfile)\n  val zos: ZipOutputStream = new ZipOutputStream(fos)\n  files.foreach(f => {\n    val fis = new FileInputStream(f)\n    val entry = new ZipEntry(f.getName())\n    zos.putNextEntry(entry)\n    val buff = Array.fill[Byte](1024)(0)\n    def loop(s: FileInputStream): Unit = {\n      val n = s.read(buff)\n      if (n < 0) ()\n      else {\n        zos.write(buff, 0, n)\n        loop(s)\n      }\n    }\n    loop(fis)\n    fis.close()\n    f.delete()\n  })\n\n  zos.close()\n  fos.close()\n  outfile\n}\nActually, we combine all the steps into a single function with the following signature:\ndef exportAndZip(\n  catalog: String, schema: String, table: String,\n  format: String, epsg: Option[Int], simplify: Option[Double]\n): Result[(String, String)]\nThe resulting tuple contains the folder where the ouptput is stored, and the name of the compressed archive. The output folder will be a temporary folder, and later it will be deleted after it has been streamed to the client–so many side-effects!"
  },
  {
    "objectID": "posts/serving-postgis-features/index.html#stream-files-to-clients",
    "href": "posts/serving-postgis-features/index.html#stream-files-to-clients",
    "title": "Serving PostGIS Features Over HTTP",
    "section": "Stream ‘files’ to clients",
    "text": "Stream ‘files’ to clients\nClients will be able to GET a feature class via the /getFeatureClass endpoint. The logic for this is:\nval getFeatureClass = path(\"getFeatureClass\") {\n  parameters(\n    \"catalog\", \"schema\", \"name\", \"format\".withDefault(\"gpkg\"), \"epsg\".as[Int].?, \"simplify\".as[Double].?\n  ){ (catalog, schema, name, format, epsg, simplify) =>\n    utils.exportAndZip(catalog, schema, name, format, epsg, simplify) match {\n      case NonEmpty(res) =>\n        val d = new java.io.File(res._1)\n        val f = new java.io.File(res._2)\n        val source = FileIO\n          .fromPath(f.toPath)\n          .watchTermination() { case (_, result) => \n            result.onComplete(_ => {\n              f.delete()\n              d.delete()\n            })\n          }\n        respondWithHeader(`Content-Disposition`(attachment, Map(\"filename\" -> f.getName()))) {\n          complete(HttpEntity(ContentTypes.`application/octet-stream`, source))\n        }\n      case Empty =>\n        complete(HttpResponse(\n          StatusCodes.NoContent\n        ))\n      case Error(e) =>\n        complete(HttpResponse(\n          StatusCodes.InternalServerError, \n          entity = HttpEntity(ContentTypes.`application/json`, s\"\"\"\"${e.getMessage()}\"\"\"\")\n        ))\n    }\n  }\n}\nNeat!"
  },
  {
    "objectID": "posts/serving-postgis-features/index.html#issues",
    "href": "posts/serving-postgis-features/index.html#issues",
    "title": "Serving PostGIS Features Over HTTP",
    "section": "Issues",
    "text": "Issues\nThe server works as it is. We can use it to download PostGIS features. We can reproject and simplify our data, and we can export to GeoPackage, shapefile, or GeoJSON. It’s likely the server will stop responding if several people simultaneously download a large feature class, but who knows! Also, there isn’t much in the way of feedback–if a client requests a large feature class that takes a while to prepare (the meshblock_2022 dataset is the biggest here, at around 90MB, and takes about 10 seconds to finish streaming, with no re-projection or simplifying) they won’t have any way of checking progress. Also, we delete objects after streaming, while it might make more sense to cache them for some period of time in the event they could be recycled."
  },
  {
    "objectID": "posts/working-productively-on-windows-using-wsl2-and-docker/index.html",
    "href": "posts/working-productively-on-windows-using-wsl2-and-docker/index.html",
    "title": "Working Productively on Windows Using Windows Subsystem for Linux 2 and Docker",
    "section": "",
    "text": "Prerequisites\nThe goal is to demonstrate that we can make Windows a productive environment in an enterprise setting, with only a minimal set of tools to bootstrap our efforts. All that is strictly required is:\n\nWSL2 is enabled on Windows\nUbuntu 20.04 is installed for WSL2\nDocker is installed on Ubuntu 20.04\nOpenSSH client is enabled on Windows\nVisual Studio Code is installed on Windows\nremote extensions are installed for vscode\n\nWSL2 is available on Windows 10, any edition, build 2004 and up, as well as for Windows Server 2016 and higher. For more installation details, see Docker on Windows with Windows Subsystem for Linux 2. In an enterprise setting, it would be possible to do most of this in a single powershell script, run as administrator on startup, for example.\n\n\nDocker Basics\nTo create a Docker image, we just need a file called Dockerfile containing a complete set of instructions for the build. That is, a container is (or can be) completely reproducible given only the Dockerfile, and so it is often sufficient to manage just this in a version control system.\nWe’ll see Dockerfile examples in the following sections, but given such a file, all we need to do to actually create a runnable image called foo is to run:\ndocker build -t foo <path to Dockerfile>\nOnce created, we can then run instances of the image by running something like the following:\ndocker run -d --rm --name mycontainer foo \nOf course, this is the simplest possible setup, and things can certainly ramp up a bit. We can copy files from local folders into the image, parameterise the build by using build arguments, add environment variables at runtime, create ad hoc networks for collections of containers, create volumes or mount local folders for persisting data, and more.\n\n\nContainers as Generic Development Environments\nDocker containers can be used as isolated development environments. In an enterprise setting, it is common to block Windows executable files that have not been certified in some way. However, we can write code, compile it, and run the resulting binaries inside containers, rather than on the host operating system. Even if we copy the resulting binaries to the host, and the binaries aren’t blocked as such, they might require a runtime that isn’t present, or they might otherwise not be binary compatible.\nSecurity aside, the approach also proves useful for other reasons. Namely, it can just be easier to manage a myriad of different build environments, all on a single host. We can develop standard images for various different use cases–a container for Java development, or maybe even different containers for different versions of Java or even different build tools (Maven or Gradle, for example), a container for Node.js development, and so on.\nConsider the following Dockerfile:\nFROM ubuntu:20.04\n\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN  apt-get update && apt-get -y dist-upgrade && \\\n  apt-get install -y --no-install-recommends \\\n    openssh-server openjdk-8-jdk locales apt-transport-https gnupg2 git vim wget curl xz-utils ca-certificates \\\n    build-essential gfortran && \\\n  sed -i -e 's/# en_US.UTF-8 UTF-8/en_US.UTF-8 UTF-8/' /etc/locale.gen && \\\n  dpkg-reconfigure --frontend=noninteractive locales && \\\n  update-locale LANG=en_US.UTF-8 && \\\n  echo \"deb https://dl.bintray.com/sbt/debian /\" | tee -a /etc/apt/sources.list.d/sbt.list && \\\n  curl -sL \"https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x2EE0EA64E40A89B84B2DF73499E82A75642AC823\" | apt-key add && \\\n  apt-get update && apt-get install -y sbt && \\\n  curl https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.deb --output packages-microsoft-prod.deb && \\\n  dpkg -i packages-microsoft-prod.deb && \\\n  curl https://nodejs.org/dist/v14.15.1/node-v14.15.1-linux-x64.tar.xz --output node-v14.15.1-linux-x64.tar.xz && \\\n  tar -xf node-v14.15.1-linux-x64.tar.xz && \\\n  mv node-v14.15.1-linux-x64 /usr/local/node && \\\n  echo \"export PATH=$PATH:/usr/local/node/bin\" >> /etc/bash.bashrc && \\\n  rm node-v14.15.1-linux-x64.tar.xz && \\\n  apt-get update && apt-get install -y dotnet-sdk-5.0 && \\\n  rm packages-microsoft-prod.deb && \\\n  apt-get clean && \\\n  rm -rf /var/lib/apt/lists/* \n\n  EXPOSE 22\n  \n  CMD mkdir -p /root/.ssh && \\\n    echo \"$PUB_KEY\" >> /root/.ssh/authorized_keys && \\\n    echo \"$PUB_KEY\" >> /root/.ssh/id_rsa.pub && \\\n    echo \"$PRIVATE_KEY\" >> /root/.ssh/id_rsa && \\\n    chmod 700 /root/.ssh && chmod 600 /root/.ssh/* && \\\n    service ssh start && \\\n    tail -f /dev/null  \nThose accustomed to working on Linux will find the content of the file relatively straightforward, but in summary the file does the following:\n\ninstalls the usual GNU compiler suite\ninstalls OpenJDK 8 for Java development\ninstalls git so we can work with git repositories\ninstalls sbt, a Java and Scala build tool\ninstalls .NET 5.0 for C# 9 and F# 5 development, including ASP.NET web applications\ninstalls Node.js for JavaScript development\ninstalls OpenSSH server so the container can be accessed remotely\n\nTo build the container from the development directory:\ndocker build -t development .\nand to run an instance:\ndocker run -d --rm --name development \\\n  -v $PWD/.ivy2:/root/.ivy2 \\\n  -v $PWD/.cache:/root/.cache \\\n  -v $PWD/.npm:/root/.npm \\\n  -v $PWD/.vscode-server:/root/.vscode-server \\\n  -e \"PUB_KEY=$(cat $HOME/.ssh/id_rsa.pub)\" \\\n  -e \"PRIVATE_KEY=$(cat $HOME/.ssh/id_rsa)\" \\\n  -p 23:22 -p 9001:9001 -p 5001:5001 -p 3001:3001 \\\n  development\nIn this case, we copy our keys to the container at runtime so that we can connect without a password via a standard SSH client. In particular, we add the following to our SSH config:\nHost devdocker\n  HostName localhost\n  Port 23\n  User root\nand we can then use Visual Studio Code to develop in the usual way, but using the running container. We bring up the command palette (CTRL+SHIFT+P) and select “Remote-SSH: Connect to Host…”\n\nThen select “dockerdev”:\n\nAt this point, a new vscode window will open, connected remotely to the running container:\n\nWe can clone repositories from a version control system, and commit changes; and provided the right ports are open, we can also test web services and so forth locally:\n\n\n\nDeployment via Containers\nApplications are routinely deployed using containers. In this case, we run a simple seasonal adjustment service. The Dockerfile is as follows:\nFROM ubuntu:20.04\n\nENV DEBIAN_FRONTEND noninteractive\nENV SHELL /bin/bash\n\nRUN  apt-get update && apt-get -y dist-upgrade && \\\n  apt-get install -y --no-install-recommends ca-certificates openjdk-8-jre-headless wget gfortran make && \\\n  mkdir -p /tmp/x13 && \\\n  cd /tmp/x13 && \\\n  wget https://www.census.gov/ts/x13as/unix/x13ashtmlsrc_V1.1_B39.tar.gz && \\\n  tar -xvf x13ashtmlsrc_V1.1_B39.tar.gz && \\\n  make -j20 -f makefile.gf && \\\n  mv x13asHTMLv11b39 /usr/bin/x13ashtml && \\\n  cd / && \\\n  rm -fR /tmp/x13 && \\\n  wget https://github.com/cmhh/seasadj/releases/download/0.1.0-SNAPSHOT/seasadj.jar && \\\n  apt-get remove -y wget gfortran make && \\ \n  apt-get autoremove -y && \\\n  apt-get clean && \\\n  rm -rf /var/lib/apt/lists/* \n\nEXPOSE 9001\n\nENTRYPOINT [\"java\", \"-cp\", \"/seasadj.jar\", \"org.cmhh.seasadj.Service\"]\nIn this example, we simply create a basic instance with a Java runtime, and download an artefact from GitHub to serve as our entrypoint. Note that the service itself also requires X13-ARIMA-SEATS be present, so we download the source and compile it as part of the image build.\nAs usual, the container is built as follows:\ndocker build -t seasadj .\nand run as follows:\ndocker run -td --rm --name seasadj -p 9001:9001 seasadj\nThe service is stateless. It accepts one or more input specifications as a JSON array, and returns seasonally adjusted data in the same format. For example, to adjust the following JSON input:\n{\n  \"ap\": {\n    \"series\": {\n      \"title\": \"Air Passengers\",\n      \"start\": 1958.01,\n      \"data\": [\n        340.0, 318.0, 362.0, 348.0, 363.0, 435.0, \n        491.0, 505.0, 404.0, 359.0, 310.0, 337.0, \n        360.0, 342.0, 406.0, 396.0, 420.0, 472.0, \n        548.0, 559.0, 463.0, 407.0, 362.0, 405.0, \n        417.0, 391.0, 419.0, 461.0, 472.0, 535.0, \n        622.0, 606.0, 508.0, 461.0, 390.0, 432.0\n      ]\n    },\n    \"x11\": null\n  }\n}\nwe could use CURL as follows:\ncurl \\\n  -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -d @airpassengers.min.json \\\n  localhost:9001/seasadj/adjust \\\n  --compressed --output airpassengers.output.json\nWe could even build a SPA. The following is a screenshot of a basic app written using Vue.js, for example:\n\nUnder basic load testing, the service could handle around 230 transactions per second on a laptop with an 8 core AMD Ryzen 5 2500U with Radeon Vega Mobile Gfx 2.00 GHz processor. And while no security is provided directly, authentication methods can be added easily if required, and communication can be encrypted easily enough using Nginx or similar.\n\n\nDatabases via Containers\nFully functional versions of popular databases are often available via official Docker images, both relational and NoSQL alike. For development purposes, containerised databases will generally suffice, and can be run in a completely local context.\nConsider the following Dockerfile, a modified version of adventureworks-docker, which provides a containerised version of Microsoft SQL Server with the popular AdventureWorks database pre-loaded:\nFROM mcr.microsoft.com/mssql/server\n\nWORKDIR /setup\n\nCOPY ./setup .\n\nCMD /bin/bash ./entrypoint.sh\nThis is very simple, though does depend on a large SQL script, setup-db.sql. To build:\ndocker build -t adventureworks .\nTo run an instance:\ndocker run -d --rm \\\n  --name adventureworks \\\n  -e 'ACCEPT_EULA=Y' \\\n  -e 'MSSQL_SA_PASSWORD=password-1234' \\\n  -p 1433:1433 \\\n  adventureworks\nOnce running, one can then connect using database clients such as the excellent DBeaver:\n\nThe database provided is a full-featured version of SQL Server, and so is sufficient for local development, but also general purpose testing. Want to know what sort of performance improvement would be achieved with the addition of a columnstore index? You can test the locally–no need to appeal to a physical or remote test environment, certainly if one doesn’t already exist, or if a process must be followed to be granted access.\n\n\nAnalytics via Containers\nContainers can be used to house any manner of analytical environment, from R with RStudio, to JupyterLab with a variety of kernels, and even to a fully-functional, if not highly performing, Big Data stack.\nConsider the following Dockerfile, which, when built, provides a running instance of RStudio Server:\nARG ubuntu_version=20.04\n\nFROM ubuntu:$ubuntu_version\n\nARG cores=12\nARG r_version=4.0.3\nARG rstudio_version=1.4.1032\nENV DEBIAN_FRONTEND=noninteractive\nENV SHELL=/bin/bash\n\nRUN  apt-get update && apt-get -y dist-upgrade && \\\n  apt-get install -y --no-install-recommends \\\n    gnupg2 dirmngr ca-certificates build-essential \\\n    libcairo2-dev '^perl-modules-[0-9].[0-9]+$' \\\n    libssl-dev libgit2-dev libcurl4-gnutls-dev libxml2-dev curl wget htop locales \\\n    openjdk-8-jdk python3-pip git vim libudunits2-dev \\\n    grass gdal-bin libgdal-dev libgeos-dev libproj-dev proj-bin proj-data \\\n    libblas3 libatlas-base-dev liblapack-dev libreadline-dev gfortran \\\n    libx11-dev libxt-dev zlib1g-dev libbz2-dev liblzma-dev libpcre2-dev \\\n    sudo lsb-release gdebi-core psmisc libclang-dev libapparmor1 && \\\n  apt-get clean && \\\n  rm -rf /var/lib/apt/lists/* && \\\n  sed -i -e 's/# en_US.UTF-8 UTF-8/en_US.UTF-8 UTF-8/' /etc/locale.gen && \\\n  dpkg-reconfigure --frontend=noninteractive locales && \\\n  update-locale LANG=en_US.UTF-8 && \\\n  wget -qO- \"https://yihui.org/gh/tinytex/tools/install-unx.sh\" | sh -s - --admin --no-path && \\\n  mv /root/.TinyTeX /usr/local/TinyTex && \\\n  /usr/local/TinyTex/bin/*/tlmgr path add && \\\n  mkdir /src && \\\n  cd /src && \\\n  wget https://cran.r-project.org/src/base/R-$(echo $r_version | cut -d'.' -f 1)/R-${r_version}.tar.gz && \\\n  tar -xvf R-${r_version}.tar.gz && \\\n  cd /src/R-${r_version} && \\\n  ./configure --enable-R-shlib --with-blas --with-lapack --enable-memory-profiling --with-cairo && \\\n  make -j $cores && make install && \\\n  ln -s /usr/local/lib/R/bin/R /usr/bin/R && \\\n  ln -s /usr/local/lib/R/bin/Rscript /usr/bin/Rscript && \\\n  cd / && rm -fR src && \\\n  echo \"local({\\n  r <- getOption('repos')\\n  r['CRAN'] <- 'https://cloud.r-project.org'\\n  options(repos = r)\\n})\" > /usr/local/lib/R/etc/Rprofile.site && \\\n  echo \"\\noptions('bitmapType' = 'cairo')\" >> /usr/local/lib/R/etc/Rprofile.site && \\\n  echo \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" > /usr/local/lib/R/etc/Renviron.site && \\\n  R -e \"install.packages('renv')\" && \\\n  wget https://s3.amazonaws.com/rstudio-ide-build/server/bionic/amd64/rstudio-server-${rstudio_version}-amd64.deb && \\\n  gdebi --non-interactive rstudio-server-${rstudio_version}-amd64.deb && \\\n  rm rstudio-server-${rstudio_version}-amd64.deb\n  \nRUN adduser --disabled-password --gecos \"\" guest && \\\n  usermod --password $(openssl passwd -1 guest) guest && \\\n  usermod -aG sudo guest \n\nEXPOSE 8787\n\nCMD service rstudio-server start && \\\n  tail -f /dev/null\nThe Dockerfile accepts build arguments, so images containing different versions of R, for example, can be created from the same set of instructions. For example:\ndocker build -t rstudio:4.0.3 --build-arg r_version=4.0.3 .\nTo run a container with the image:\ndocker run -d --rm \\\n  --name rstudio \\\n  -p 8787:8787 \\\n  -v ${PWD}/.local:/home/guest/.local \\\n  rstudio:4.0.3\nAt this point, RStudio Server will be available via a web browser at localhost:8787:\n\n\n\nUsing a Registry\nConsider the analytics example above, consisting of a containerised R environment. Such an image might well be of value to an entire group of analysts, so rather than provide a Dockerfile that each person can use to build their own image, we could build the image once and then push it to a central registry for anybody to use.\nWe use Docker compose to create a very simple example using a local registry and a basic web-based UI. Consider the following, which we save as docker-compose.yml:\nversion: \"3.8\"\nservices:\n  registry:\n    image: registry:2\n    ports:\n      - 5000:5000\n    volumes:\n      - ./registry-data:/var/lib/registry\n    networks:\n      - registrynet\n  ui:\n    image: joxit/docker-registry-ui:static\n    ports:\n      - 8080:80\n    environment:\n      - REGISTRY_TITLE=Local Docker Registry\n      - REGISTRY_URL=http://registry:5000\n    depends_on:\n      - registry\n    networks:\n      - registrynet\nnetworks:\n  registrynet:\nThis describes the coordination of two containers as a single registry setup, and can be run as follows:\ndocker-compose up -d\nThe R image build earlier can be tagged and pushed to the registry as follows:\ndocker tag rstudio:4.0.3 localhost:8080/rstudio:4.0.3\ndocker push localhost:8080/rstudio:4.0.3\nThe UI itself will be available in our web browser at localhost:8080:\n\n\n\nSummary\nWSL2 lets us use a Windows machine for any manner of local development and analytics. A Linux instance running via WSL2 gives us a degree of isolation, and so is relatively safe–perhaps no less safe than allowing the local use of something like Python. We can ramp up the isolation by using the Linux instance exclusively to build Docker images and run Docker containers. Besides, using containers dramatically simplifies application deployment, and is relatively standard at this point, and so is a useful skill to develop.\nHaving access to a Linux instance, and having the ability to build and run container images, means there’s almost nothing we couldn’t build or test locally, so with little cost or administrative drag. The use cases here are by no means exhaustive, but we can:\n\nuse containers to build and test applications locally\nuse containers to test deployment of applications\nuse containers to house (possibly ephemeral) database instances\nuse containers to run or test analytical / data science platforms\nand much more!"
  },
  {
    "objectID": "posts/docker-on-windows-with-wsl2/index.html",
    "href": "posts/docker-on-windows-with-wsl2/index.html",
    "title": "Docker on Windows with Windows Subsystem for Linux 2",
    "section": "",
    "text": "Windows Subsystem for Linux version 2 (WSL2) brings something like a native Linux environment to Windows. And because it’s native, we can even run Docker on WSL2 and access the running containers from the Windows host without hassle."
  },
  {
    "objectID": "posts/docker-on-windows-with-wsl2/index.html#enable-openssh-client",
    "href": "posts/docker-on-windows-with-wsl2/index.html#enable-openssh-client",
    "title": "Docker on Windows with Windows Subsystem for Linux 2",
    "section": "Enable OpenSSH client",
    "text": "Enable OpenSSH client\nTo confirm OpenSSH is available for install, open PowerShell as administrator and run:\n> Get-WindowsCapability -Online | ? Name -like 'OpenSSH*'\nIf OpenSSH is available, you’ll see something like:\nName  : OpenSSH.Client~~~~0.0.1.0\nState : Installed\n\nName  : OpenSSH.Server~~~~0.0.1.0\nState : NotPresent\nTo enable OpenSSH, run:\n> Add-WindowsCapability -Online -Name OpenSSH.Client~~~~0.0.1.0"
  },
  {
    "objectID": "posts/docker-on-windows-with-wsl2/index.html#enable-wsl2",
    "href": "posts/docker-on-windows-with-wsl2/index.html#enable-wsl2",
    "title": "Docker on Windows with Windows Subsystem for Linux 2",
    "section": "Enable WSL2",
    "text": "Enable WSL2\nTo enable WSL2, open PowerShell as administrator and run:\n> dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart\n> dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart\nYou will need to restart Windows at this point. After rebooting, download and run the following installer:\nWSL2 Linux Kernel\nThen, open PowerShell as administrator and run:\n> wsl --set-default-version 2"
  },
  {
    "objectID": "posts/docker-on-windows-with-wsl2/index.html#install-ubuntu-20.04-sans-windows-store",
    "href": "posts/docker-on-windows-with-wsl2/index.html#install-ubuntu-20.04-sans-windows-store",
    "title": "Docker on Windows with Windows Subsystem for Linux 2",
    "section": "Install Ubuntu 20.04 (sans Windows Store)",
    "text": "Install Ubuntu 20.04 (sans Windows Store)\nInstalling a Linux distribution is easily done via the Windows Store. However, in an enterprise environment where Store access is limited, or where one simply does not wish to use the Store, one can still download distributions directly, as listed:\nManually download Windows Subsystem for Linux distro packages\nIn our case, we download Ubuntu 20.04, which is the current LTS version of Ubuntu:\nUbuntu 20.04\nWe’ll install Ubuntu in C:\\Program Files\\Linux, though perhaps users might rather place it somewhere in their own user directory. To create this folder, and then download the above file from PowerShell run:\n> mkdir 'c:\\Program Files\\Linux'\n> cd 'c:\\Program Files\\Linux'\n> curl.exe -L -o ubuntu-2004.appx https://aka.ms/wslubuntu2004\nIt seems an appx file is just a zip archive, so rename the file and unzip it from PowerShell by running:\n> Rename-Item .\\ubuntu-2004.appx .\\ubuntu-2004.zip\n> Expand-Archive .\\ubuntu-2004.zip .\\ubuntu2004\nTo start Ubuntu, run:\n> .\\ubuntu2004\\ubuntu2004.exe\nThe first time Ubuntu is run you will be prompted to add a username and password. It doesn’t really matter what you choose, but either way you’ll see something like the following:\n> .\\ubuntu2004\\ubuntu2004.exe\nInstalling, this may take a few minutes...\nPlease create a default UNIX user account. The username does not need to match your Windows username.\nFor more information visit: https://aka.ms/wslusers\nEnter new UNIX username: cxhansen\nNew password:\nRetype new password:\npasswd: password updated successfully\nInstallation successful!\nTo run a command as administrator (user \"root\"), use \"sudo <command>\".\nSee \"man sudo_root\" for details.\n\nWelcome to Ubuntu 20.04 LTS (GNU/Linux 4.19.104-microsoft-standard x86_64)\n\n * Documentation:  https://help.ubuntu.com\n * Management:     https://landscape.canonical.com\n * Support:        https://ubuntu.com/advantage\n\n  System information as of Sat Jul 18 14:43:16 NZST 2020\n\n  System load:  0.16               Processes:             8\n  Usage of /:   0.4% of 250.98GB   Users logged in:       0\n  Memory usage: 1%                 IPv4 address for eth0: 172.27.186.14\n  Swap usage:   0%\n\n0 updates can be installed immediately.\n0 of these updates are security updates.\n\n\nThe list of available updates is more than a week old.\nTo check for new updates run: sudo apt update\n\n\nThis message is shown once once a day. To disable it please create the\n/home/cxhansen/.hushlogin file.\ncxhansen@DESKTOP-FA4F6IC:~$\nIf Ubuntu is the only distribution installed, or it is the default distribution, simply running wsl from PowerShell will be enough to start it. The available distributions can be listed by running wsl -d. In this case, with only Ubuntu 20.04 installed so far, the output is as follows:\nWindows Subsystem for Linux Distributions:\nUbuntu-20.04 (Default)\nIf more distributions were available, Ubuntu 20.04 could be started by running:\n> wsl -d Ubuntu-20.04\nand Ubuntu 20.04 can be set as the default by running:\n> wsl -s Ubuntu-20.04"
  },
  {
    "objectID": "posts/docker-on-windows-with-wsl2/index.html#install-docker-on-ubuntu-20.04",
    "href": "posts/docker-on-windows-with-wsl2/index.html#install-docker-on-ubuntu-20.04",
    "title": "Docker on Windows with Windows Subsystem for Linux 2",
    "section": "Install Docker on Ubuntu 20.04",
    "text": "Install Docker on Ubuntu 20.04\nDocker is available in the standard Ubuntu repositories, but it can be useful to use the official repositories instead. Docker has some prerequisites, which can be installed as follows:\n$ sudo apt update\n$ sudo apt -y install \\\n    apt-transport-https ca-certificates curl gnupg-agent \\\n    software-properties-common\nThen add the Docker repositories, and install Docker:\n$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n$ sudo apt-key fingerprint 0EBFCD88\n$ sudo add-apt-repository \\\n   \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n   $(lsb_release -cs) \\\n   stable\"\n$ sudo apt update\n$ sudo DEBIAN_FRONTEND=noninteractive \\\n    apt -y install \\\n    docker-ce docker-ce-cli containerd.io\nIn order to run Docker without using sudo, relevant users can be added to the Docker group:\n$ usermod -aG docker <user> \nThe Docker daemon will not start automatically as for a standard Ubuntu installation, but it can be started manually by running:\n$ sudo service docker start\nTo confirm it is running:\n$ sudo service docker status\n * Docker is running\nAnd to confirm it works:\n$ docker run hello-world\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://hub.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/get-started/"
  },
  {
    "objectID": "posts/docker-on-windows-with-wsl2/index.html#install-docker-compose-on-ubuntu-20.04",
    "href": "posts/docker-on-windows-with-wsl2/index.html#install-docker-compose-on-ubuntu-20.04",
    "title": "Docker on Windows with Windows Subsystem for Linux 2",
    "section": "Install Docker Compose on Ubuntu 20.04",
    "text": "Install Docker Compose on Ubuntu 20.04\nDocker Compose is a utility that allows one to coordinate the use of several containers, that is, to effectively treat several containers as a single runnable unit. To install:\n$ sudo curl -L \\ \n    \"https://github.com/docker/compose/releases/download/1.26.2/docker-compose-$(uname -s)-$(uname -m)\" \\\n    -o /usr/local/bin/docker-compose\n$ chmod +x /usr/local/bin/docker-compose\n$ docker-compose --version\ndocker-compose version 1.26.2, build eefe0d31"
  },
  {
    "objectID": "posts/docker-on-windows-with-wsl2/index.html#install-visual-studio-code",
    "href": "posts/docker-on-windows-with-wsl2/index.html#install-visual-studio-code",
    "title": "Docker on Windows with Windows Subsystem for Linux 2",
    "section": "Install Visual Studio Code",
    "text": "Install Visual Studio Code\nInstalling vscode is straightforward–simply run the installer found at the vscode website. When running vscode for the first time, one might see the following prompt:\n\nChoose ‘Install’ to install the extension. Otherwise, press CTRL+SHIFT+X and search for ‘wsl’:\n\nMany other useful extensions are available, including ‘Remote - SSH’, which allows one to edit files on any remote machine which is accessible by SSH.\nWhen the WSL extension is installed, one can simply issue CODE <folder> & from within a running WSL instance to start vscode on the host machine:"
  },
  {
    "objectID": "posts/docker-on-windows-with-wsl2/index.html#install-vcxsrv",
    "href": "posts/docker-on-windows-with-wsl2/index.html#install-vcxsrv",
    "title": "Docker on Windows with Windows Subsystem for Linux 2",
    "section": "Install VcXsrv",
    "text": "Install VcXsrv\nTo install VcXsrv, simply download and run the installer from the VcXsrv download page.\nNote that VcXsrv works best with WSL2 if it is started with certain switches. Specifically, one should ensure the target is as follows (adapt as required if VcXsrv was installed in a different folder):\n\"C:\\Program Files\\VcXsrv\\vcxsrv.exe\" :0 -ac -terminate -lesspointer -multiwindow -clipboard -wgl\nFrom inside the running Linux instance, one should also set the DISPLAY environment variable as follows:\n$ export DISPLAY=$(awk '/nameserver / {print $2; exit}' /etc/resolv.conf 2> /dev/null):0\nIt should then be possible to run graphical applications using the host display:"
  },
  {
    "objectID": "posts/docker-on-windows-with-wsl2/index.html#complete-linux-desktop",
    "href": "posts/docker-on-windows-with-wsl2/index.html#complete-linux-desktop",
    "title": "Docker on Windows with Windows Subsystem for Linux 2",
    "section": "Complete Linux Desktop",
    "text": "Complete Linux Desktop\nWe can go further than simply opening graphical applications, and start a full-blown desktop. It’s pretty involved, so we won’t cover it here. But those interested could read, for example:\nUbuntu 20.04 Desktop GUI on WSL 2 on Surface Pro 4\n\n\n\nUbuntu desktop on a Surface Pro!"
  },
  {
    "objectID": "posts/data-services-from-postgres/index.html",
    "href": "posts/data-services-from-postgres/index.html",
    "title": "Data Services from Existing PostgreSQL Databases",
    "section": "",
    "text": "A while back I made a small data service which took time series data I scraped from the web, and used it to make a data service. I used Play Framework for the API itself, and PostgreSQL as the back-end. The service wasn’t terribly complicated to put together, and I do think that building bespoke services as the need arises, rather than aiming for some sort of generic framework, can be useful. Still, there are, or must be, generic solutions that would be good enough? And maybe you already have the back-end database just sitting there and you just want to make the data available with as little effort as possible…\nI did a little Googling, and came up with two pretty cool looking options. PostGraphile is a GraphQL wrapper for PostgreSQL, and has over 10k stars on GitHub!, where the claim is:\n\nExecute one command (or mount one Node.js middleware) and get an instant high-performance GraphQL API for your PostgreSQL database!\n\nPostgREST is a REST wrapper for PostgreSQL, and has over 17k stars on GitHub!, and claims:\n\nPostgREST serves a fully RESTful API from any existing PostgreSQL database. It provides a cleaner, more standards-compliant, faster API than you are likely to write from scratch.\n\nSo, in this post, we’ll look at the back-end for the aforementioned time series service, and see just how easy it is to use both PostGraphile and PostgREST, and how easily they can replicate the functionality of the existing service.\nNote that there are quite a few commercial options, covering frameworks such as OData, and enterprise users might prefer to go that route. That doesn’t really make this post any less meaningful, of course, since a commercial offering must be even easier, or why would anybody use it? Right? RIGHT?!"
  },
  {
    "objectID": "posts/data-services-from-postgres/index.html#postgraphile-1",
    "href": "posts/data-services-from-postgres/index.html#postgraphile-1",
    "title": "Data Services from Existing PostgreSQL Databases",
    "section": "PostGraphile",
    "text": "PostGraphile\nTo use PostGraphile, one must first have Node.js installed. At the time of writing, the current LTS version of Node.js is 14.17.5, and installing is as simple as downloading and unpacking a binary archive. Something like\n$ mkdir -p ${HOME}/local/node\n$ curl -s \\\n  https://nodejs.org/dist/v14.17.5/node-v14.17.5-linux-x64.tar.xz | \\\n  tar xJvf - --strip-components=1 -C ${HOME}/local/node && \\\n$ echo \"export PATH=\\$PATH:${HOME}/local/node/bin\" >> ~/.bashrc\nOne can then install PostGraphile by running:\nnpm install -g postgraphile"
  },
  {
    "objectID": "posts/data-services-from-postgres/index.html#postgrest-1",
    "href": "posts/data-services-from-postgres/index.html#postgrest-1",
    "title": "Data Services from Existing PostgreSQL Databases",
    "section": "PostgREST",
    "text": "PostgREST\nInstalling PostgREST simply required downloading and unpacking the following file:\nhttps://github.com/PostgREST/postgrest/releases/download/v8.0.0/postgrest-v8.0.0-linux-x64-static.tar.xz\nThis contains a single binary named postgrest. Note that the program depends on the PostgreSQL API, which can be installed on Debian-like systems by running:\nsudo apt-get install libpq-dev"
  }
]